{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14169075,"sourceType":"datasetVersion","datasetId":9031677}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas spacy neo4j networkx scikit-learn\n!python -m spacy download en_core_web_sm\n!pip install --upgrade --force-reinstall numpy==1.26.4 scikit-learn==1.3.2\n!pip install --force-reinstall numpy==1.26.4\n!pip install --force-reinstall scipy==1.10.1\n!pip install --force-reinstall scikit-learn==1.3.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom neo4j import GraphDatabase\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score,precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\nimport numpy as np\nfrom collections import Counter, defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:28.781751Z","iopub.execute_input":"2025-12-16T06:59:28.782091Z","iopub.status.idle":"2025-12-16T06:59:34.063303Z","shell.execute_reply.started":"2025-12-16T06:59:28.782057Z","shell.execute_reply":"2025-12-16T06:59:34.061917Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fpl-seasons-pop/fpl_two_seasons.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:34.065088Z","iopub.execute_input":"2025-12-16T06:59:34.065892Z","iopub.status.idle":"2025-12-16T06:59:34.295379Z","shell.execute_reply.started":"2025-12-16T06:59:34.065862Z","shell.execute_reply":"2025-12-16T06:59:34.294526Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"af = pd.read_csv('/kaggle/input/fpl-seasons-pop/cleaned_merged_seasons.csv', \n                 dtype='object')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:34.296507Z","iopub.execute_input":"2025-12-16T06:59:34.296804Z","iopub.status.idle":"2025-12-16T06:59:34.907103Z","shell.execute_reply.started":"2025-12-16T06:59:34.296781Z","shell.execute_reply":"2025-12-16T06:59:34.906203Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:34.908939Z","iopub.execute_input":"2025-12-16T06:59:34.909292Z","iopub.status.idle":"2025-12-16T06:59:34.943672Z","shell.execute_reply.started":"2025-12-16T06:59:34.909256Z","shell.execute_reply":"2025-12-16T06:59:34.942645Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51952 entries, 0 to 51951\nData columns (total 35 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   season             51952 non-null  object \n 1   name               51952 non-null  object \n 2   position           51952 non-null  object \n 3   assists            51952 non-null  int64  \n 4   bonus              51952 non-null  int64  \n 5   bps                51952 non-null  int64  \n 6   clean_sheets       51952 non-null  int64  \n 7   creativity         51952 non-null  float64\n 8   element            51952 non-null  int64  \n 9   fixture            51952 non-null  int64  \n 10  goals_conceded     51952 non-null  int64  \n 11  goals_scored       51952 non-null  int64  \n 12  ict_index          51952 non-null  float64\n 13  influence          51952 non-null  float64\n 14  kickoff_time       51952 non-null  object \n 15  minutes            51952 non-null  int64  \n 16  own_goals          51952 non-null  int64  \n 17  penalties_missed   51952 non-null  int64  \n 18  penalties_saved    51952 non-null  int64  \n 19  red_cards          51952 non-null  int64  \n 20  saves              51952 non-null  int64  \n 21  selected           51952 non-null  int64  \n 22  team_a_score       51952 non-null  int64  \n 23  team_h_score       51952 non-null  int64  \n 24  threat             51952 non-null  int64  \n 25  total_points       51952 non-null  int64  \n 26  transfers_balance  51952 non-null  int64  \n 27  transfers_in       51952 non-null  int64  \n 28  transfers_out      51952 non-null  int64  \n 29  value              51952 non-null  int64  \n 30  yellow_cards       51952 non-null  int64  \n 31  GW                 51952 non-null  int64  \n 32  form               51952 non-null  float64\n 33  home_team          51952 non-null  object \n 34  away_team          51952 non-null  object \ndtypes: float64(4), int64(25), object(6)\nmemory usage: 13.9+ MB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"af.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:34.944866Z","iopub.execute_input":"2025-12-16T06:59:34.945187Z","iopub.status.idle":"2025-12-16T06:59:35.133226Z","shell.execute_reply.started":"2025-12-16T06:59:34.945151Z","shell.execute_reply":"2025-12-16T06:59:35.132240Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 96169 entries, 0 to 96168\nData columns (total 37 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   season_x           96169 non-null  object\n 1   name               96169 non-null  object\n 2   position           96169 non-null  object\n 3   team_x             76317 non-null  object\n 4   assists            96169 non-null  object\n 5   bonus              96169 non-null  object\n 6   bps                96169 non-null  object\n 7   clean_sheets       96169 non-null  object\n 8   creativity         96169 non-null  object\n 9   element            96169 non-null  object\n 10  fixture            96169 non-null  object\n 11  goals_conceded     96169 non-null  object\n 12  goals_scored       96169 non-null  object\n 13  ict_index          96169 non-null  object\n 14  influence          96169 non-null  object\n 15  kickoff_time       96169 non-null  object\n 16  minutes            96169 non-null  object\n 17  opponent_team      96169 non-null  object\n 18  opp_team_name      96169 non-null  object\n 19  own_goals          96169 non-null  object\n 20  penalties_missed   96169 non-null  object\n 21  penalties_saved    96169 non-null  object\n 22  red_cards          96169 non-null  object\n 23  round              96169 non-null  object\n 24  saves              96169 non-null  object\n 25  selected           96169 non-null  object\n 26  team_a_score       96169 non-null  object\n 27  team_h_score       96169 non-null  object\n 28  threat             96169 non-null  object\n 29  total_points       96169 non-null  object\n 30  transfers_balance  96169 non-null  object\n 31  transfers_in       96169 non-null  object\n 32  transfers_out      96169 non-null  object\n 33  value              96169 non-null  object\n 34  was_home           96169 non-null  object\n 35  yellow_cards       96169 non-null  object\n 36  GW                 96169 non-null  object\ndtypes: object(37)\nmemory usage: 27.1+ MB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:35.134052Z","iopub.execute_input":"2025-12-16T06:59:35.134392Z","iopub.status.idle":"2025-12-16T06:59:35.256107Z","shell.execute_reply.started":"2025-12-16T06:59:35.134366Z","shell.execute_reply":"2025-12-16T06:59:35.254813Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"            assists         bonus           bps  clean_sheets    creativity  \\\ncount  51952.000000  51952.000000  51952.000000  51952.000000  51952.000000   \nmean       0.035725      0.093875      5.276043      0.090930      4.094651   \nstd        0.201135      0.459547      9.228127      0.287512      9.931570   \nmin        0.000000      0.000000    -21.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      8.000000      0.000000      1.500000   \nmax        4.000000      3.000000    128.000000      1.000000    136.200000   \n\n            element       fixture  goals_conceded  goals_scored     ict_index  \\\ncount  51952.000000  51952.000000    51952.000000  51952.000000  51952.000000   \nmean     344.981637    199.273271        0.456287      0.039941      1.473866   \nstd      201.805312    108.743119        0.946539      0.217700      2.819401   \nmin        1.000000      1.000000        0.000000      0.000000      0.000000   \n25%      171.000000    106.000000        0.000000      0.000000      0.000000   \n50%      342.000000    204.000000        0.000000      0.000000      0.000000   \n75%      513.000000    294.000000        1.000000      0.000000      2.000000   \nmax      778.000000    380.000000        9.000000      4.000000     31.500000   \n\n       ...  team_h_score        threat  total_points  transfers_balance  \\\ncount  ...  51952.000000  51952.000000  51952.000000       5.195200e+04   \nmean   ...      1.569391      4.507488      1.215314       1.999289e+03   \nstd    ...      1.365193     12.189522      2.403077       7.288251e+04   \nmin    ...      0.000000      0.000000     -4.000000      -2.180978e+06   \n25%    ...      1.000000      0.000000      0.000000      -1.089000e+03   \n50%    ...      1.000000      0.000000      0.000000      -4.500000e+01   \n75%    ...      2.000000      2.000000      1.000000       5.600000e+01   \nmax    ...      9.000000    169.000000     24.000000       1.983733e+06   \n\n       transfers_in  transfers_out         value  yellow_cards            GW  \\\ncount  5.195200e+04   5.195200e+04  51952.000000  51952.000000  51952.000000   \nmean   1.576509e+04   1.376511e+04     50.031491      0.051297     21.121073   \nstd    6.421021e+04   5.377897e+04     11.917808      0.220606     10.951132   \nmin    0.000000e+00   0.000000e+00     37.000000      0.000000      1.000000   \n25%    3.300000e+01   9.800000e+01     44.000000      0.000000     12.000000   \n50%    3.460000e+02   8.790000e+02     45.000000      0.000000     22.000000   \n75%    4.464000e+03   6.863250e+03     53.000000      0.000000     31.000000   \nmax    2.104464e+06   2.233619e+06    133.000000      1.000000     38.000000   \n\n               form  \ncount  51952.000000  \nmean       0.119020  \nstd        0.175579  \nmin       -0.200000  \n25%        0.000000  \n50%        0.025000  \n75%        0.200000  \nmax        2.000000  \n\n[8 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>assists</th>\n      <th>bonus</th>\n      <th>bps</th>\n      <th>clean_sheets</th>\n      <th>creativity</th>\n      <th>element</th>\n      <th>fixture</th>\n      <th>goals_conceded</th>\n      <th>goals_scored</th>\n      <th>ict_index</th>\n      <th>...</th>\n      <th>team_h_score</th>\n      <th>threat</th>\n      <th>total_points</th>\n      <th>transfers_balance</th>\n      <th>transfers_in</th>\n      <th>transfers_out</th>\n      <th>value</th>\n      <th>yellow_cards</th>\n      <th>GW</th>\n      <th>form</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>...</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>5.195200e+04</td>\n      <td>5.195200e+04</td>\n      <td>5.195200e+04</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n      <td>51952.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.035725</td>\n      <td>0.093875</td>\n      <td>5.276043</td>\n      <td>0.090930</td>\n      <td>4.094651</td>\n      <td>344.981637</td>\n      <td>199.273271</td>\n      <td>0.456287</td>\n      <td>0.039941</td>\n      <td>1.473866</td>\n      <td>...</td>\n      <td>1.569391</td>\n      <td>4.507488</td>\n      <td>1.215314</td>\n      <td>1.999289e+03</td>\n      <td>1.576509e+04</td>\n      <td>1.376511e+04</td>\n      <td>50.031491</td>\n      <td>0.051297</td>\n      <td>21.121073</td>\n      <td>0.119020</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.201135</td>\n      <td>0.459547</td>\n      <td>9.228127</td>\n      <td>0.287512</td>\n      <td>9.931570</td>\n      <td>201.805312</td>\n      <td>108.743119</td>\n      <td>0.946539</td>\n      <td>0.217700</td>\n      <td>2.819401</td>\n      <td>...</td>\n      <td>1.365193</td>\n      <td>12.189522</td>\n      <td>2.403077</td>\n      <td>7.288251e+04</td>\n      <td>6.421021e+04</td>\n      <td>5.377897e+04</td>\n      <td>11.917808</td>\n      <td>0.220606</td>\n      <td>10.951132</td>\n      <td>0.175579</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-21.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-4.000000</td>\n      <td>-2.180978e+06</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>37.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.200000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>171.000000</td>\n      <td>106.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-1.089000e+03</td>\n      <td>3.300000e+01</td>\n      <td>9.800000e+01</td>\n      <td>44.000000</td>\n      <td>0.000000</td>\n      <td>12.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>342.000000</td>\n      <td>204.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-4.500000e+01</td>\n      <td>3.460000e+02</td>\n      <td>8.790000e+02</td>\n      <td>45.000000</td>\n      <td>0.000000</td>\n      <td>22.000000</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>1.500000</td>\n      <td>513.000000</td>\n      <td>294.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>5.600000e+01</td>\n      <td>4.464000e+03</td>\n      <td>6.863250e+03</td>\n      <td>53.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>128.000000</td>\n      <td>1.000000</td>\n      <td>136.200000</td>\n      <td>778.000000</td>\n      <td>380.000000</td>\n      <td>9.000000</td>\n      <td>4.000000</td>\n      <td>31.500000</td>\n      <td>...</td>\n      <td>9.000000</td>\n      <td>169.000000</td>\n      <td>24.000000</td>\n      <td>1.983733e+06</td>\n      <td>2.104464e+06</td>\n      <td>2.233619e+06</td>\n      <td>133.000000</td>\n      <td>1.000000</td>\n      <td>38.000000</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 29 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:35.257314Z","iopub.execute_input":"2025-12-16T06:59:35.257639Z","iopub.status.idle":"2025-12-16T06:59:35.286825Z","shell.execute_reply.started":"2025-12-16T06:59:35.257612Z","shell.execute_reply":"2025-12-16T06:59:35.285425Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"season               0\nname                 0\nposition             0\nassists              0\nbonus                0\nbps                  0\nclean_sheets         0\ncreativity           0\nelement              0\nfixture              0\ngoals_conceded       0\ngoals_scored         0\nict_index            0\ninfluence            0\nkickoff_time         0\nminutes              0\nown_goals            0\npenalties_missed     0\npenalties_saved      0\nred_cards            0\nsaves                0\nselected             0\nteam_a_score         0\nteam_h_score         0\nthreat               0\ntotal_points         0\ntransfers_balance    0\ntransfers_in         0\ntransfers_out        0\nvalue                0\nyellow_cards         0\nGW                   0\nform                 0\nhome_team            0\naway_team            0\ndtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:35.288182Z","iopub.execute_input":"2025-12-16T06:59:35.288583Z","iopub.status.idle":"2025-12-16T06:59:35.310286Z","shell.execute_reply.started":"2025-12-16T06:59:35.288550Z","shell.execute_reply":"2025-12-16T06:59:35.309080Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"    season            name position  assists  bonus  bps  clean_sheets  \\\n0  2021-22  Aaron Connolly      FWD        0      0    0             0   \n1  2021-22  Aaron Connolly      FWD        0      0   -2             0   \n2  2021-22  Aaron Connolly      FWD        0      0    0             0   \n3  2021-22  Aaron Connolly      FWD        0      0    0             0   \n4  2021-22  Aaron Connolly      FWD        0      0    0             0   \n\n   creativity  element  fixture  ...  total_points  transfers_balance  \\\n0         0.0       72        2  ...             0                  0   \n1         1.1       72       12  ...             1              -1682   \n2         0.0       72       22  ...             0               -737   \n3         0.0       72       32  ...             0              -3682   \n4         0.0       72       42  ...             0              -2548   \n\n   transfers_in  transfers_out value  yellow_cards  GW   form  home_team  \\\n0             0              0    55             0   1  0.000    Burnley   \n1          1899           3581    55             0   2  0.000   Brighton   \n2          2897           3634    55             0   3  0.050   Brighton   \n3          1857           5539    54             0   4  0.033  Brentford   \n4           301           2849    54             0   5  0.025   Brighton   \n\n   away_team  \n0   Brighton  \n1    Watford  \n2    Everton  \n3   Brighton  \n4  Leicester  \n\n[5 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>name</th>\n      <th>position</th>\n      <th>assists</th>\n      <th>bonus</th>\n      <th>bps</th>\n      <th>clean_sheets</th>\n      <th>creativity</th>\n      <th>element</th>\n      <th>fixture</th>\n      <th>...</th>\n      <th>total_points</th>\n      <th>transfers_balance</th>\n      <th>transfers_in</th>\n      <th>transfers_out</th>\n      <th>value</th>\n      <th>yellow_cards</th>\n      <th>GW</th>\n      <th>form</th>\n      <th>home_team</th>\n      <th>away_team</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-22</td>\n      <td>Aaron Connolly</td>\n      <td>FWD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>72</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>55</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.000</td>\n      <td>Burnley</td>\n      <td>Brighton</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-22</td>\n      <td>Aaron Connolly</td>\n      <td>FWD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-2</td>\n      <td>0</td>\n      <td>1.1</td>\n      <td>72</td>\n      <td>12</td>\n      <td>...</td>\n      <td>1</td>\n      <td>-1682</td>\n      <td>1899</td>\n      <td>3581</td>\n      <td>55</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.000</td>\n      <td>Brighton</td>\n      <td>Watford</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-22</td>\n      <td>Aaron Connolly</td>\n      <td>FWD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>72</td>\n      <td>22</td>\n      <td>...</td>\n      <td>0</td>\n      <td>-737</td>\n      <td>2897</td>\n      <td>3634</td>\n      <td>55</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.050</td>\n      <td>Brighton</td>\n      <td>Everton</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-22</td>\n      <td>Aaron Connolly</td>\n      <td>FWD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>72</td>\n      <td>32</td>\n      <td>...</td>\n      <td>0</td>\n      <td>-3682</td>\n      <td>1857</td>\n      <td>5539</td>\n      <td>54</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0.033</td>\n      <td>Brentford</td>\n      <td>Brighton</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-22</td>\n      <td>Aaron Connolly</td>\n      <td>FWD</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>72</td>\n      <td>42</td>\n      <td>...</td>\n      <td>0</td>\n      <td>-2548</td>\n      <td>301</td>\n      <td>2849</td>\n      <td>54</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0.025</td>\n      <td>Brighton</td>\n      <td>Leicester</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Create config.txt file in current directory\nconfig_content = \"\"\"URI=neo4j+s://1da86c19.databases.neo4j.io\nUSERNAME=neo4j\nPASSWORD=HA4iunTOGen7RYpeISs3ZRhcWjpcokqam9przCqCuQ8\"\"\"\n\n# Write to config.txt in current directory\nwith open('config.txt', 'w') as f:\n    f.write(config_content)\n\nprint(\"config.txt created successfully!\")\n\n# Verify creation\nimport os\nif os.path.exists('config.txt'):\n    print(\"✓ config.txt exists in:\", os.path.abspath('config.txt'))\nelse:\n    print(\"✗ config.txt was not created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:59:35.311449Z","iopub.execute_input":"2025-12-16T06:59:35.311814Z","iopub.status.idle":"2025-12-16T06:59:35.333336Z","shell.execute_reply.started":"2025-12-16T06:59:35.311783Z","shell.execute_reply":"2025-12-16T06:59:35.332224Z"}},"outputs":[{"name":"stdout","text":"config.txt created successfully!\n✓ config.txt exists in: /kaggle/working/config.txt\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nCreate Knowledge Graph for FPL (Fantasy Premier League) Data\nThis script builds a Neo4j knowledge graph from FPL two-season dataset\n\"\"\"\n\nimport pandas as pd\nimport spacy\nfrom neo4j import GraphDatabase\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\nimport numpy as np\nfrom collections import Counter, defaultdict\nimport os\n\n# ============================================================================\n# Configuration\n# ============================================================================\n\ndef load_config():\n    \"\"\"Load Neo4j configuration from config.txt\"\"\"\n    try:\n        with open('config.txt', 'r') as f:\n            lines = f.readlines()\n        config = {}\n        for line in lines:\n            if '=' in line:\n                key, value = line.strip().split('=', 1)\n                config[key] = value\n        \n        uri = config.get('URI')\n        user = config.get('USERNAME')\n        password = config.get('PASSWORD')\n        \n        if uri and user and password:\n            return uri, user, password\n        else:\n            raise ValueError(\"Missing credentials in config file\")\n    except FileNotFoundError:\n        raise FileNotFoundError(\"config.txt file not found!\")\n\n# ============================================================================\n# Data Loading\n# ============================================================================\n\ndef load_data(filepath='/kaggle/input/fpl-seasons-pop/fpl_two_seasons.csv'):\n    \"\"\"Load FPL dataset from CSV file\"\"\"\n    print(f\"Loading data from {filepath}...\")\n    df = pd.read_csv(filepath)\n    print(f\"Loaded {len(df)} records\")\n    print(\"\\nDataset Info:\")\n    df.info()\n    print(\"\\nFirst few rows:\")\n    print(df.head())\n    print(\"\\nMissing values:\")\n    print(df.isnull().sum())\n    return df\n\ndef load_team_data(filepath='/kaggle/input/fpl-seasons-pop/cleaned_merged_seasons.csv'):\n    \"\"\"Load team data for players\"\"\"\n    print(f\"Loading team data from {filepath}...\")\n    \n    # Load with explicit dtype handling for mixed types\n    team_df = pd.read_csv(filepath, low_memory=False)\n    \n    print(f\"Loaded {len(team_df)} records\")\n    print(\"\\nTeam dataset info:\")\n    team_df.info()\n    \n    # Clean and process team data\n    # Create a mapping of player to team for each season\n    # We'll need to handle the mixed types in team_x column\n    team_df['team_x'] = team_df['team_x'].astype(str).replace('nan', np.nan)\n    \n    # Remove rows where team is null\n    team_df_clean = team_df.dropna(subset=['team_x'])\n    \n    print(f\"\\nCleaned team data: {len(team_df_clean)} records\")\n    \n    # Create a mapping of (season, name) -> team\n    # Note: There might be multiple teams per season if players transferred\n    # We'll take the most frequent team for each player in each season\n    player_team_mapping = {}\n    \n    # Group by season, player name, and team\n    grouped = team_df_clean.groupby(['season_x', 'name', 'team_x']).size().reset_index(name='count')\n    \n    # For each season and player, get the team with highest count\n    for (season, player), group in grouped.groupby(['season_x', 'name']):\n        most_common_team = group.loc[group['count'].idxmax(), 'team_x']\n        player_team_mapping[(season, player)] = most_common_team\n    \n    print(f\"\\nCreated player-team mapping for {len(player_team_mapping)} player-season combinations\")\n    \n    # Also create a simple name->team mapping (most recent team)\n    # This will be used as fallback if season-specific mapping not found\n    simple_mapping = {}\n    for (season, player), team in player_team_mapping.items():\n        simple_mapping[player] = team\n    \n    print(f\"Created simple player-team mapping for {len(simple_mapping)} unique players\")\n    \n    return player_team_mapping, simple_mapping\n\n# ============================================================================\n# Knowledge Graph Construction\n# ============================================================================\n\ndef build_kg(tx, batch, player_team_mapping, simple_mapping):\n    \"\"\"Build Knowledge Graph from batch of records with team information\"\"\"\n    tx.run(\"\"\"\n    UNWIND $rows AS row\n    MERGE (s:Season {season_name: row.season})\n    WITH row, s\n    \n    MERGE (gw:Gameweek {season: row.season, GW_number: row.GW})\n    MERGE (s)-[:HAS_GW]->(gw)\n    \n    WITH row, gw\n    \n    MERGE (f:Fixture {season: row.season, fixture_number: row.fixture})\n    SET f.kickoff_time = row.kickoff_time\n    MERGE (gw)-[:HAS_FIXTURE]->(f)\n    \n    WITH row, f\n    \n    MERGE (home:Team {name: row.home_team})\n    MERGE (away:Team {name: row.away_team})\n    MERGE (f)-[:HAS_HOME_TEAM]->(home)\n    MERGE (f)-[:HAS_AWAY_TEAM]->(away)\n    \n    WITH row, f, home, away\n    \n    // Determine which team the player is on for this fixture\n    // First check if player's team matches home or away\n    WITH row, f, home, away,\n         CASE \n             WHEN row.player_team = row.home_team THEN home\n             WHEN row.player_team = row.away_team THEN away\n             ELSE null\n         END AS player_team_node\n    \n    MERGE (p:Player {player_name: row.name, player_element: row.element})\n    // Add team to player node property\n    SET p.team = row.player_team\n    \n    WITH row, p, f, player_team_node\n    \n    MATCH (pos:Position {name: row.position})\n    MERGE (p)-[:PLAYS_AS]->(pos)\n    \n    // Create relationship between player and their team\n    WITH row, p, f, player_team_node\n    WHERE player_team_node IS NOT NULL\n    MERGE (p)-[:PLAYS_FOR]->(player_team_node)\n    \n    WITH row, p, f\n    \n    MERGE (p)-[r:PLAYED_IN]->(f)\n    SET r.minutes = row.minutes,\n        r.goals_scored = row.goals_scored,\n        r.assists = row.assists,\n        r.total_points = row.total_points,\n        r.bonus = row.bonus,\n        r.clean_sheets = row.clean_sheets,\n        r.goals_conceded = row.goals_conceded,\n        r.own_goals = row.own_goals,\n        r.penalties_saved = row.penalties_saved,\n        r.penalties_missed = row.penalties_missed,\n        r.yellow_cards = row.yellow_cards,\n        r.red_cards = row.red_cards,\n        r.saves = row.saves,\n        r.bps = row.bps,\n        r.influence = row.influence,\n        r.creativity = row.creativity,\n        r.threat = row.threat,\n        r.ict_index = row.ict_index,\n        r.form = row.form\n    \"\"\", rows=batch)\n\ndef initialize_database(driver):\n    \"\"\"Clear database and create initial Position nodes\"\"\"\n    with driver.session() as session:\n        # Clear all existing data\n        session.run(\"MATCH (n) DETACH DELETE n\")\n        print(\"Database cleared\")\n        \n        # Create Position nodes\n        session.run(\"\"\"\n        UNWIND $pos AS p\n        MERGE (:Position {name: p})\n        \"\"\", pos=[\"GK\", \"DEF\", \"MID\", \"FWD\"])\n        print(\"Position nodes created\")\n\ndef populate_knowledge_graph(driver, df, player_team_mapping, simple_mapping, batch_size=500):\n    \"\"\"Populate the knowledge graph with FPL data including team information\"\"\"\n    print(f\"\\nPopulating knowledge graph with {len(df)} records...\")\n    \n    # Add team information to each row\n    print(\"Adding team information to dataset...\")\n    rows_with_teams = []\n    \n    for _, row in df.iterrows():\n        row_dict = row.to_dict()\n        \n        # Try to get team from player_team_mapping using season and name\n        team_key = (row_dict['season'], row_dict['name'])\n        \n        if team_key in player_team_mapping:\n            row_dict['player_team'] = player_team_mapping[team_key]\n        elif row_dict['name'] in simple_mapping:\n            # Fallback to simple mapping\n            row_dict['player_team'] = simple_mapping[row_dict['name']]\n        else:\n            # If no team found, use a default or try to infer from fixture\n            # Check if player's team can be inferred from home/away team based on typical patterns\n            # For now, set to unknown\n            row_dict['player_team'] = \"Unknown\"\n        \n        rows_with_teams.append(row_dict)\n    \n    print(f\"Added team information to {len(rows_with_teams)} records\")\n    \n    total_batches = (len(rows_with_teams) + batch_size - 1) // batch_size\n    \n    with driver.session() as session:\n        for i in range(0, len(rows_with_teams), batch_size):\n            batch = rows_with_teams[i:i + batch_size]\n            session.execute_write(build_kg, batch, player_team_mapping, simple_mapping)\n            batch_num = i // batch_size + 1\n            print(f\"Inserted batch {batch_num}/{total_batches} ({len(batch)} records)\")\n    \n    print(\"\\n✓ Knowledge graph population complete!\")\n    \n    # Print summary of team assignments\n    team_counts = {}\n    for row in rows_with_teams:\n        team = row['player_team']\n        team_counts[team] = team_counts.get(team, 0) + 1\n    \n    print(\"\\nTeam assignment summary:\")\n    for team, count in sorted(team_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f\"  {team}: {count} records\")\n    \n    if \"Unknown\" in team_counts:\n        unknown_pct = (team_counts[\"Unknown\"] / len(rows_with_teams)) * 100\n        print(f\"\\nNote: {team_counts['Unknown']} records ({unknown_pct:.1f}%) have unknown team\")\n\n# ============================================================================\n# Query Functions (for verification)\n# ============================================================================\n\ndef verify_team_data(driver):\n    \"\"\"Verify that team data was properly added\"\"\"\n    with driver.session() as session:\n        # Query to check player-team relationships\n        result = session.run(\"\"\"\n        MATCH (p:Player)-[:PLAYS_FOR]->(t:Team)\n        RETURN p.player_name AS player, t.name AS team, COUNT(*) as fixture_count\n        ORDER BY fixture_count DESC\n        LIMIT 10\n        \"\"\")\n        \n        print(\"\\nTop 10 player-team relationships:\")\n        for record in result:\n            print(f\"  {record['player']} -> {record['team']} ({record['fixture_count']} fixtures)\")\n        \n        # Count players with team information\n        result = session.run(\"\"\"\n        MATCH (p:Player)\n        OPTIONAL MATCH (p)-[:PLAYS_FOR]->(t:Team)\n        RETURN \n            COUNT(DISTINCT p) as total_players,\n            COUNT(DISTINCT t) as total_teams_with_relationships,\n            SUM(CASE WHEN t IS NOT NULL THEN 1 ELSE 0 END) as players_with_team_relationship\n        \"\"\")\n        \n        stats = result.single()\n        print(f\"\\nTeam relationship statistics:\")\n        print(f\"  Total players: {stats['total_players']}\")\n        print(f\"  Players with team relationship: {stats['players_with_team_relationship']}\")\n        print(f\"  Unique teams in relationships: {stats['total_teams_with_relationships']}\")\n\n# ============================================================================\n# Main Execution\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution function\"\"\"\n    print(\"=\" * 70)\n    print(\"FPL Knowledge Graph Builder with Team Information\")\n    print(\"=\" * 70)\n    \n    # Step 1: Load configuration\n    print(\"\\n[Step 1] Loading Neo4j configuration...\")\n    uri, user, password = load_config()\n    \n    # Step 2: Connect to Neo4j\n    print(\"\\n[Step 2] Connecting to Neo4j database...\")\n    driver = GraphDatabase.driver(uri, auth=(user, password))\n    print(\"Connected to Neo4j database\")\n    \n    # Step 3: Load main data\n    print(\"\\n[Step 3] Loading FPL dataset...\")\n    df = load_data('/kaggle/input/fpl-seasons-pop/fpl_two_seasons.csv')\n    \n    # Step 4: Load team data\n    print(\"\\n[Step 4] Loading team data...\")\n    player_team_mapping, simple_mapping = load_team_data('/kaggle/input/fpl-seasons-pop/cleaned_merged_seasons.csv')\n    \n    # Step 5: Initialize database\n    print(\"\\n[Step 5] Initializing database...\")\n    initialize_database(driver)\n    \n    # Step 6: Populate knowledge graph with team data\n    print(\"\\n[Step 6] Building knowledge graph with team information...\")\n    populate_knowledge_graph(driver, df, player_team_mapping, simple_mapping, batch_size=500)\n    \n    # Step 7: Verify team data\n    print(\"\\n[Step 7] Verifying team data...\")\n    verify_team_data(driver)\n    \n    # Step 8: Close connection\n    print(\"\\n[Step 8] Closing connection...\")\n    driver.close()\n    print(\" Connection closed\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"Knowledge Graph Creation Complete!\")\n    print(\"=\" * 70)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:01:12.861087Z","iopub.execute_input":"2025-12-16T07:01:12.861585Z","iopub.status.idle":"2025-12-16T07:02:51.026349Z","shell.execute_reply.started":"2025-12-16T07:01:12.861558Z","shell.execute_reply":"2025-12-16T07:02:51.024397Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nFPL Knowledge Graph Builder with Team Information\n======================================================================\n\n[Step 1] Loading Neo4j configuration...\n\n[Step 2] Connecting to Neo4j database...\nConnected to Neo4j database\n\n[Step 3] Loading FPL dataset...\nLoading data from /kaggle/input/fpl-seasons-pop/fpl_two_seasons.csv...\nLoaded 51952 records\n\nDataset Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51952 entries, 0 to 51951\nData columns (total 35 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   season             51952 non-null  object \n 1   name               51952 non-null  object \n 2   position           51952 non-null  object \n 3   assists            51952 non-null  int64  \n 4   bonus              51952 non-null  int64  \n 5   bps                51952 non-null  int64  \n 6   clean_sheets       51952 non-null  int64  \n 7   creativity         51952 non-null  float64\n 8   element            51952 non-null  int64  \n 9   fixture            51952 non-null  int64  \n 10  goals_conceded     51952 non-null  int64  \n 11  goals_scored       51952 non-null  int64  \n 12  ict_index          51952 non-null  float64\n 13  influence          51952 non-null  float64\n 14  kickoff_time       51952 non-null  object \n 15  minutes            51952 non-null  int64  \n 16  own_goals          51952 non-null  int64  \n 17  penalties_missed   51952 non-null  int64  \n 18  penalties_saved    51952 non-null  int64  \n 19  red_cards          51952 non-null  int64  \n 20  saves              51952 non-null  int64  \n 21  selected           51952 non-null  int64  \n 22  team_a_score       51952 non-null  int64  \n 23  team_h_score       51952 non-null  int64  \n 24  threat             51952 non-null  int64  \n 25  total_points       51952 non-null  int64  \n 26  transfers_balance  51952 non-null  int64  \n 27  transfers_in       51952 non-null  int64  \n 28  transfers_out      51952 non-null  int64  \n 29  value              51952 non-null  int64  \n 30  yellow_cards       51952 non-null  int64  \n 31  GW                 51952 non-null  int64  \n 32  form               51952 non-null  float64\n 33  home_team          51952 non-null  object \n 34  away_team          51952 non-null  object \ndtypes: float64(4), int64(25), object(6)\nmemory usage: 13.9+ MB\n\nFirst few rows:\n    season            name position  assists  bonus  bps  clean_sheets  \\\n0  2021-22  Aaron Connolly      FWD        0      0    0             0   \n1  2021-22  Aaron Connolly      FWD        0      0   -2             0   \n2  2021-22  Aaron Connolly      FWD        0      0    0             0   \n3  2021-22  Aaron Connolly      FWD        0      0    0             0   \n4  2021-22  Aaron Connolly      FWD        0      0    0             0   \n\n   creativity  element  fixture  ...  total_points  transfers_balance  \\\n0         0.0       72        2  ...             0                  0   \n1         1.1       72       12  ...             1              -1682   \n2         0.0       72       22  ...             0               -737   \n3         0.0       72       32  ...             0              -3682   \n4         0.0       72       42  ...             0              -2548   \n\n   transfers_in  transfers_out value  yellow_cards  GW   form  home_team  \\\n0             0              0    55             0   1  0.000    Burnley   \n1          1899           3581    55             0   2  0.000   Brighton   \n2          2897           3634    55             0   3  0.050   Brighton   \n3          1857           5539    54             0   4  0.033  Brentford   \n4           301           2849    54             0   5  0.025   Brighton   \n\n   away_team  \n0   Brighton  \n1    Watford  \n2    Everton  \n3   Brighton  \n4  Leicester  \n\n[5 rows x 35 columns]\n\nMissing values:\nseason               0\nname                 0\nposition             0\nassists              0\nbonus                0\nbps                  0\nclean_sheets         0\ncreativity           0\nelement              0\nfixture              0\ngoals_conceded       0\ngoals_scored         0\nict_index            0\ninfluence            0\nkickoff_time         0\nminutes              0\nown_goals            0\npenalties_missed     0\npenalties_saved      0\nred_cards            0\nsaves                0\nselected             0\nteam_a_score         0\nteam_h_score         0\nthreat               0\ntotal_points         0\ntransfers_balance    0\ntransfers_in         0\ntransfers_out        0\nvalue                0\nyellow_cards         0\nGW                   0\nform                 0\nhome_team            0\naway_team            0\ndtype: int64\n\n[Step 4] Loading team data...\nLoading team data from /kaggle/input/fpl-seasons-pop/cleaned_merged_seasons.csv...\nLoaded 96169 records\n\nTeam dataset info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 96169 entries, 0 to 96168\nData columns (total 37 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   season_x           96169 non-null  object \n 1   name               96169 non-null  object \n 2   position           96169 non-null  object \n 3   team_x             76317 non-null  object \n 4   assists            96169 non-null  int64  \n 5   bonus              96169 non-null  int64  \n 6   bps                96169 non-null  int64  \n 7   clean_sheets       96169 non-null  int64  \n 8   creativity         96169 non-null  float64\n 9   element            96169 non-null  int64  \n 10  fixture            96169 non-null  int64  \n 11  goals_conceded     96169 non-null  int64  \n 12  goals_scored       96169 non-null  int64  \n 13  ict_index          96169 non-null  float64\n 14  influence          96169 non-null  float64\n 15  kickoff_time       96169 non-null  object \n 16  minutes            96169 non-null  int64  \n 17  opponent_team      96169 non-null  int64  \n 18  opp_team_name      96169 non-null  object \n 19  own_goals          96169 non-null  int64  \n 20  penalties_missed   96169 non-null  int64  \n 21  penalties_saved    96169 non-null  int64  \n 22  red_cards          96169 non-null  int64  \n 23  round              96169 non-null  int64  \n 24  saves              96169 non-null  int64  \n 25  selected           96169 non-null  int64  \n 26  team_a_score       96169 non-null  float64\n 27  team_h_score       96169 non-null  float64\n 28  threat             96169 non-null  float64\n 29  total_points       96169 non-null  int64  \n 30  transfers_balance  96169 non-null  int64  \n 31  transfers_in       96169 non-null  int64  \n 32  transfers_out      96169 non-null  int64  \n 33  value              96169 non-null  int64  \n 34  was_home           96169 non-null  bool   \n 35  yellow_cards       96169 non-null  int64  \n 36  GW                 96169 non-null  int64  \ndtypes: bool(1), float64(6), int64(24), object(6)\nmemory usage: 26.5+ MB\n\nCleaned team data: 76317 records\n\nCreated player-team mapping for 2224 player-season combinations\nCreated simple player-team mapping for 1327 unique players\n\n[Step 5] Initializing database...\nDatabase cleared\nPosition nodes created\n\n[Step 6] Building knowledge graph with team information...\n\nPopulating knowledge graph with 51952 records...\nAdding team information to dataset...\nAdded team information to 51952 records\nInserted batch 1/104 (500 records)\nInserted batch 2/104 (500 records)\nInserted batch 3/104 (500 records)\nInserted batch 4/104 (500 records)\nInserted batch 5/104 (500 records)\nInserted batch 6/104 (500 records)\nInserted batch 7/104 (500 records)\nInserted batch 8/104 (500 records)\nInserted batch 9/104 (500 records)\nInserted batch 10/104 (500 records)\nInserted batch 11/104 (500 records)\nInserted batch 12/104 (500 records)\nInserted batch 13/104 (500 records)\nInserted batch 14/104 (500 records)\nInserted batch 15/104 (500 records)\nInserted batch 16/104 (500 records)\nInserted batch 17/104 (500 records)\nInserted batch 18/104 (500 records)\nInserted batch 19/104 (500 records)\nInserted batch 20/104 (500 records)\nInserted batch 21/104 (500 records)\nInserted batch 22/104 (500 records)\nInserted batch 23/104 (500 records)\nInserted batch 24/104 (500 records)\nInserted batch 25/104 (500 records)\nInserted batch 26/104 (500 records)\nInserted batch 27/104 (500 records)\nInserted batch 28/104 (500 records)\nInserted batch 29/104 (500 records)\nInserted batch 30/104 (500 records)\nInserted batch 31/104 (500 records)\nInserted batch 32/104 (500 records)\nInserted batch 33/104 (500 records)\nInserted batch 34/104 (500 records)\nInserted batch 35/104 (500 records)\nInserted batch 36/104 (500 records)\nInserted batch 37/104 (500 records)\nInserted batch 38/104 (500 records)\nInserted batch 39/104 (500 records)\nInserted batch 40/104 (500 records)\nInserted batch 41/104 (500 records)\nInserted batch 42/104 (500 records)\nInserted batch 43/104 (500 records)\nInserted batch 44/104 (500 records)\nInserted batch 45/104 (500 records)\nInserted batch 46/104 (500 records)\nInserted batch 47/104 (500 records)\nInserted batch 48/104 (500 records)\nInserted batch 49/104 (500 records)\nInserted batch 50/104 (500 records)\nInserted batch 51/104 (500 records)\nInserted batch 52/104 (500 records)\nInserted batch 53/104 (500 records)\nInserted batch 54/104 (500 records)\nInserted batch 55/104 (500 records)\nInserted batch 56/104 (500 records)\nInserted batch 57/104 (500 records)\nInserted batch 58/104 (500 records)\nInserted batch 59/104 (500 records)\nInserted batch 60/104 (500 records)\nInserted batch 61/104 (500 records)\nInserted batch 62/104 (500 records)\nInserted batch 63/104 (500 records)\nInserted batch 64/104 (500 records)\nInserted batch 65/104 (500 records)\nInserted batch 66/104 (500 records)\nInserted batch 67/104 (500 records)\nInserted batch 68/104 (500 records)\nInserted batch 69/104 (500 records)\nInserted batch 70/104 (500 records)\nInserted batch 71/104 (500 records)\nInserted batch 72/104 (500 records)\nInserted batch 73/104 (500 records)\nInserted batch 74/104 (500 records)\nInserted batch 75/104 (500 records)\nInserted batch 76/104 (500 records)\nInserted batch 77/104 (500 records)\nInserted batch 78/104 (500 records)\nInserted batch 79/104 (500 records)\nInserted batch 80/104 (500 records)\nInserted batch 81/104 (500 records)\nInserted batch 82/104 (500 records)\nInserted batch 83/104 (500 records)\nInserted batch 84/104 (500 records)\nInserted batch 85/104 (500 records)\nInserted batch 86/104 (500 records)\nInserted batch 87/104 (500 records)\nInserted batch 88/104 (500 records)\nInserted batch 89/104 (500 records)\nInserted batch 90/104 (500 records)\nInserted batch 91/104 (500 records)\nInserted batch 92/104 (500 records)\nInserted batch 93/104 (500 records)\nInserted batch 94/104 (500 records)\nInserted batch 95/104 (500 records)\nInserted batch 96/104 (500 records)\nInserted batch 97/104 (500 records)\nInserted batch 98/104 (500 records)\nInserted batch 99/104 (500 records)\nInserted batch 100/104 (500 records)\nInserted batch 101/104 (500 records)\nInserted batch 102/104 (500 records)\nInserted batch 103/104 (500 records)\nInserted batch 104/104 (452 records)\n\n✓ Knowledge graph population complete!\n\nTeam assignment summary:\n  Everton: 2868 records\n  Arsenal: 2784 records\n  Aston Villa: 2778 records\n  Liverpool: 2716 records\n  Brighton: 2669 records\n  Leeds: 2649 records\n  Chelsea: 2642 records\n  Brentford: 2634 records\n  Wolves: 2572 records\n  Spurs: 2484 records\n\n[Step 7] Verifying team data...\n\nTop 10 player-team relationships:\n  Ben Davies -> Liverpool (4 fixtures)\n  Jakub Moder -> Brighton (2 fixtures)\n  Adam Lallana -> Brighton (2 fixtures)\n  Ed Turns -> Brighton (2 fixtures)\n  Evan Ferguson -> Brighton (2 fixtures)\n  Haydon Roberts -> Brighton (2 fixtures)\n  Alexis Mac Allister -> Brighton (2 fixtures)\n  Adam Webster -> Brighton (2 fixtures)\n  Danny Welbeck -> Brighton (2 fixtures)\n  Enock Mwepu -> Brighton (2 fixtures)\n\nTeam relationship statistics:\n  Total players: 1513\n  Players with team relationship: 1513\n  Unique teams in relationships: 23\n\n[Step 8] Closing connection...\n Connection closed\n\n======================================================================\nKnowledge Graph Creation Complete!\n======================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile fpl_input_preprocessing.py\n\n\n\"\"\"\nEnhanced FPL Input Preprocessor with Robust LLM-based Multi-Entity Extraction\n\"\"\"\n\nimport re\nimport json\nfrom typing import Dict, List, Tuple, Optional\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom collections import Counter\nfrom huggingface_hub import InferenceClient\n\n\nclass FPLInputPreprocessorLLM:\n    \"\"\"\n    Enhanced FPL preprocessor with improved LLM-based multi-entity extraction\n    \"\"\"\n    \n    def __init__(self, \n                 hf_token: str,\n                 llm_model: str = \"google/gemma-2-2b-it\",\n                 embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n                 use_llm: bool = True):\n        \"\"\"Initialize the preprocessor\"\"\"\n        print(\"🚀 Initializing FPL Input Preprocessor (Enhanced)...\")\n        \n        self.use_llm = use_llm\n        \n        # Initialize LLM client if enabled\n        if use_llm:\n            try:\n                self.llm_client = InferenceClient(model=llm_model, token=hf_token)\n                print(f\"✅ Connected to LLM: {llm_model}\")\n            except Exception as e:\n                print(f\"⚠️ LLM connection failed: {e}\")\n                print(\"⚠️ Falling back to rule-based approach\")\n                self.use_llm = False\n        \n        # Initialize embedding model\n        self.embedder = SentenceTransformer(embedding_model_name)\n        print(f\"✅ Loaded embedding model: {embedding_model_name}\")\n        \n        # Define intent types\n        self.intent_types = [\n            'player_search',\n            'performance_query', \n            'comparison',\n            'recommendation',\n            'team_analysis',\n            'fixture_query',\n            'value_analysis',\n            'form_query',\n            'general_query'\n        ]\n        \n        self._init_rule_based_entities()\n        self._init_player_database()\n        self.error_log = []\n        print(\"✅ System initialized\\n\")\n    \n    def _init_rule_based_entities(self):\n        \"\"\"Initialize rule-based entity dictionaries\"\"\"\n        self.positions = {\n            'GK': ['gk', 'goalkeeper', 'keeper', 'goalie', 'keepers', 'goalkeepers'],\n            'DEF': ['def', 'defender', 'defence', 'defense', 'back', 'defenders', 'defensive'],\n            'MID': ['mid', 'midfielder', 'midfield', 'midfielders'],\n            'FWD': ['fwd', 'forward', 'striker', 'attacker', 'attack', 'forwards', 'strikers', 'attackers']\n        }\n        \n        self.teams = {\n            'Arsenal': ['arsenal', 'ars', 'gunners'],\n            'Aston Villa': ['aston villa', 'villa', 'avl'],\n            'Bournemouth': ['bournemouth', 'bou', 'cherries'],\n            'Brentford': ['brentford', 'bre', 'bees'],\n            'Brighton': ['brighton', 'bha', 'seagulls'],\n            'Burnley': ['burnley', 'bur', 'clarets'],\n            'Chelsea': ['chelsea', 'che', 'blues'],\n            'Crystal Palace': ['crystal palace', 'palace', 'cry', 'eagles'],\n            'Everton': ['everton', 'eve', 'toffees'],\n            'Fulham': ['fulham', 'ful', 'cottagers'],\n            'Liverpool': ['liverpool', 'liv', 'reds'],\n            'Luton': ['luton', 'lut', 'hatters'],\n            'Man City': ['man city', 'manchester city', 'city', 'mci', 'mcfc'],\n            'Man Utd': ['man utd', 'manchester united', 'united', 'mun', 'mufc'],\n            'Newcastle': ['newcastle', 'new', 'magpies', 'toon'],\n            'Nottingham Forest': ['nottingham', 'forest', 'nfo', 'nottm forest'],\n            'Sheffield Utd': ['sheffield', 'sheffield united', 'shu', 'blades'],\n            'Tottenham': ['tottenham', 'spurs', 'tot', 'thfc'],\n            'West Ham': ['west ham', 'whu', 'hammers'],\n            'Wolves': ['wolves', 'wolverhampton', 'wol'],\n            'Leeds': ['leeds', 'leeds united', 'lee', 'lufc'],\n            'Leicester': ['leicester', 'leicester city', 'lei', 'lcfc', 'foxes'],\n            'Southampton': ['southampton', 'saints', 'sou'],\n        }\n        \n        self.metrics = {\n            'total_points': ['points', 'total points', 'score', 'scoring'],\n            'goals_scored': ['goals', 'goal', 'scored', 'scorer', 'scorers', 'scoring goals'],\n            'assists': ['assists', 'assist', 'assisting', 'assist provider'],\n            'bonus': ['bonus', 'bonus points', 'bps'],\n            'minutes': ['minutes', 'playing time', 'game time'],\n            'clean_sheets': ['clean sheets', 'clean sheet', 'cs', 'cleansheet', 'cleansheets'],\n            'saves': ['saves', 'save', 'saved'],\n            'ict_index': ['ict', 'ict index'],\n            'influence': ['influence', 'influential'],\n            'creativity': ['creativity', 'creative'],\n            'threat': ['threat', 'threatening'],\n            'form': ['form', 'recent form', 'hot', 'recent', 'lately', 'in form'],\n            'cards': ['cards', 'yellow cards', 'red cards', 'yellow', 'red', 'disciplinary'],\n            'goals_conceded': ['goals conceded', 'conceded', 'goals against'],\n            'own_goals': ['own goals', 'own goal', 'og'],\n            'penalties_saved': ['penalties saved', 'penalty saves', 'pen saves'],\n            'penalties_missed': ['penalties missed', 'penalty misses', 'pen misses'],\n            'yellow_cards': ['yellow cards', 'yellows', 'bookings'],\n            'red_cards': ['red cards', 'reds', 'sent off', 'dismissals'],\n            'bps': ['bps', 'bonus point system'],\n            'value': ['value', 'price', 'cost', 'worth', 'bargain', 'expensive'],\n            'ownership': ['ownership', 'owned', 'selected', '% owned']\n        }\n    \n    def _init_player_database(self):\n        \"\"\"Initialize comprehensive player database with variations\"\"\"\n        self.known_players = {\n            # Top Forwards\n            'Erling Haaland': ['haaland', 'erling', 'erling haaland'],\n            'Harry Kane': ['kane', 'harry kane'],\n            'Ivan Toney': ['toney', 'ivan toney'],\n            'Alexander Isak': ['isak', 'alexander isak'],\n            'Ollie Watkins': ['watkins', 'ollie watkins', 'oliver watkins'],\n            'Darwin Nunez': ['nunez', 'darwin', 'darwin nunez'],\n            'Dominic Calvert-Lewin': ['calvert-lewin', 'dcl', 'dominic calvert-lewin'],\n            'Callum Wilson': ['wilson', 'callum wilson'],\n            \n            # Top Midfielders\n            'Mohamed Salah': ['salah', 'mo salah', 'mohamed salah'],\n            'Kevin De Bruyne': ['de bruyne', 'kdb', 'kevin de bruyne'],\n            'Heung-Min Son': ['son', 'heung-min son', 'sonny'],\n            'Bukayo Saka': ['saka', 'bukayo saka'],\n            'Phil Foden': ['foden', 'phil foden'],\n            'Bruno Fernandes': ['bruno', 'fernandes', 'bruno fernandes'],\n            'Martin Odegaard': ['odegaard', 'martin odegaard'],\n            'James Maddison': ['maddison', 'james maddison', 'madders'],\n            'Marcus Rashford': ['rashford', 'marcus rashford'],\n            'Jack Grealish': ['grealish', 'jack grealish'],\n            \n            # Top Defenders\n            'Trent Alexander-Arnold': ['trent', 'taa', 'alexander-arnold', 'trent alexander-arnold'],\n            'Reece James': ['reece james', 'reece'],\n            'Kieran Trippier': ['trippier', 'kieran trippier'],\n            'Andrew Robertson': ['robertson', 'andy robertson', 'andrew robertson', 'robbo'],\n            'Virgil van Dijk': ['van dijk', 'virgil', 'virgil van dijk', 'vvd'],\n            'William Saliba': ['saliba', 'william saliba'],\n            'Ruben Dias': ['dias', 'ruben dias'],\n            'Gabriel Magalhaes': ['gabriel', 'gabriel magalhaes'],\n            \n            # Top Goalkeepers\n            'Alisson': ['alisson', 'alisson becker'],\n            'Ederson': ['ederson', 'ederson moraes'],\n            'Aaron Ramsdale': ['ramsdale', 'aaron ramsdale'],\n            'Nick Pope': ['pope', 'nick pope'],\n            'David Raya': ['raya', 'david raya'],\n            'Robert Sanchez': ['sanchez', 'robert sanchez']\n        }\n        \n        # Create reverse mapping for quick lookup\n        self.player_variants = {}\n        for player, variants in self.known_players.items():\n            for variant in variants:\n                self.player_variants[variant] = player\n    \n    def classify_intent(self, text: str) -> Tuple[str, float]:\n        \"\"\"Enhanced intent classification with LLM support\"\"\"\n        text_lower = text.lower()\n        \n        # ===== EMERGENCY HOTFIX =====\n        # Force performance_query for any question about specific gameweek performance\n        gameweek_patterns = [\n            r'performance.*gameweek.*\\d+',\n            r'gameweek.*\\d+.*performance',\n            r'how.*did.*gameweek.*\\d+',\n            r'what.*was.*gameweek.*\\d+'\n        ]\n        for pattern in gameweek_patterns:\n           if re.search(pattern, text_lower, re.IGNORECASE):\n                return 'performance_query', 0.99  # Highest confidence\n    # ===== END HOTFIX =====\n        # Rule-based classification (fast path)\n        \n        # Top/best/leading queries\n        top_keywords = ['top', 'best', 'leading', 'highest', 'most', 'who scored', 'who has']\n        if any(keyword in text_lower for keyword in top_keywords):\n            if any(word in text_lower for word in ['scorer', 'scorers', 'goals', 'goal']):\n                return 'recommendation', 0.95\n            if any(word in text_lower for word in ['assist', 'assists']):\n                return 'recommendation', 0.95\n            if any(word in text_lower for word in ['clean sheet', 'cleansheet']):\n                return 'recommendation', 0.95\n            if any(word in text_lower for word in ['bonus', 'bonus points']):\n                return 'recommendation', 0.95\n            return 'recommendation', 0.90\n        \n        # Comparison queries (check for multiple entities)\n        comparison_keywords = ['compare', 'vs', 'versus', 'between', 'or', 'and', 'versus']\n        if any(word in text_lower for word in comparison_keywords):\n            # Check if there are at least two potential entities for comparison\n            potential_players = self._extract_capitalized_names(text)\n            if len(potential_players) >= 2 or len(re.findall(r'\\b(vs|versus|compare)\\b', text_lower)) > 0:\n                return 'comparison', 0.95\n            return 'comparison', 0.85\n        \n        # Fixture queries\n        if any(word in text_lower for word in ['fixture', 'fixtures', 'schedule', 'match', 'matches', 'playing against', 'upcoming']):\n            return 'fixture_query', 0.90\n        \n        # Team analysis queries\n        if any(word in text_lower for word in ['players for', 'play for', 'roster', 'squad', 'team', 'from arsenal', 'from liverpool']):\n            return 'team_analysis', 0.85\n        \n        # Form queries\n        if any(word in text_lower for word in ['form', 'recent', 'lately', 'hot', 'in form', 'last 5', 'recently']):\n            return 'form_query', 0.85\n        \n        # Performance queries (gameweek-specific)\n        if 'gameweek' in text_lower or 'gw' in text_lower:\n            return 'performance_query', 0.85\n        \n        # Player search (specific player names)\n        if any(player.lower() in text_lower for variants in self.known_players.values() for player in variants):\n            if 'gameweek' in text_lower or 'gw' in text_lower:\n                return 'performance_query', 0.90\n            return 'player_search', 0.80\n        \n        # Value analysis\n        if any(word in text_lower for word in ['value', 'price', 'cost', 'worth', 'bargain', 'differential', 'cheap', 'expensive']):\n            return 'value_analysis', 0.85\n        \n        # Use LLM for ambiguous cases\n        if self.use_llm:\n            try:\n                return self._classify_intent_with_llm(text)\n            except Exception as e:\n                self.error_log.append({'type': 'llm_classification_error', 'error': str(e)})\n        \n        return 'general_query', 0.30\n    \n    def _classify_intent_with_llm(self, text: str) -> Tuple[str, float]:\n        \"\"\"Use LLM for intent classification\"\"\"\n        prompt = f\"\"\"Analyze this Fantasy Premier League query and classify its intent:\n\nQuery: \"{text}\"\n\nAvailable intents:\n- player_search: Finding specific player information (e.g., \"Haaland stats\", \"Salah price\")\n- performance_query: Player performance in specific gameweek/match (e.g., \"Salah points GW5\", \"Haaland gameweek 3\")\n- comparison: Comparing multiple players or teams (e.g., \"Haaland vs Salah\", \"compare TAA and Robertson\")\n- recommendation: Finding top/best players (e.g., \"top scorers\", \"best defenders\", \"highest points\")\n- team_analysis: Team-related queries (e.g., \"Arsenal players\", \"Liverpool squad\")\n- fixture_query: Match schedule and fixtures (e.g., \"Man City fixtures\", \"upcoming matches\")\n- value_analysis: Player value and pricing (e.g., \"best value defenders\", \"Haaland worth it\")\n- form_query: Recent player form (e.g., \"players in form\", \"recent performance\")\n- general_query: General questions\n\nDetermine the SINGLE most appropriate intent. Return format: intent,confidence (0.0-1.0)\n\nResponse:\"\"\"\n        \n        response = self.llm_client.text_generation(prompt, max_new_tokens=50)\n        parts = response.strip().split(',')\n        if len(parts) == 2:\n            intent = parts[0].strip().lower()\n            try:\n                confidence = float(parts[1].strip())\n                # Validate intent\n                if intent in self.intent_types:\n                    return intent, confidence\n            except ValueError:\n                pass\n        return 'general_query', 0.30\n    \n    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n        \"\"\"Enhanced multi-entity extraction with improved LLM support\"\"\"\n        # Always run rule-based extraction first for robustness\n        rule_entities = self._extract_entities_rule_based(text)\n        \n        # Use LLM for enhanced extraction if available\n        if self.use_llm:\n            try:\n                llm_entities = self._extract_entities_with_llm(text)\n                return self._merge_entities(llm_entities, rule_entities)\n            except Exception as e:\n                self.error_log.append({'type': 'llm_extraction_error', 'error': str(e)})\n        \n        return rule_entities\n    \n    def _extract_entities_with_llm(self, text: str) -> Dict[str, List[str]]:\n        \"\"\"Use LLM for comprehensive entity extraction with multi-entity support\"\"\"\n        prompt = f\"\"\"Extract ALL entities from this Fantasy Premier League query.\n\nQuery: \"{text}\"\n\nExtract entities for ALL these categories:\n1. players: ALL player names mentioned (extract ALL names)\n2. teams: ALL team names mentioned\n3. positions: GK, DEF, MID, FWD (only these values)\n4. seasons: Year ranges like 2023-24, 2023/24, or single years\n5. gameweeks: Gameweek numbers (extract ALL numbers mentioned as gameweeks)\n6. metrics: Statistics mentioned (goals, assists, points, clean_sheets, bonus, form, etc.)\n7. numerical_values: Any numbers that are thresholds, counts, or values\n8. comparators: Comparison words (more than, less than, over, under, above, below)\n\nCRITICAL INSTRUCTIONS:\n- Extract ALL instances of each entity type. If multiple players are mentioned, extract ALL of them.\n- For players: Use full canonical names when possible (e.g., \"Erling Haaland\" not just \"Haaland\")\n- For teams: Use full canonical names (e.g., \"Man City\" not just \"City\")\n- For metrics: Use standardized names from: total_points, goals_scored, assists, bonus, minutes, clean_sheets, saves, ict_index, influence, creativity, threat, form, cards, goals_conceded, own_goals, penalties_saved, penalties_missed, yellow_cards, red_cards, bps, value, ownership\n- Return ONLY valid JSON with exactly these 8 keys.\n\nExample response for \"Compare Haaland and Salah goals vs assists in gameweek 5\":\n{{\n  \"players\": [\"Erling Haaland\", \"Mohamed Salah\"],\n  \"teams\": [],\n  \"positions\": [],\n  \"seasons\": [],\n  \"gameweeks\": [\"5\"],\n  \"metrics\": [\"goals_scored\", \"assists\"],\n  \"numerical_values\": [],\n  \"comparators\": []\n}}\n\nNow extract entities from the query above:\"\"\"\n\n        try:\n            response = self.llm_client.text_generation(prompt, max_new_tokens=500)\n            \n            # Extract JSON from response\n            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n            if json_match:\n                try:\n                    entities = json.loads(json_match.group())\n                    \n                    # Validate and clean entities\n                    cleaned_entities = self._clean_llm_entities(entities, text)\n                    \n                    # Ensure all required keys exist\n                    required_keys = ['players', 'teams', 'positions', 'seasons', 'gameweeks', \n                                   'metrics', 'numerical_values', 'comparators']\n                    for key in required_keys:\n                        if key not in cleaned_entities:\n                            cleaned_entities[key] = []\n                        elif not isinstance(cleaned_entities[key], list):\n                            cleaned_entities[key] = [str(cleaned_entities[key])]\n                    \n                    return cleaned_entities\n                except json.JSONDecodeError as e:\n                    self.error_log.append({\n                        'type': 'llm_json_parse_error',\n                        'response': response[:200],\n                        'error': str(e)\n                    })\n        except Exception as e:\n            self.error_log.append({\n                'type': 'llm_extraction_failure',\n                'error': str(e)\n            })\n        \n        return {\n            'players': [], 'teams': [], 'positions': [], 'seasons': [],\n            'gameweeks': [], 'metrics': [], 'numerical_values': [], 'comparators': []\n        }\n    \n    def _clean_llm_entities(self, entities: Dict, original_text: str) -> Dict:\n        \"\"\"Clean and validate LLM-extracted entities\"\"\"\n        cleaned = {}\n        text_lower = original_text.lower()\n        \n        for key, values in entities.items():\n            if not isinstance(values, list):\n                values = [values] if values else []\n            \n            cleaned_values = []\n            seen = set()\n            \n            for value in values:\n                if not value:\n                    continue\n                \n                # Convert to string and strip whitespace\n                str_value = str(value).strip()\n                \n                # Deduplicate (case-insensitive)\n                val_lower = str_value.lower()\n                if val_lower in seen:\n                    continue\n                \n                seen.add(val_lower)\n                \n                # Validate specific entity types\n                if key == 'players':\n                    # Try to map to canonical player name\n                    canonical_name = self._map_to_canonical_player(str_value)\n                    if canonical_name:\n                        cleaned_values.append(canonical_name)\n                    elif len(str_value.split()) >= 2:  # Likely a player name\n                        cleaned_values.append(str_value)\n                \n                elif key == 'teams':\n                    # Map team aliases to canonical names\n                    canonical_team = self._map_to_canonical_team(str_value)\n                    if canonical_team:\n                        cleaned_values.append(canonical_team)\n                \n                elif key == 'positions':\n                    # Standardize position values\n                    pos = str_value.upper()\n                    if pos in ['GK', 'DEF', 'MID', 'FWD']:\n                        cleaned_values.append(pos)\n                \n                elif key == 'metrics':\n                    # Standardize metric names\n                    metric = self._map_to_canonical_metric(str_value)\n                    if metric:\n                        cleaned_values.append(metric)\n                \n                elif key == 'gameweeks':\n                    # Extract numbers only\n                    match = re.search(r'(\\d+)', str_value)\n                    if match:\n                        cleaned_values.append(match.group(1))\n                \n                elif key == 'numerical_values':\n                    try:\n                        num = float(str_value)\n                        cleaned_values.append(num)\n                    except ValueError:\n                        pass\n                \n                else:\n                    # Keep other values as-is\n                    cleaned_values.append(str_value)\n            \n            cleaned[key] = cleaned_values\n        \n        return cleaned\n    \n    def _map_to_canonical_player(self, player_name: str) -> Optional[str]:\n        \"\"\"Map player name or alias to canonical name\"\"\"\n        name_lower = player_name.lower()\n        \n        # Check exact variants first\n        if name_lower in self.player_variants:\n            return self.player_variants[name_lower]\n        \n        # Check partial matches\n        for canonical, variants in self.known_players.items():\n            if name_lower in canonical.lower() or any(variant in name_lower for variant in variants):\n                return canonical\n        \n        # Check if name contains known player last names\n        words = name_lower.split()\n        for word in words:\n            for canonical, variants in self.known_players.items():\n                if any(word == v for v in variants):\n                    return canonical\n        \n        return None\n    \n    def _map_to_canonical_team(self, team_name: str) -> Optional[str]:\n        \"\"\"Map team alias to canonical name\"\"\"\n        team_lower = team_name.lower()\n        for canonical, variants in self.teams.items():\n            if team_lower == canonical.lower() or team_lower in variants:\n                return canonical\n        return None\n    \n    def _map_to_canonical_metric(self, metric_name: str) -> Optional[str]:\n        \"\"\"Map metric name to canonical form\"\"\"\n        metric_lower = metric_name.lower()\n        for canonical, variants in self.metrics.items():\n            if metric_lower == canonical or metric_lower in variants:\n                return canonical\n        return None\n    \n    def _extract_entities_rule_based(self, text: str) -> Dict[str, List[str]]:\n        \"\"\"Enhanced rule-based entity extraction with multi-entity support\"\"\"\n        entities = {\n            'players': [],\n            'teams': [],\n            'positions': [],\n            'seasons': [],\n            'gameweeks': [],\n            'metrics': [],\n            'numerical_values': [],\n            'comparators': []\n        }\n        \n        text_lower = text.lower()\n        \n        # Extract ALL metrics mentioned\n        for canonical_metric, variants in self.metrics.items():\n            for variant in variants:\n                # Use word boundary matching for better accuracy\n                pattern = r'\\b' + re.escape(variant) + r'\\b'\n                if re.search(pattern, text_lower):\n                    if canonical_metric not in entities['metrics']:\n                        entities['metrics'].append(canonical_metric)\n                    break\n        \n        # Extract ALL known players (improved detection)\n        for canonical_name, variants in self.known_players.items():\n            for variant in variants:\n                # Use word boundary and case-insensitive matching\n                pattern = r'\\b' + re.escape(variant) + r'\\b'\n                if re.search(pattern, text_lower, re.IGNORECASE):\n                    if canonical_name not in entities['players']:\n                        entities['players'].append(canonical_name)\n                    break\n        \n        # Extract capitalized names (for players not in database)\n        if 'vs' in text_lower or 'versus' in text_lower or 'compare' in text_lower or 'and' in text:\n            potential_players = self._extract_capitalized_names(text)\n            for player in potential_players:\n                if player not in entities['players']:\n                    entities['players'].append(player)\n        \n        # Extract ALL positions\n        for canonical_pos, variants in self.positions.items():\n            for variant in variants:\n                pattern = r'\\b' + re.escape(variant) + r'\\b'\n                if re.search(pattern, text_lower):\n                    if canonical_pos not in entities['positions']:\n                        entities['positions'].append(canonical_pos)\n                    break\n        \n        # Extract ALL teams (with word boundary matching)\n        for canonical_team, variants in self.teams.items():\n            for variant in variants:\n                pattern = r'\\b' + re.escape(variant) + r'\\b'\n                if re.search(pattern, text_lower):\n                    if canonical_team not in entities['teams']:\n                        entities['teams'].append(canonical_team)\n                    break\n        \n        # Extract ALL seasons\n        season_patterns = [\n            r'20\\d{2}[-/]20?\\d{2}',  # 2022-23 or 2022/23\n            r'20\\d{2}'                # 2022\n        ]\n        for pattern in season_patterns:\n            matches = re.findall(pattern, text)\n            for match in matches:\n                if match not in entities['seasons']:\n                    entities['seasons'].append(match)\n        \n        # Extract ALL gameweeks (improved patterns)\n        gw_patterns = [\n            r'(?:gameweek|gw|week)\\s*(\\d+)',\n            r'round\\s*(\\d+)',\n            r'(\\d+)\\s*(?:st|nd|rd|th)\\s*(?:gameweek|gw|week)'\n        ]\n        for pattern in gw_patterns:\n            matches = re.findall(pattern, text_lower, re.IGNORECASE)\n            for match in matches:\n                if str(match) not in entities['gameweeks']:\n                    entities['gameweeks'].append(str(match))\n        \n        # Extract ALL comparators\n        comparators = [\n            'more than', 'less than', 'greater than', 'fewer than',\n            'under', 'over', 'above', 'below', 'at least', 'at most',\n            'higher than', 'lower than'\n        ]\n        for comp in comparators:\n            if comp in text_lower:\n                if comp not in entities['comparators']:\n                    entities['comparators'].append(comp)\n        \n        # Extract ALL numerical values (excluding seasons and gameweeks)\n        number_pattern = r'\\b(\\d+(?:\\.\\d+)?)\\b'\n        numbers = re.findall(number_pattern, text)\n        seen_numbers = set()\n        \n        for num in numbers:\n            # Skip if already captured as season or gameweek\n            is_season = any(season_pattern in num for season_pattern in ['202', '201', '200'])\n            is_gameweek = num in entities['gameweeks']\n            \n            if not is_season and not is_gameweek:\n                if num not in seen_numbers:\n                    try:\n                        entities['numerical_values'].append(float(num))\n                        seen_numbers.add(num)\n                    except ValueError:\n                        pass\n        \n        return entities\n    \n    def _extract_capitalized_names(self, text: str) -> List[str]:\n        \"\"\"Extract potential player names from capitalized words with improved logic\"\"\"\n        stopwords = {\n            'who', 'what', 'find', 'show', 'tell', 'compare', 'the', 'vs', 'v', 'and', 'or',\n            'in', 'for', 'of', 'did', 'are', 'have', 'has', 'is', 'how', 'which', 'where',\n            'when', 'why', 'do', 'does', 'received', 'earned', 'had', 'players', 'playing',\n            'play', 'between', 'against', 'premier', 'league', 'fantasy', 'fpl', 'scored',\n            'score', 'gameweek', 'season', 'team', 'teams', 'a', 'an', 'with', 'from', 'to',\n            'by', 'their', 'this', 'that', 'these', 'those', 'my', 'your', 'our', 'his', 'her'\n        }\n        \n        words = re.split(r'(\\s+|,|;|vs|versus|and|or)', text)\n        potential_players = []\n        current_name = []\n        \n        for i, word in enumerate(words):\n            clean_word = word.strip().rstrip(\"'s,?!.:;-\")\n            \n            # Check if word is a capitalized name component\n            if (len(clean_word) >= 2 and clean_word[0].isupper() and \n                clean_word.lower() not in stopwords and not clean_word.isdigit()):\n                \n                # Handle hyphenated names (e.g., Calvert-Lewin)\n                if '-' in clean_word:\n                    parts = clean_word.split('-')\n                    if all(len(p) >= 2 for p in parts):\n                        potential_players.append(clean_word)\n                        current_name = []\n                    continue\n                \n                current_name.append(clean_word)\n                \n                # Check if next word is also capitalized (for multi-word names)\n                if i + 1 < len(words):\n                    next_word = words[i + 1].strip().rstrip(\"'s,?!.:;-\")\n                    if (len(next_word) >= 2 and next_word[0].isupper() and \n                        next_word.lower() not in stopwords):\n                        continue\n                \n                # Complete the current name\n                if len(current_name) >= 1:\n                    player_name = ' '.join(current_name)\n                    if len(player_name) >= 3:  # Minimum name length\n                        potential_players.append(player_name)\n                    current_name = []\n        \n        # Remove duplicates while preserving order\n        seen = set()\n        unique_players = []\n        for player in potential_players:\n            if player.lower() not in seen:\n                unique_players.append(player)\n                seen.add(player.lower())\n        \n        return unique_players\n    \n    def _merge_entities(self, llm_entities: Dict, rule_entities: Dict) -> Dict:\n        \"\"\"Merge LLM and rule-based entities intelligently\"\"\"\n        merged = {}\n        \n        for key in rule_entities.keys():\n            llm_vals = llm_entities.get(key, [])\n            rule_vals = rule_entities.get(key, [])\n            \n            # Combine and deduplicate\n            combined = []\n            seen = set()\n            \n            # Prioritize LLM-extracted players (often more accurate)\n            if key == 'players':\n                # Add LLM players first\n                for val in llm_vals:\n                    if isinstance(val, str):\n                        val_lower = val.lower()\n                        if val_lower not in seen:\n                            combined.append(val)\n                            seen.add(val_lower)\n                \n                # Add rule-based players that weren't caught by LLM\n                for val in rule_vals:\n                    if isinstance(val, str):\n                        val_lower = val.lower()\n                        if val_lower not in seen:\n                            combined.append(val)\n                            seen.add(val_lower)\n            else:\n                # For other entities, combine all\n                all_vals = llm_vals + rule_vals\n                for val in all_vals:\n                    if isinstance(val, (str, int, float)):\n                        val_str = str(val).lower()\n                        if val_str not in seen:\n                            combined.append(val)\n                            seen.add(val_str)\n            \n            merged[key] = combined\n        \n        return merged\n    \n    def generate_embedding(self, user_input: str) -> np.ndarray:\n        \"\"\"Generate semantic text embedding vector\"\"\"\n        try:\n            embedding = self.embedder.encode([user_input], convert_to_numpy=True)\n            return embedding[0]\n        except Exception as e:\n            self.error_log.append({\n                'type': 'embedding_error',\n                'query': user_input,\n                'error': str(e)\n            })\n            return np.zeros(self.embedder.get_sentence_embedding_dimension())\n    \n    def entities_to_criteria(self, entities: Dict, position: str = None) -> Dict[str, str]:\n        \"\"\"Convert extracted entities to search criteria\"\"\"\n        criteria = {'total_points': 'high'}  # Default\n        \n        # Determine position\n        positions_list = entities.get('positions', [])\n        pos = position if position else (positions_list[0] if positions_list else None)\n        \n        # Position-specific default criteria\n        if pos == 'FWD':\n            criteria['goals'] = 'high'\n            criteria['threat'] = 'high'\n        elif pos == 'MID':\n            criteria['goals'] = 'high'\n            criteria['assists'] = 'high'\n            criteria['creativity'] = 'high'\n        elif pos == 'DEF':\n            criteria['clean_sheets'] = 'high'\n            criteria['bonus'] = 'high'\n        elif pos == 'GK':\n            criteria['clean_sheets'] = 'high'\n            criteria['bonus'] = 'high'\n            criteria['saves'] = 'high'\n        \n        # Override with explicit metrics from query\n        metrics = entities.get('metrics', [])\n        \n        metric_to_criteria = {\n            'goals_scored': 'goals',\n            'total_points': 'total_points',\n            'assists': 'assists',\n            'clean_sheets': 'clean_sheets',\n            'bonus': 'bonus',\n            'minutes': 'minutes',\n            'form': 'form',\n            'ict_index': 'ict_index',\n            'influence': 'influence',\n            'creativity': 'creativity',\n            'threat': 'threat',\n            'cards': 'cards',\n            'saves': 'saves',\n            'yellow_cards': 'cards',\n            'red_cards': 'cards',\n            'value': 'value',\n            'ownership': 'ownership'\n        }\n        \n        for metric in metrics:\n            if metric in metric_to_criteria:\n                criteria[metric_to_criteria[metric]] = 'high'\n        \n        return criteria\n    \n    def generate_numeric_embedding(self, criteria: Dict) -> np.ndarray:\n        \"\"\"Generate a 12-dimensional numeric query embedding\"\"\"\n        embedding = np.full(12, 0.5, dtype=np.float32)\n        \n        feature_map = {\n            'goals': 0, \n            'assists': 1, \n            'total_points': 2,\n            'clean_sheets': 3, \n            'minutes': 4, \n            'bonus': 5,\n            'form': 6, \n            'ict_index': 7, \n            'influence': 8, \n            'creativity': 9, \n            'threat': 10, \n            'value': 11\n        }\n        \n        for feature, value in criteria.items():\n            if feature in feature_map:\n                idx = feature_map[feature]\n                if value == 'high':\n                    embedding[idx] = 1.0\n                elif value == 'low':\n                    embedding[idx] = 0.0\n                elif isinstance(value, (int, float)):\n                    embedding[idx] = min(max(float(value), 0.0), 1.0)\n        \n        return embedding\n    \n    def preprocess(self, user_input: str, include_embedding: bool = True) -> Dict:\n        \"\"\"Complete preprocessing pipeline with enhanced entity extraction\"\"\"\n        if not user_input or not user_input.strip():\n            self.error_log.append({\n                'type': 'empty_query',\n                'query': user_input\n            })\n            return {\n                'original_query': user_input,\n                'intent': 'unknown',\n                'intent_confidence': 0.0,\n                'entities': {},\n                'text_embedding': None,\n                'numeric_embedding': None,\n                'search_criteria': {},\n                'method': 'none',\n                'error': 'Empty query provided'\n            }\n        \n        # Step 1: Classify intent\n        intent, confidence = self.classify_intent(user_input)\n        \n        # Step 2: Extract ALL entities\n        entities = self.extract_entities(user_input)\n        \n        # Step 3: Generate search criteria from entities\n        search_criteria = self.entities_to_criteria(entities)\n        \n        result = {\n            'original_query': user_input,\n            'intent': intent,\n            'intent_confidence': confidence,\n            'entities': entities,\n            'method': 'llm' if self.use_llm else 'rule-based',\n            'search_criteria': search_criteria,\n        }\n        \n        # Step 4: Generate embeddings if requested\n        if include_embedding:\n            result['text_embedding'] = self.generate_embedding(user_input)\n            result['numeric_embedding'] = self.generate_numeric_embedding(search_criteria)\n        else:\n            result['text_embedding'] = None\n            result['numeric_embedding'] = None\n        \n        result['embedding'] = result['text_embedding']\n        \n        return result\n    \n    def _convert_to_season_format(self, year_str: str) -> str:\n        \"\"\"Convert year to KG season format\"\"\"\n        try:\n            year = int(year_str)\n            next_year = str(year + 1)[-2:]\n            return f\"{year}-{next_year}\"\n        except ValueError:\n            return year_str\n    \n    def get_cypher_params(self, preprocessing_result: Dict) -> Dict[str, any]:\n        \"\"\"Convert entities to Cypher parameters with robust multi-entity support\"\"\"\n        entities = preprocessing_result['entities']\n        \n        params = {\n            'intent': preprocessing_result['intent'],\n            'intent_confidence': preprocessing_result['intent_confidence']\n        }\n        \n        # Handle multiple positions\n        if entities.get('positions'):\n            params['position'] = entities['positions'][0]\n            if len(entities['positions']) > 1:\n                params['positions'] = entities['positions']\n        \n        # Handle multiple teams\n        if entities.get('teams'):\n            params['team'] = entities['teams'][0]\n            if len(entities['teams']) > 1:\n                params['team2'] = entities['teams'][1]\n                params['teams'] = entities['teams']\n        \n        # Handle multiple seasons\n        if entities.get('seasons'):\n            raw_season = entities['seasons'][0]\n            params['season'] = self._convert_to_season_format(raw_season)\n            if len(entities['seasons']) > 1:\n                params['seasons'] = [self._convert_to_season_format(s) for s in entities['seasons']]\n        \n        # Handle multiple gameweeks\n        if entities.get('gameweeks'):\n            try:\n                params['gameweek'] = int(entities['gameweeks'][0])\n                if len(entities['gameweeks']) > 1:\n                    params['gameweeks'] = [int(gw) for gw in entities['gameweeks']]\n            except ValueError:\n                pass\n        \n        # Handle multiple metrics\n        if entities.get('metrics'):\n            params['metric'] = entities['metrics'][0]\n            if len(entities['metrics']) > 1:\n                params['metric2'] = entities['metrics'][1]\n                params['metrics'] = entities['metrics']\n        \n        # Handle multiple players (CRITICAL for comparisons)\n        if entities.get('players'):\n            params['player_name'] = entities['players'][0]\n            if len(entities['players']) > 1:\n                params['player_name2'] = entities['players'][1]\n                params['player_names'] = entities['players']\n        \n        # Handle numerical values and comparators\n        if entities.get('numerical_values'):\n            params['threshold'] = entities['numerical_values'][0]\n            if len(entities['numerical_values']) > 1:\n                params['thresholds'] = entities['numerical_values']\n        \n        if entities.get('comparators'):\n            params['comparator'] = entities['comparators'][0]\n        \n        return params\n    \n    def get_errors(self) -> List[Dict]:\n        \"\"\"Get error log for debugging\"\"\"\n        return self.error_log\n    \n    def clear_errors(self):\n        \"\"\"Clear the error log\"\"\"\n        self.error_log = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:02:51.027603Z","iopub.execute_input":"2025-12-16T07:02:51.027930Z","iopub.status.idle":"2025-12-16T07:02:51.061741Z","shell.execute_reply.started":"2025-12-16T07:02:51.027904Z","shell.execute_reply":"2025-12-16T07:02:51.060707Z"}},"outputs":[{"name":"stdout","text":"Writing fpl_input_preprocessing.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile fpl_Task2.py\n\"\"\"\nMilestone 3 - System Requirements Part 2: Graph Retrieval Layer\nImplements:\n  2.a - Baseline: Cypher query templates (10+ queries)\n  2.b - Embeddings: Semantic similarity search (2 models for comparison)\n\"\"\"\n\nfrom neo4j import GraphDatabase\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import Dict, List, Any\nfrom fpl_input_preprocessing import FPLInputPreprocessorLLM as FPLInputPreprocessor\n\n\nclass FPLGraphRetrieval:\n    \"\"\"\n    Graph Retrieval Layer for FPL Graph-RAG system.\n    Combines baseline Cypher queries with embedding-based semantic search.\n    \"\"\"\n    \n    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str, \n                 hf_token: str = None, use_llm: bool = False):\n        \"\"\"\n        Initialize Neo4j connection and embedding models.\n        \n        Args:\n            neo4j_uri: Neo4j database URI\n            neo4j_user: Neo4j username\n            neo4j_password: Neo4j password\n            hf_token: HuggingFace API token (required if use_llm=True)\n            use_llm: Use LLM for intent/entity extraction (default: False for rule-based)\n        \"\"\"\n        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n        \n        self.preprocessor = FPLInputPreprocessor(\n            hf_token=hf_token or \"\",\n            use_llm=use_llm and hf_token is not None\n        )\n        \n        # Two embedding models for comparison (requirement 2.b)\n        self.embedding_models = {\n            'model_1': SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'),\n            'model_2': SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n        }\n        self.active_model = 'model_1'\n        \n        self._init_query_templates()\n    \n    def close(self):\n        \"\"\"Close Neo4j connection.\"\"\"\n        self.driver.close()\n\n    # =========================================================================\n    # SECTION 2.a: BASELINE - CYPHER QUERY TEMPLATES\n    # =========================================================================\n    \n    def _init_query_templates(self):\n        \"\"\"Initialize 12 FIXED Cypher query templates for different intents.\"\"\"\n        self.query_templates = {\n            \n            'top_players_by_position': \"\"\"\n                // Top players by position for a specific season\n                MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player)-[:PLAYS_AS]->(pos:Position {name: $position})\n                MATCH (p)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    t.name AS team,\n                    SUM(played.total_points) AS total_points,\n                    SUM(played.goals_scored) AS goals,\n                    SUM(played.assists) AS assists,\n                    COUNT(DISTINCT f) AS games_played\n                ORDER BY total_points DESC\n                LIMIT $limit\n            \"\"\",\n            \n            'player_gameweek_performance': \"\"\"\n                // Player performance in specific gameweek\n                MATCH (gw:Gameweek {season: $season, GW_number: $gameweek})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player {player_name: $player_name})-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    t.name AS team,\n                    played.total_points AS points,\n                    played.goals_scored AS goals,\n                    played.assists AS assists,\n                    played.minutes AS minutes,\n                    played.bonus AS bonus,\n                    played.clean_sheets AS clean_sheets,\n                    played.ict_index AS ict_index\n            \"\"\",\n            \n            'compare_players': \"\"\"\n                // Compare two players for a specific season - FIXED VERSION\n                MATCH (p1:Player {player_name: $player_name})\n                MATCH (p2:Player {player_name: $player_name2})\n                \n                // Get player 1 stats for the season\n                OPTIONAL MATCH (p1)-[played1:PLAYED_IN]->(f1:Fixture)\n                WHERE EXISTS {\n                    MATCH (f1)<-[:HAS_FIXTURE]-(:Gameweek {season: $season})\n                }\n                \n                // Get player 2 stats for the season\n                OPTIONAL MATCH (p2)-[played2:PLAYED_IN]->(f2:Fixture)\n                WHERE EXISTS {\n                    MATCH (f2)<-[:HAS_FIXTURE]-(:Gameweek {season: $season})\n                }\n                \n                // Get teams\n                OPTIONAL MATCH (p1)-[:PLAYS_FOR]->(t1:Team)\n                OPTIONAL MATCH (p2)-[:PLAYS_FOR]->(t2:Team)\n                \n                // Aggregate with DISTINCT to avoid duplicates\n                WITH p1, t1, \n                    SUM(DISTINCT played1.total_points) AS p1_points,\n                    SUM(DISTINCT played1.goals_scored) AS p1_goals,\n                    SUM(DISTINCT played1.assists) AS p1_assists,\n                    COUNT(DISTINCT f1) AS p1_games,\n                    p2, t2,\n                    SUM(DISTINCT played2.total_points) AS p2_points,\n                    SUM(DISTINCT played2.goals_scored) AS p2_goals,\n                    SUM(DISTINCT played2.assists) AS p2_assists,\n                    COUNT(DISTINCT f2) AS p2_games\n                \n                RETURN p1.player_name AS player1, \n                    COALESCE(t1.name, 'Unknown') AS team1, \n                    COALESCE(p1_points, 0) AS p1_points,\n                    COALESCE(p1_goals, 0) AS p1_goals,\n                    COALESCE(p1_assists, 0) AS p1_assists,\n                    p1_games,\n                    \n                    p2.player_name AS player2, \n                    COALESCE(t2.name, 'Unknown') AS team2, \n                    COALESCE(p2_points, 0) AS p2_points,\n                    COALESCE(p2_goals, 0) AS p2_goals,\n                    COALESCE(p2_assists, 0) AS p2_assists,\n                    p2_games\n            \"\"\",\n            \n            'team_fixtures': \"\"\"\n                // Team fixtures for a specific season\n                MATCH (t:Team {name: $team})\n                MATCH (f:Fixture)-[:HAS_HOME_TEAM|HAS_AWAY_TEAM]->(t)\n                MATCH (f)<-[:HAS_FIXTURE]-(gw:Gameweek {season: $season})\n                MATCH (f)-[:HAS_HOME_TEAM]->(home:Team)\n                MATCH (f)-[:HAS_AWAY_TEAM]->(away:Team)\n                RETURN gw.GW_number AS gameweek,\n                    home.name AS home_team,\n                    away.name AS away_team,\n                    f.kickoff_time AS kickoff,\n                    CASE WHEN home = t THEN 'home' ELSE 'away' END AS venue\n                ORDER BY gw.GW_number\n            \"\"\",\n            \n            'players_by_team': \"\"\"\n                // Players by team\n                MATCH (t:Team {name: $team})\n                MATCH (p:Player)-[:PLAYS_FOR]->(t)\n                OPTIONAL MATCH (p)-[:PLAYS_AS]->(pos:Position)\n                RETURN p.player_name AS player,\n                    COALESCE(pos.name, 'Unknown') AS position,\n                    p.player_element AS player_id\n                ORDER BY position, p.player_name\n            \"\"\",\n\n            'player_team': \"\"\"\n                // Get team for a specific player\n                MATCH (p:Player {player_name: $player_name})-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player, \n                    t.name AS team,\n                    p.player_element AS player_id\n            \"\"\",\n            \n            'top_scorers': \"\"\"\n                // Top scorers for a specific season\n                MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    t.name AS team,\n                    SUM(played.goals_scored) AS goals,\n                    SUM(played.total_points) AS points,\n                    SUM(played.assists) AS assists,\n                    COUNT(DISTINCT f) AS games_played\n                ORDER BY goals DESC\n                LIMIT $limit\n            \"\"\",\n            \n            'top_assisters': \"\"\"\n                // Top assisters for a specific season\n                MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    t.name AS team,\n                    SUM(played.assists) AS assists,\n                    SUM(played.total_points) AS points,\n                    SUM(played.goals_scored) AS goals,\n                    COUNT(DISTINCT f) AS games_played\n                ORDER BY assists DESC\n                LIMIT $limit\n            \"\"\",\n            \n            'clean_sheet_leaders': \"\"\"\n                // Clean sheet leaders for a specific season (GK and DEF only)\n                MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player)-[:PLAYS_AS]->(pos:Position)\n                WHERE pos.name IN ['GK', 'DEF']\n                MATCH (p)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    pos.name AS position,\n                    t.name AS team,\n                    SUM(played.clean_sheets) AS clean_sheets,\n                    SUM(played.total_points) AS points,\n                    COUNT(DISTINCT f) AS games_played\n                ORDER BY clean_sheets DESC\n                LIMIT $limit\n            \"\"\",\n            \n            'players_by_form': \"\"\"\n                // Players by recent form (last 5 gameweeks)\n                MATCH (gw:Gameweek {season: $season})\n                WHERE gw.GW_number >= $gameweek - 5 AND gw.GW_number <= $gameweek\n                MATCH (gw)<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    t.name AS team,\n                    AVG(played.form) AS avg_form,\n                    SUM(played.total_points) AS recent_points,\n                    SUM(played.goals_scored) AS recent_goals,\n                    SUM(played.assists) AS recent_assists,\n                    COUNT(DISTINCT f) AS games_count\n                ORDER BY avg_form DESC\n                LIMIT $limit\n            \"\"\",\n            \n            'player_season_summary': \"\"\"\n                // Player season summary\n                MATCH (p:Player {player_name: $player_name})\n                MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_AS]->(pos:Position)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    pos.name AS position,\n                    t.name AS team,\n                    COUNT(DISTINCT f) AS games_played,\n                    SUM(played.minutes) AS total_minutes,\n                    SUM(played.total_points) AS total_points,\n                    SUM(played.goals_scored) AS goals,\n                    SUM(played.assists) AS assists,\n                    SUM(played.clean_sheets) AS clean_sheets,\n                    SUM(played.bonus) AS bonus_points,\n                    AVG(played.ict_index) AS avg_ict,\n                    AVG(played.form) AS avg_form\n            \"\"\",\n            \n            'bonus_leaders': \"\"\"\n                // Bonus points leaders for a specific season\n                MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    t.name AS team,\n                    SUM(played.bonus) AS total_bonus,\n                    SUM(played.total_points) AS total_points,\n                    SUM(played.bps) AS bps,\n                    COUNT(DISTINCT f) AS games_played\n                ORDER BY total_bonus DESC\n                LIMIT $limit\n            \"\"\",\n            \n            'most_cards': \"\"\"\n                // Players with most cards (yellow/red) for a specific season\n                MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\n                MATCH (p:Player)-[played:PLAYED_IN]->(f)\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\n                RETURN p.player_name AS player,\n                    t.name AS team,\n                    SUM(played.yellow_cards) AS yellows,\n                    SUM(played.red_cards) AS reds,\n                    SUM(played.yellow_cards) + SUM(played.red_cards) * 2 AS card_score,\n                    COUNT(DISTINCT f) AS games_played\n                ORDER BY card_score DESC\n                LIMIT $limit\n            \"\"\"\n        }\n        \n        # Map intents to query templates (ordered by preference)\n        self.intent_to_query = {\n        'recommendation': ['top_scorers', 'top_assisters', 'clean_sheet_leaders', 'bonus_leaders', 'top_players_by_position'],\n        'performance_query': ['player_gameweek_performance', 'player_season_summary', 'players_by_form'],\n        'comparison': ['compare_players', 'top_scorers', 'player_season_summary'],\n        'player_search': ['player_season_summary', 'player_gameweek_performance', 'players_by_form'],\n        'fixture_query': ['team_fixtures', 'players_by_team'],\n        'form_query': ['players_by_form', 'player_season_summary', 'top_scorers'],\n        'team_analysis': ['players_by_team', 'team_fixtures', 'top_scorers'],\n        'value_analysis': ['top_players_by_position', 'players_by_form', 'bonus_leaders'],\n        'general_query': ['top_scorers', 'top_assisters', 'bonus_leaders', 'clean_sheet_leaders'],\n        'player_team_query': ['player_team', 'player_season_summary', 'players_by_team']\n        }\n        self.intent_primary_query = {\n        'recommendation': 'top_scorers',\n        'performance_query': 'player_gameweek_performance',\n        'comparison': 'compare_players',  # This is the bug! Should be 'compare_players'\n        'player_search': 'player_season_summary',\n        'fixture_query': 'team_fixtures',\n        'form_query': 'players_by_form',\n        'team_analysis': 'players_by_team',\n        'player_team_query': 'player_team',\n        'value_analysis': 'top_players_by_position',\n        'general_query': 'top_scorers'\n    }\n    \n    def select_query(self, intent: str, entities: Dict) -> str:\n        \"\"\"Select appropriate query template based on intent and available entities.\n        \n        Priority order (highest to lowest):\n        1. Intent-specific with complete entities (highest confidence)\n        2. Entity combinations that clearly indicate a specific query type\n        3. Intent-based fallback\n        4. Generic fallback\n        \"\"\"\n        \n        metrics = entities.get('metrics', [])\n        positions = entities.get('positions', [])\n        teams = entities.get('teams', [])\n        players = entities.get('players', [])\n        gameweeks = entities.get('gameweeks', [])\n        \n        # =====================================================================\n        # PRIORITY 1: HIGH-CONFIDENCE INTENT + ENTITY COMBINATIONS\n        # =====================================================================\n        \n        # 1A: PERFORMANCE QUERY - Single player + specific gameweek\n        if intent == 'performance_query' and len(players) >= 1 and len(gameweeks) >= 1:\n            return 'player_gameweek_performance'\n        \n        # 1B: COMPARISON QUERY - Exactly 2 players (no gameweek needed for comparison)\n        if intent == 'comparison' and len(players) == 2:\n            return 'compare_players'\n        \n        # 1C: PLAYER SEARCH - Single player, no gameweek\n        if intent == 'player_search' and len(players) >= 1 and len(gameweeks) == 0:\n            return 'player_season_summary'\n        \n        # =====================================================================\n        # PRIORITY 2: CLEAR ENTITY PATTERNS (regardless of intent)\n        # =====================================================================\n        \n        # 2A: Single player + gameweek = ALWAYS gameweek performance\n        if len(players) == 1 and len(gameweeks) >= 1:\n            return 'player_gameweek_performance'\n        \n        # 2B: Two players = comparison (but check if it's REALLY a comparison)\n        if len(players) == 2:\n            # Extra validation: check if query has comparison words\n            query_text = str(entities).lower()\n            comparison_words = ['vs', 'versus', 'compare', 'comparison', 'versus']\n            if any(word in query_text for word in comparison_words):\n                return 'compare_players'\n        \n                # Two players without comparison words? Might be a mistake\n                # Fall through to lower priority checks\n        \n        # 2C: Team + gameweek = team fixtures\n        if len(teams) >= 1 and len(gameweeks) >= 1:\n            return 'team_fixtures'\n        \n        # 2D: Team alone (depends on context)\n        if len(teams) >= 1:\n            # If asking about \"players on team\"\n            if 'players' in str(entities).lower() or 'roster' in str(entities).lower():\n                return 'players_by_team'\n            return 'team_fixtures'  # Default for team queries\n        \n        # 2E: Position alone = top players by position\n        if len(positions) >= 1 and len(players) == 0:\n            return 'top_players_by_position'\n        \n        # =====================================================================\n        # PRIORITY 3: INTENT-BASED FALLBACK (using corrected mapping)\n        # =====================================================================\n        \n        intent_query_map = {\n            'recommendation': 'top_scorers',\n            'performance_query': 'player_gameweek_performance',\n            'comparison': 'compare_players',\n            'player_search': 'player_season_summary',\n            'fixture_query': 'team_fixtures',\n            'form_query': 'players_by_form',\n            'team_analysis': 'players_by_team',\n            'value_analysis': 'top_players_by_position',\n            'general_query': 'top_scorers',\n            'player_team_query': 'player_team'\n        }\n        \n        if intent in intent_query_map:\n            return intent_query_map[intent]\n        \n        # =====================================================================\n        # PRIORITY 4: METRIC-SPECIFIC FALLBACK\n        # =====================================================================\n        \n        metric_to_query = {\n            'clean_sheets': 'clean_sheet_leaders',\n            'assists': 'top_assisters',\n            'goals_scored': 'top_scorers',\n            'bonus': 'bonus_leaders',\n            'cards': 'most_cards',\n            'form': 'players_by_form',\n            'value': 'top_players_by_position'\n        }\n        \n        for metric in metrics:\n            if metric in metric_to_query:\n                return metric_to_query[metric]\n        \n        # =====================================================================\n        # PRIORITY 5: SAFE GENERIC FALLBACK\n        # =====================================================================\n        return 'top_scorers'\n    \n    def execute_cypher(self, query_name: str, params: Dict) -> List[Dict]:\n        \"\"\"Execute a Cypher query template with parameters.\"\"\"\n        if query_name not in self.query_templates:\n            raise ValueError(f\"Unknown query template: {query_name}\")\n        \n        query = self.query_templates[query_name]\n        \n        params.setdefault('limit', 10)\n        params.setdefault('season', '2022-23')\n        \n        # Convert year to season format (e.g., '2022' -> '2022-23')\n        if 'season' in params:\n            season = params['season']\n            if len(season) == 4 and season.isdigit():\n                year = int(season)\n                params['season'] = f\"{year}-{str(year + 1)[-2:]}\"\n        \n        with self.driver.session() as session:\n            result = session.run(query, params)\n            return [record.data() for record in result]\n    \n    def baseline_retrieve(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Baseline retrieval using Cypher queries only.\"\"\"\n        preprocessed = self.preprocessor.preprocess(user_input, include_embedding=False)\n        params = self.preprocessor.get_cypher_params(preprocessed)\n        query_name = self.select_query(preprocessed['intent'], preprocessed['entities'])\n        \n        try:\n            results = self.execute_cypher(query_name, params)\n            return {\n                'method': 'baseline',\n                'intent': preprocessed['intent'],\n                'query_used': query_name,\n                'parameters': params,\n                'results': results,\n                'cypher': self.query_templates[query_name]\n            }\n        except Exception as e:\n            return {\n                'method': 'baseline',\n                'error': str(e),\n                'intent': preprocessed['intent'],\n                'query_used': query_name,\n                'parameters': params\n            }\n\n    # =========================================================================\n    # SECTION 2.b: NODE EMBEDDINGS - NUMERIC FEATURE VECTORS\n    # =========================================================================\n    #\n    # APPROACH: Create numeric embeddings from player statistics\n    # \n    # Why numeric embeddings for FPL?\n    # - FPL data is purely numerical (goals, points, assists, etc.)\n    # - No textual features to embed\n    # - Direct numeric vectors preserve exact statistical relationships\n    # - Faster computation than text-based embeddings\n    #\n    # Feature Vector Structure (12 dimensions):\n    # [goals_norm, assists_norm, points_norm, clean_sheets_norm, minutes_norm,\n    #  bonus_norm, form_norm, ict_norm, influence_norm, creativity_norm, \n    #  threat_norm, games_norm]\n    #\n    # Each feature is normalized to [0, 1] range for fair comparison\n    # =========================================================================\n    \n    def set_embedding_model(self, model_name: str):\n        \"\"\"Switch between embedding models for comparison.\"\"\"\n        if model_name not in self.embedding_models:\n            raise ValueError(f\"Unknown model: {model_name}. Available: {list(self.embedding_models.keys())}\")\n        self.active_model = model_name\n    \n    def fetch_all_players_stats(self, season: str = None, use_per_game_avg: bool = True) -> List[Dict]:\n        \"\"\"\n        Fetch ALL players with their stats and LATEST position.\n        \n        How it works:\n        1. Match each unique player by player_element (unique ID)\n        2. Get ONE position per player (collect and take first)\n        3. Option A (use_per_game_avg=True): Compute PER-GAME AVERAGES\n           - This ensures players with high performance per game rank higher\n           - Avoids bias toward veterans who played more seasons\n        4. Option B (use_per_game_avg=False): Use totals (for season-specific queries)\n        5. Return one row per player (no duplicates)\n        \n        Args:\n            season: Optional season filter (e.g., '2022-23'). If None, aggregates all.\n            use_per_game_avg: If True, computes per-game averages for fair comparison.\n        \n        Returns:\n            List of player dictionaries with stats\n        \"\"\"\n        # Build season filter if provided\n        season_filter = \"\"\n        if season:\n            season_filter = \"MATCH (f)<-[:HAS_FIXTURE]-(gw:Gameweek {season: $season})\"\n        \n        if use_per_game_avg:\n            # Use per-game averages - FAIR comparison across players with different game counts\n            query = f\"\"\"\n                // Get all unique players\n                MATCH (p:Player)\n                \n                // Get position (take first if multiple)\n                OPTIONAL MATCH (p)-[:PLAYS_AS]->(pos:Position)\n                \n                // Aggregate stats from fixtures\n                OPTIONAL MATCH (p)-[played:PLAYED_IN]->(f:Fixture)\n                {season_filter}\n                \n                // Group by player_element (unique ID) to avoid duplicates\n                // Use PER-GAME AVERAGES for fair comparison\n                WITH p.player_element AS player_id,\n                     COLLECT(DISTINCT p.player_name)[0] AS player_name,\n                     COLLECT(DISTINCT pos.name)[0] AS position,\n                     COUNT(played) AS games_played,\n                     // Per-game averages (more fair for embedding comparison)\n                     COALESCE(AVG(played.total_points), 0) AS avg_points_per_game,\n                     COALESCE(SUM(played.goals_scored) * 1.0 / NULLIF(COUNT(played), 0), 0) AS goals_per_game,\n                     COALESCE(SUM(played.assists) * 1.0 / NULLIF(COUNT(played), 0), 0) AS assists_per_game,\n                     COALESCE(SUM(played.clean_sheets) * 1.0 / NULLIF(COUNT(played), 0), 0) AS clean_sheets_per_game,\n                     COALESCE(AVG(played.minutes), 0) AS avg_minutes,\n                     COALESCE(AVG(played.bonus), 0) AS avg_bonus,\n                     COALESCE(AVG(played.form), 0) AS form,\n                     COALESCE(AVG(played.ict_index), 0) AS ict_index,\n                     COALESCE(AVG(played.influence), 0) AS influence,\n                     COALESCE(AVG(played.creativity), 0) AS creativity,\n                     COALESCE(AVG(played.threat), 0) AS threat,\n                     // Also keep totals for reference\n                     COALESCE(SUM(played.total_points), 0) AS total_points,\n                     COALESCE(SUM(played.goals_scored), 0) AS goals,\n                     COALESCE(SUM(played.assists), 0) AS assists,\n                     COALESCE(SUM(played.clean_sheets), 0) AS clean_sheets\n                \n                // Filter out any null entries and return\n                WHERE player_name IS NOT NULL AND position IS NOT NULL AND games_played > 0\n                RETURN player_id, player_name, position, games_played,\n                       avg_points_per_game, goals_per_game, assists_per_game,\n                       clean_sheets_per_game, avg_minutes, avg_bonus,\n                       form, ict_index, influence, creativity, threat,\n                       total_points, goals, assists, clean_sheets\n                ORDER BY avg_points_per_game DESC\n            \"\"\"\n        else:\n            # Use totals - for season-specific comparisons\n            query = f\"\"\"\n                MATCH (p:Player)\n                OPTIONAL MATCH (p)-[:PLAYS_AS]->(pos:Position)\n                OPTIONAL MATCH (p)-[played:PLAYED_IN]->(f:Fixture)\n                {season_filter}\n                WITH p.player_element AS player_id,\n                     COLLECT(DISTINCT p.player_name)[0] AS player_name,\n                     COLLECT(DISTINCT pos.name)[0] AS position,\n                     COALESCE(SUM(played.total_points), 0) AS total_points,\n                     COALESCE(SUM(played.goals_scored), 0) AS goals,\n                     COALESCE(SUM(played.assists), 0) AS assists,\n                     COALESCE(SUM(played.clean_sheets), 0) AS clean_sheets,\n                     COALESCE(SUM(played.minutes), 0) AS minutes,\n                     COALESCE(SUM(played.bonus), 0) AS bonus,\n                     COALESCE(AVG(played.form), 0) AS form,\n                     COALESCE(AVG(played.ict_index), 0) AS ict_index,\n                     COALESCE(AVG(played.influence), 0) AS influence,\n                     COALESCE(AVG(played.creativity), 0) AS creativity,\n                     COALESCE(AVG(played.threat), 0) AS threat,\n                     COUNT(played) AS games_played\n                WHERE player_name IS NOT NULL AND position IS NOT NULL\n                RETURN player_id, player_name, position,\n                       total_points, goals, assists, clean_sheets,\n                       minutes, bonus, form, ict_index,\n                       influence, creativity, threat, games_played\n                ORDER BY total_points DESC\n            \"\"\"\n        \n        params = {'season': season} if season else {}\n        \n        with self.driver.session() as session:\n            result = session.run(query, params)\n            players = [dict(record) for record in result]\n            \n        print(f\"📊 Fetched {len(players)} unique players\" + (f\" (per-game averages)\" if use_per_game_avg else \"\"))\n        return players\n    \n    def compute_normalization_stats(self, players: List[Dict]) -> Dict[str, Dict]:\n        \"\"\"\n        Compute min/max for each feature to normalize to [0, 1] range.\n        \n        Why normalize?\n        - Goals per game range: 0-1, Points per game range: 0-15\n        - Without normalization, points would dominate similarity\n        - Normalized: each feature contributes equally\n        \n        Uses PER-GAME AVERAGES for fair comparison across players with different game counts.\n        \n        Returns:\n            Dictionary with min/max for each feature\n        \"\"\"\n        # Use per-game average features for fair comparison\n        features = ['goals_per_game', 'assists_per_game', 'avg_points_per_game', \n                    'clean_sheets_per_game', 'avg_minutes', 'avg_bonus', \n                    'form', 'ict_index', 'influence', 'creativity', 'threat', 'games_played']\n        \n        # Fallback features if per-game stats not available\n        fallback_features = ['goals', 'assists', 'total_points', 'clean_sheets', \n                             'minutes', 'bonus', 'form', 'ict_index',\n                             'influence', 'creativity', 'threat', 'games_played']\n        \n        stats = {}\n        for i, feat in enumerate(features):\n            # Try per-game feature first, fallback to totals\n            values = [p.get(feat, 0) or p.get(fallback_features[i], 0) or 0 for p in players]\n            stats[feat] = {\n                'min': min(values),\n                'max': max(values),\n                'range': max(values) - min(values) if max(values) != min(values) else 1\n            }\n        \n        return stats\n    \n    def create_numeric_embedding(self, player: Dict, norm_stats: Dict) -> np.ndarray:\n        \"\"\"\n        Create a normalized numeric embedding vector for a player.\n        \n        Uses PER-GAME AVERAGES for fair comparison:\n        - A striker with 1 goal per game ranks higher than one with 0.2 goals/game\n        - This prevents bias toward players who simply played more games\n        \n        Process:\n        1. Extract per-game average values (or fallback to totals)\n        2. Normalize each to [0, 1] using min-max scaling\n        3. Return as numpy array\n        \n        Formula: normalized = (value - min) / (max - min)\n        \n        Args:\n            player: Player dictionary with stats\n            norm_stats: Normalization statistics from compute_normalization_stats\n            \n        Returns:\n            numpy array of shape (12,) with normalized features\n        \"\"\"\n        # Per-game average features\n        features = ['goals_per_game', 'assists_per_game', 'avg_points_per_game', \n                    'clean_sheets_per_game', 'avg_minutes', 'avg_bonus', \n                    'form', 'ict_index', 'influence', 'creativity', 'threat', 'games_played']\n        \n        # Fallback features if per-game not available\n        fallback_features = ['goals', 'assists', 'total_points', 'clean_sheets', \n                             'minutes', 'bonus', 'form', 'ict_index',\n                             'influence', 'creativity', 'threat', 'games_played']\n        \n        embedding = []\n        for i, feat in enumerate(features):\n            # Try per-game feature first, fallback to totals\n            raw_value = player.get(feat, 0) or player.get(fallback_features[i], 0) or 0\n            stats = norm_stats[feat]\n            # Min-max normalization to [0, 1]\n            normalized = (raw_value - stats['min']) / stats['range']\n            embedding.append(normalized)\n        \n        return np.array(embedding, dtype=np.float32)\n    \n    def create_all_embeddings(self, players: List[Dict]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Create embeddings for all players in batch.\n        \n        Returns:\n            Dictionary mapping player_name to embedding vector\n        \"\"\"\n        print(\"🔢 Computing normalization statistics...\")\n        norm_stats = self.compute_normalization_stats(players)\n        \n        print(\"📐 Creating embeddings for all players...\")\n        embeddings = {}\n        for player in players:\n            name = player['player_name']\n            embeddings[name] = self.create_numeric_embedding(player, norm_stats)\n        \n        print(f\"✅ Created {len(embeddings)} embeddings of dimension {len(list(embeddings.values())[0])}\")\n        return embeddings, norm_stats\n    \n    # ═════════════════════════════════════════════════════════════════════════\n    # FIXTURE EMBEDDINGS\n    # ═════════════════════════════════════════════════════════════════════════\n    \n    def fetch_all_fixtures(self, season: str = None) -> List[Dict]:\n        \"\"\"Fetch all fixtures with their stats.\"\"\"\n        season_filter = \"\"\n        params = {}\n        \n        if season:\n            season_filter = \"MATCH (gw:Gameweek {season: $season})<-[:HAS_FIXTURE]-(f:Fixture)\"\n            params['season'] = season\n        else:\n            season_filter = \"MATCH (f:Fixture)<-[:HAS_FIXTURE]-(gw:Gameweek)\"\n        \n        query = f\"\"\"\n            {season_filter}\n            OPTIONAL MATCH (f)-[:HAS_HOME_TEAM]->(home:Team)\n            OPTIONAL MATCH (f)-[:HAS_AWAY_TEAM]->(away:Team)\n            WITH f, gw, home.name AS home_team, away.name AS away_team\n            OPTIONAL MATCH (p)-[played:PLAYED_IN]->(f)\n            WITH f, gw, home_team, away_team,\n                 COUNT(DISTINCT p) AS players_in_fixture,\n                 AVG(played.total_points) AS avg_points,\n                 MAX(played.total_points) AS max_points,\n                 MIN(played.total_points) AS min_points\n            RETURN DISTINCT \n                   f.fixture_number AS fixture_number,\n                   gw.GW_number AS gameweek,\n                   gw.season AS season,\n                   home_team,\n                   away_team,\n                   players_in_fixture,\n                   avg_points,\n                   max_points,\n                   min_points\n        \"\"\"\n        \n        with self.driver.session() as session:\n            result = session.run(query, params)\n            return [dict(record) for record in result]\n    \n    def create_fixture_embedding(self, fixture: Dict) -> np.ndarray:\n        \"\"\"Create embedding for a fixture based on team strength and match context.\"\"\"\n        # 8-dimensional embedding for fixtures:\n        # [gameweek_norm, home_strength, away_strength, avg_points, max_points, min_points, num_players, fixture_quality]\n        \n        embedding = np.zeros(8, dtype=np.float32)\n        \n        # Normalize gameweek (assuming max 38 gameweeks)\n        gameweek = fixture.get('gameweek', 1)\n        embedding[0] = min(gameweek / 38.0, 1.0)\n        \n        # Team strength (proxy: use team name hash for consistency)\n        home_team = fixture.get('home_team', '')\n        away_team = fixture.get('away_team', '')\n        \n        # Hash teams to get pseudo-strength (0-1)\n        home_strength = (hash(home_team) % 100) / 100.0 if home_team else 0.5\n        away_strength = (hash(away_team) % 100) / 100.0 if away_team else 0.5\n        \n        embedding[1] = home_strength\n        embedding[2] = away_strength\n        \n        # Points statistics (normalized to 0-1 range, assuming max ~100 points)\n        embedding[3] = min((fixture.get('avg_points', 0) or 0) / 100.0, 1.0)\n        embedding[4] = min((fixture.get('max_points', 0) or 0) / 150.0, 1.0)\n        embedding[5] = min((fixture.get('min_points', 0) or 0) / 50.0, 1.0)\n        \n        # Number of players in fixture (normalized, assuming max ~1000)\n        num_players = fixture.get('players_in_fixture', 0) or 0\n        embedding[6] = min(num_players / 1000.0, 1.0)\n        \n        # Fixture \"quality\" - higher when there's good scoring potential\n        quality = ((embedding[3] + embedding[4]) / 2.0) * (1.0 + embedding[6] / 2.0)\n        embedding[7] = min(quality, 1.0)\n        \n        return embedding\n    \n    def store_fixture_embeddings_in_neo4j(self, season: str = None) -> Dict:\n        \"\"\"Store fixture embeddings in Neo4j.\"\"\"\n        fixtures = self.fetch_all_fixtures(season)\n        \n        if not fixtures:\n            return {'error': 'No fixtures found', 'count': 0}\n        \n        # Create embeddings\n        embeddings = {}\n        for fixture in fixtures:\n            fixture_key = (fixture['fixture_number'], fixture['season'])\n            embeddings[fixture_key] = self.create_fixture_embedding(fixture)\n        \n        # Store in Neo4j\n        update_query = \"\"\"\n            MATCH (f:Fixture {fixture_number: $fixture_number})\n            WHERE (f)<-[:HAS_FIXTURE]-(gw:Gameweek {season: $season})\n            SET f.embedding = $embedding,\n                f.embedding_type = 'fixture',\n                f.embedding_dim = 8\n        \"\"\"\n        \n        stored = 0\n        with self.driver.session() as session:\n            for fixture in fixtures:\n                fixture_key = (fixture['fixture_number'], fixture['season'])\n                if fixture_key in embeddings:\n                    try:\n                        session.run(update_query, {\n                            'fixture_number': fixture['fixture_number'],\n                            'season': fixture['season'],\n                            'embedding': embeddings[fixture_key].tolist()\n                        })\n                        stored += 1\n                    except Exception as e:\n                        print(f\"⚠️ Error storing fixture {fixture['fixture_number']}: {e}\")\n        \n        return {\n            'fixtures_processed': len(fixtures),\n            'embeddings_stored': stored,\n            'embedding_dimensions': 8\n        }\n    \n    # ═════════════════════════════════════════════════════════════════════════\n    # GAMEWEEK EMBEDDINGS\n    # ═════════════════════════════════════════════════════════════════════════\n    \n    def fetch_all_gameweeks(self, season: str = None) -> List[Dict]:\n        \"\"\"Fetch all gameweeks with their stats.\"\"\"\n        season_filter = \"\"\n        params = {}\n        \n        if season:\n            season_filter = \"WHERE gw.season = $season\"\n            params['season'] = season\n        \n        query = f\"\"\"\n            MATCH (gw:Gameweek)\n            {season_filter}\n            OPTIONAL MATCH (gw)<-[:HAS_FIXTURE]-(f:Fixture)\n            OPTIONAL MATCH (p)-[played:PLAYED_IN]->(f)\n            WITH gw,\n                 COUNT(DISTINCT f) AS num_fixtures,\n                 COUNT(DISTINCT p) AS num_players,\n                 AVG(played.total_points) AS avg_points,\n                 MAX(played.total_points) AS max_points,\n                 MIN(played.total_points) AS min_points,\n                 STDEV(played.total_points) AS points_variance\n            RETURN gw.GW_number AS gameweek,\n                   gw.season AS season,\n                   num_fixtures,\n                   num_players,\n                   avg_points,\n                   max_points,\n                   min_points,\n                   COALESCE(points_variance, 0) AS points_variance\n        \"\"\"\n        \n        with self.driver.session() as session:\n            result = session.run(query, params)\n            return [dict(record) for record in result]\n    \n    def create_gameweek_embedding(self, gameweek: Dict) -> np.ndarray:\n        \"\"\"Create embedding for a gameweek based on match activity and scoring stats.\"\"\"\n        # 8-dimensional embedding for gameweeks:\n        # [gameweek_norm, fixture_density, player_coverage, avg_points, max_points, min_points, variance, excitement]\n        \n        embedding = np.zeros(8, dtype=np.float32)\n        \n        # Gameweek number (normalized to 0-1, assuming max 38)\n        gw_num = gameweek.get('gameweek', 1)\n        embedding[0] = min(gw_num / 38.0, 1.0)\n        \n        # Fixture density (max 10 fixtures per gameweek)\n        num_fixtures = gameweek.get('num_fixtures', 0) or 0\n        embedding[1] = min(num_fixtures / 10.0, 1.0)\n        \n        # Player coverage (normalized, assuming ~1000 players across all fixtures)\n        num_players = gameweek.get('num_players', 0) or 0\n        embedding[2] = min(num_players / 1000.0, 1.0)\n        \n        # Points statistics (normalized)\n        embedding[3] = min((gameweek.get('avg_points', 0) or 0) / 100.0, 1.0)\n        embedding[4] = min((gameweek.get('max_points', 0) or 0) / 150.0, 1.0)\n        embedding[5] = min((gameweek.get('min_points', 0) or 0) / 50.0, 1.0)\n        \n        # Points variance (normalized, high variance = more variability)\n        variance = (gameweek.get('points_variance', 0) or 0)\n        embedding[6] = min(variance / 100.0, 1.0)\n        \n        # \"Excitement\" score - combination of activity and variance\n        excitement = (embedding[1] * 0.3 + embedding[2] * 0.3 + embedding[6] * 0.4)\n        embedding[7] = excitement\n        \n        return embedding\n    \n    def store_gameweek_embeddings_in_neo4j(self, season: str = None) -> Dict:\n        \"\"\"Store gameweek embeddings in Neo4j.\"\"\"\n        gameweeks = self.fetch_all_gameweeks(season)\n        \n        if not gameweeks:\n            return {'error': 'No gameweeks found', 'count': 0}\n        \n        # Create embeddings\n        embeddings = {}\n        for gw in gameweeks:\n            gw_key = (gw['gameweek'], gw['season'])\n            embeddings[gw_key] = self.create_gameweek_embedding(gw)\n        \n        # Store in Neo4j\n        update_query = \"\"\"\n            MATCH (gw:Gameweek {GW_number: $gameweek, season: $season})\n            SET gw.embedding = $embedding,\n                gw.embedding_type = 'gameweek',\n                gw.embedding_dim = 8\n        \"\"\"\n        \n        stored = 0\n        with self.driver.session() as session:\n            for gw in gameweeks:\n                gw_key = (gw['gameweek'], gw['season'])\n                if gw_key in embeddings:\n                    try:\n                        session.run(update_query, {\n                            'gameweek': gw['gameweek'],\n                            'season': gw['season'],\n                            'embedding': embeddings[gw_key].tolist()\n                        })\n                        stored += 1\n                    except Exception as e:\n                        print(f\"⚠️ Error storing gameweek {gw['gameweek']}: {e}\")\n        \n        return {\n            'gameweeks_processed': len(gameweeks),\n            'embeddings_stored': stored,\n            'embedding_dimensions': 8\n        }\n    \n    def create_vector_index(self, embedding_type: str = 'numeric'):\n        \"\"\"\n        Create a vector index in Neo4j for fast similarity search.\n        \n        Args:\n            embedding_type: 'numeric' (12 dims), 'minilm' (384 dims), or 'mpnet' (768 dims)\n        \"\"\"\n        dims_map = {'numeric': 12, 'minilm': 384, 'mpnet': 768}\n        property_map = {'numeric': 'embedding', 'minilm': 'embedding_minilm', 'mpnet': 'embedding_mpnet'}\n        \n        dims = dims_map.get(embedding_type, 12)\n        prop = property_map.get(embedding_type, 'embedding')\n        index_name = f\"player_{embedding_type}_embeddings\"\n        \n        print(f\"📁 Creating vector index '{index_name}' in Neo4j...\")\n        \n        drop_query = f\"DROP INDEX {index_name} IF EXISTS\"\n        create_query = f\"\"\"\n            CREATE VECTOR INDEX {index_name} IF NOT EXISTS\n            FOR (p:Player) ON (p.{prop})\n            OPTIONS {{\n                indexConfig: {{\n                    `vector.dimensions`: {dims},\n                    `vector.similarity_function`: 'cosine'\n                }}\n            }}\n        \"\"\"\n        \n        with self.driver.session() as session:\n            try:\n                session.run(drop_query)\n            except:\n                pass\n            session.run(create_query)\n        \n        print(f\"✅ Vector index created ({dims} dimensions, cosine similarity)\")\n    \n    def create_text_representation(self, player: Dict) -> str:\n        \"\"\"Create text description from player stats for text embedding.\"\"\"\n        return (\n            f\"Football player {player.get('player_name', 'Unknown')} \"\n            f\"plays as {player.get('position', 'Unknown')} position. \"\n            f\"Season statistics: {player.get('total_points', 0)} total FPL points, \"\n            f\"{player.get('goals', 0)} goals scored, \"\n            f\"{player.get('assists', 0)} assists provided, \"\n            f\"{player.get('clean_sheets', 0)} clean sheets. \"\n            f\"Playing time: {player.get('minutes', 0)} minutes in {player.get('games_played', 0)} games. \"\n            f\"Bonus points earned: {player.get('bonus', 0)}. \"\n            f\"Performance metrics: form {player.get('form', 0):.2f}, \"\n            f\"ICT index {player.get('ict_index', 0):.2f}.\"\n        )\n    \n    def store_text_embeddings_in_neo4j(self, model_name: str = 'model_1', batch_size: int = 32) -> Dict:\n        \"\"\"\n        Store TEXT-BASED embeddings using sentence transformers.\n        \n        This satisfies the requirement to \"experiment with at least TWO different\n        embedding models for comparison\" - storing both MiniLM and MPNet embeddings.\n        \n        Args:\n            model_name: 'model_1' (MiniLM, 384 dims) or 'model_2' (MPNet, 768 dims)\n            batch_size: Batch size for encoding\n            \n        Returns:\n            Dictionary with statistics\n        \"\"\"\n        model_info = {\n            'model_1': {'name': 'MiniLM', 'dims': 384, 'property': 'embedding_minilm'},\n            'model_2': {'name': 'MPNet', 'dims': 768, 'property': 'embedding_mpnet'}\n        }\n        \n        if model_name not in model_info:\n            raise ValueError(f\"Unknown model: {model_name}\")\n        \n        info = model_info[model_name]\n        \n        print(\"=\" * 70)\n        print(f\"STORING TEXT EMBEDDINGS ({info['name']}) IN NEO4J\")\n        print(\"=\" * 70)\n        \n        # Fetch players\n        print(\"📊 Fetching players...\")\n        players = self.fetch_all_players_stats()\n        \n        if not players:\n            return {'error': 'No players found'}\n        \n        # Create text representations\n        print(\"📝 Creating text representations...\")\n        texts = [self.create_text_representation(p) for p in players]\n        \n        # Generate embeddings in batches\n        print(f\"🔢 Generating embeddings with {info['name']} (batch_size={batch_size})...\")\n        model = self.embedding_models[model_name]\n        \n        all_embeddings = []\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            batch_emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n            all_embeddings.append(batch_emb)\n            if (i + batch_size) % 200 == 0:\n                print(f\"   Encoded {min(i + batch_size, len(texts))}/{len(texts)} texts...\")\n        \n        embeddings = np.vstack(all_embeddings)\n        print(f\"✅ Generated {len(embeddings)} embeddings of dimension {embeddings.shape[1]}\")\n        \n        # Store in Neo4j\n        print(f\"💾 Storing embeddings as '{info['property']}' in Neo4j...\")\n        \n        update_query = f\"\"\"\n            MATCH (p:Player {{player_name: $player_name}})\n            SET p.{info['property']} = $embedding,\n                p.{info['property']}_model = $model_name\n        \"\"\"\n        \n        stored = 0\n        with self.driver.session() as session:\n            for i, player in enumerate(players):\n                if i < len(embeddings):\n                    session.run(update_query, {\n                        'player_name': player['player_name'],\n                        'embedding': embeddings[i].tolist(),\n                        'model_name': info['name']\n                    })\n                    stored += 1\n                    \n                    if stored % 200 == 0:\n                        print(f\"   Stored {stored}/{len(players)} embeddings...\")\n        \n        print(f\"✅ Stored {stored} embeddings\")\n        \n        # Create index\n        idx_type = 'minilm' if model_name == 'model_1' else 'mpnet'\n        self.create_vector_index(idx_type)\n        \n        print(\"=\" * 70)\n        \n        return {\n            'model': info['name'],\n            'players_processed': len(players),\n            'embeddings_stored': stored,\n            'dimensions': info['dims']\n        }\n    \n    def store_all_embeddings(self) -> Dict:\n        \"\"\"\n        Store ALL THREE types of embeddings for complete comparison:\n        1. Numeric (12 dims)\n        2. MiniLM text (384 dims)\n        3. MPNet text (768 dims)\n        \n        This fully satisfies the project requirement to experiment with\n        at least TWO different embedding models.\n        \"\"\"\n        print(\"=\" * 80)\n        print(\"STORING ALL EMBEDDING TYPES FOR COMPARISON\")\n        print(\"=\" * 80)\n        \n        results = {}\n        \n        # 1. Numeric embeddings\n        print(\"\\n[1/3] NUMERIC EMBEDDINGS\")\n        results['numeric'] = self.store_embeddings_in_neo4j()\n        \n        # 2. MiniLM text embeddings\n        print(\"\\n[2/3] TEXT EMBEDDINGS (MiniLM)\")\n        results['minilm'] = self.store_text_embeddings_in_neo4j('model_1')\n        \n        # 3. MPNet text embeddings\n        print(\"\\n[3/3] TEXT EMBEDDINGS (MPNet)\")\n        results['mpnet'] = self.store_text_embeddings_in_neo4j('model_2')\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"ALL EMBEDDINGS STORED SUCCESSFULLY\")\n        print(\"=\" * 80)\n        \n        print(\"\\nSummary:\")\n        print(f\"  • Numeric: {results['numeric'].get('embeddings_stored', 0)} players, 12 dims\")\n        print(f\"  • MiniLM:  {results['minilm'].get('embeddings_stored', 0)} players, 384 dims\")\n        print(f\"  • MPNet:   {results['mpnet'].get('embeddings_stored', 0)} players, 768 dims\")\n        \n        return results\n    \n    def store_embeddings_in_neo4j(self) -> Dict:\n        \"\"\"\n        Main function to create and store embeddings for ALL players.\n        \n        Process:\n        1. Fetch all players with latest position (no duplicates)\n        2. Compute normalization statistics\n        3. Create numeric embeddings for each player\n        4. Store embeddings in Neo4j Player nodes\n        5. Create vector index for fast search\n        \n        Returns:\n            Dictionary with statistics about the operation\n        \"\"\"\n        print(\"=\" * 70)\n        print(\"STORING NUMERIC NODE EMBEDDINGS IN NEO4J\")\n        print(\"=\" * 70)\n        \n        # Step 1: Fetch all players\n        players = self.fetch_all_players_stats()\n        \n        if not players:\n            return {'error': 'No players found', 'count': 0}\n        \n        # Step 2 & 3: Create embeddings\n        embeddings, norm_stats = self.create_all_embeddings(players)\n        \n        # Step 4: Store in Neo4j\n        print(\"💾 Storing embeddings in Neo4j...\")\n        \n        update_query = \"\"\"\n            MATCH (p:Player {player_name: $player_name})\n            SET p.embedding = $embedding,\n                p.embedding_type = 'numeric',\n                p.embedding_dim = 12\n        \"\"\"\n        \n        stored = 0\n        with self.driver.session() as session:\n            for player in players:\n                name = player['player_name']\n                if name in embeddings:\n                    session.run(update_query, {\n                        'player_name': name,\n                        'embedding': embeddings[name].tolist()\n                    })\n                    stored += 1\n                    \n                    if stored % 200 == 0:\n                        print(f\"   Stored {stored}/{len(players)} embeddings...\")\n        \n        print(f\"✅ Stored {stored} embeddings in Neo4j\")\n        \n        # Step 5: Create vector index\n        self.create_vector_index()\n        \n        # Store normalization stats for later use\n        self._norm_stats = norm_stats\n        \n        print(\"=\" * 70)\n        print(\"✅ EMBEDDING STORAGE COMPLETE\")\n        print(\"=\" * 70)\n        \n        return {\n            'players_processed': len(players),\n            'embeddings_stored': stored,\n            'embedding_dimensions': 12,\n            'features': list(norm_stats.keys()),\n            'normalization_stats': {k: {'min': v['min'], 'max': v['max']} \n                                    for k, v in norm_stats.items()}\n        }\n    \n    def create_query_embedding(self, criteria: Dict) -> np.ndarray:\n        \"\"\"\n        Create an embedding vector from search criteria.\n        \n        Example criteria:\n        {'goals_per_game': 'high', 'assists_per_game': 'high', 'position': 'FWD'}\n        \n        Uses per-game average features to match the stored embeddings.\n        Converts to normalized vector for similarity search.\n        \"\"\"\n        # Default: mid-range values (0.5)\n        embedding = np.full(12, 0.5, dtype=np.float32)\n        \n        # Feature indices (per-game averages)\n        feature_map = {\n            # Per-game features (primary)\n            'goals_per_game': 0, 'assists_per_game': 1, 'avg_points_per_game': 2, \n            'clean_sheets_per_game': 3, 'avg_minutes': 4, 'avg_bonus': 5,\n            'form': 6, 'ict_index': 7, 'influence': 8, 'creativity': 9, \n            'threat': 10, 'games_played': 11,\n            # Legacy aliases (for backward compatibility)\n            'goals': 0, 'assists': 1, 'total_points': 2, 'points': 2,\n            'clean_sheets': 3, 'minutes': 4, 'bonus': 5\n        }\n        \n        for feature, value in criteria.items():\n            if feature in feature_map:\n                idx = feature_map[feature]\n                if value == 'high':\n                    embedding[idx] = 1.0\n                elif value == 'low':\n                    embedding[idx] = 0.0\n                elif isinstance(value, (int, float)):\n                    embedding[idx] = min(max(value, 0), 1)\n        \n        return embedding\n    \n    def semantic_search(self, query_embedding: np.ndarray, top_k: int = 10, \n                    position: str = None, embedding_type: str = 'numeric') -> List[Dict]:\n        \"\"\"\n        Find similar players using cosine similarity with any embedding type.\n        \n        Args:\n            query_embedding: Query vector (dims depend on embedding_type)\n            top_k: Number of results to return\n            position: Optional position filter (GK, DEF, MID, FWD)\n            embedding_type: 'numeric' (12d), 'minilm' (384d), or 'mpnet' (768d)\n        \"\"\"\n        # Map embedding type to Neo4j property\n        property_map = {\n            'numeric': 'embedding',\n            'minilm': 'embedding_minilm',\n            'mpnet': 'embedding_mpnet'\n        }\n        \n        if embedding_type not in property_map:\n            raise ValueError(f\"Unsupported embedding_type: {embedding_type}\")\n        \n        emb_prop = property_map[embedding_type]\n        \n        # Build query\n        if position:\n            query = f\"\"\"\n                MATCH (p:Player)\n                WHERE p.{emb_prop} IS NOT NULL\n                OPTIONAL MATCH (p)-[:PLAYS_AS]->(pos:Position {{name: $position}})\n                WITH p.player_element AS player_id,\n                    COLLECT(DISTINCT p.player_name)[0] AS player,\n                    COLLECT(DISTINCT pos.name)[0] AS position,\n                    p.{emb_prop} AS embedding\n                WHERE player IS NOT NULL AND position = $position\n                RETURN player, position, embedding\n            \"\"\"\n            params = {'position': position}\n        else:\n            query = f\"\"\"\n                MATCH (p:Player)\n                WHERE p.{emb_prop} IS NOT NULL\n                OPTIONAL MATCH (p)-[:PLAYS_AS]->(pos:Position)\n                WITH p.player_element AS player_id,\n                    COLLECT(DISTINCT p.player_name)[0] AS player,\n                    COLLECT(DISTINCT pos.name)[0] AS position,\n                    p.{emb_prop} AS embedding\n                WHERE player IS NOT NULL\n                RETURN player, position, embedding\n            \"\"\"\n            params = {}\n        \n        with self.driver.session() as session:\n            result = session.run(query, params)\n            players = [dict(record) for record in result]\n        \n        if not players:\n            return []\n        \n        # Compute cosine similarity\n        query_norm = query_embedding / (np.linalg.norm(query_embedding) + 1e-10)\n        \n        similarities = []\n        seen_players = set()\n        \n        for player in players:\n            player_name = player['player']\n            if player_name in seen_players:\n                continue\n            seen_players.add(player_name)\n            \n            player_emb = np.array(player['embedding'], dtype=np.float32)\n            player_norm = player_emb / (np.linalg.norm(player_emb) + 1e-10)\n            similarity = float(np.dot(query_norm, player_norm))\n            \n            similarities.append({\n                'player': player_name,\n                'position': player['position'],\n                'similarity_score': round(similarity, 4),\n                'embedding_type': embedding_type\n            })\n        \n        similarities.sort(key=lambda x: x['similarity_score'], reverse=True)\n        return similarities[:top_k]\n    \n    def search_with_text_embedding(self, query: str, model_name: str = 'model_1',\n                                   top_k: int = 10, position: str = None) -> Dict:\n        \"\"\"\n        Search using text embeddings (MiniLM or MPNet).\n        \n        Args:\n            query: Natural language query\n            model_name: 'model_1' (MiniLM) or 'model_2' (MPNet)\n            top_k: Number of results\n            position: Optional position filter\n        \"\"\"\n        model = self.embedding_models[model_name]\n        query_emb = model.encode([query], convert_to_numpy=True)[0]\n        \n        emb_type = 'minilm' if model_name == 'model_1' else 'mpnet'\n        results = self.semantic_search(query_emb, top_k, position, emb_type)\n        \n        return {\n            'method': f'text_{emb_type}',\n            'model': model_name,\n            'query': query,\n            'position_filter': position,\n            'results': results\n        }\n    \n    def compare_all_embedding_models(self, position: str = 'FWD', top_k: int = 10) -> Dict:\n        \"\"\"\n        Compare search results across all THREE embedding approaches.\n        \n        This is the main comparison function for the project requirement:\n        \"experiment with at least TWO different embedding models for comparison\"\n        \n        Args:\n            position: Position to filter (FWD, MID, DEF, GK)\n            top_k: Number of results per approach\n        \"\"\"\n        print(\"=\" * 80)\n        print(f\"COMPARING ALL EMBEDDING MODELS (Position: {position})\")\n        print(\"=\" * 80)\n        \n        results = {}\n        \n        # 1. Numeric search (criteria-based)\n        print(\"\\n📊 [1/3] Numeric Embeddings (12 dims)\")\n        criteria = {'goals': 'high', 'total_points': 'high', 'assists': 'high'}\n        query_numeric = self.create_query_embedding(criteria)\n        results['numeric'] = {\n            'dims': 12,\n            'criteria': criteria,\n            'results': self.semantic_search(query_numeric, top_k, position, 'numeric')\n        }\n        \n        # 2. MiniLM search (semantic)\n        print(\"📊 [2/3] MiniLM Text Embeddings (384 dims)\")\n        query_text = f\"Best {position} player with high goals and assists\"\n        results['minilm'] = self.search_with_text_embedding(query_text, 'model_1', top_k, position)\n        results['minilm']['dims'] = 384\n        \n        # 3. MPNet search (semantic)\n        print(\"📊 [3/3] MPNet Text Embeddings (768 dims)\")\n        results['mpnet'] = self.search_with_text_embedding(query_text, 'model_2', top_k, position)\n        results['mpnet']['dims'] = 768\n        \n        # Display results\n        print(\"\\n\" + \"─\" * 80)\n        print(\"SEARCH RESULTS COMPARISON\")\n        print(\"─\" * 80)\n        \n        print(f\"\\n{'Rank':<6} {'Numeric (12d)':<25} {'MiniLM (384d)':<25} {'MPNet (768d)':<25}\")\n        print(\"─\" * 85)\n        \n        for i in range(min(top_k, 10)):\n            num_res = results['numeric']['results'][i] if i < len(results['numeric']['results']) else {}\n            mini_res = results['minilm']['results'][i] if i < len(results['minilm']['results']) else {}\n            mpn_res = results['mpnet']['results'][i] if i < len(results['mpnet']['results']) else {}\n            \n            num_name = num_res.get('player', '-')[:22] if num_res else '-'\n            mini_name = mini_res.get('player', '-')[:22] if mini_res else '-'\n            mpn_name = mpn_res.get('player', '-')[:22] if mpn_res else '-'\n            \n            print(f\"{i+1:<6} {num_name:<25} {mini_name:<25} {mpn_name:<25}\")\n        \n        # Calculate agreement\n        print(\"\\n\" + \"─\" * 80)\n        print(\"MODEL AGREEMENT ANALYSIS\")\n        print(\"─\" * 80)\n        \n        top10_numeric = set(r['player'] for r in results['numeric']['results'][:10])\n        top10_minilm = set(r['player'] for r in results['minilm']['results'][:10])\n        top10_mpnet = set(r['player'] for r in results['mpnet']['results'][:10])\n        \n        print(f\"\\n   Numeric ∩ MiniLM: {len(top10_numeric & top10_minilm)}/10 players agree\")\n        print(f\"   Numeric ∩ MPNet:  {len(top10_numeric & top10_mpnet)}/10 players agree\")\n        print(f\"   MiniLM ∩ MPNet:   {len(top10_minilm & top10_mpnet)}/10 players agree\")\n        print(f\"   All three agree:  {len(top10_numeric & top10_minilm & top10_mpnet)}/10 players\")\n        \n        print(\"\\n\" + \"=\" * 80)\n        \n        return results\n    \n    def embedding_retrieve(self, criteria: Dict = None, position: str = None, \n                        top_k: int = 10, embedding_type: str = 'numeric') -> Dict[str, Any]:\n        \"\"\"\n        Retrieve similar players based on criteria using specified embedding type.\n        \n        Args:\n            criteria: Dictionary of desired attributes\n            position: Filter by position (GK, DEF, MID, FWD)\n            top_k: Number of results\n            embedding_type: 'numeric', 'minilm', or 'mpnet'\n        \"\"\"\n        if criteria is None:\n            criteria = {'total_points': 'high'}\n        \n        if embedding_type == 'numeric':\n            # For numeric embeddings, create query vector from criteria\n            query_embedding = self.create_query_embedding(criteria)\n        else:\n            # For text embeddings, convert criteria to text query\n            query_text = self._criteria_to_text_query(criteria, position)\n            model_name = 'model_1' if embedding_type == 'minilm' else 'model_2'\n            model = self.embedding_models[model_name]\n            query_embedding = model.encode([query_text], convert_to_numpy=True)[0]\n        \n        results = self.semantic_search(query_embedding, top_k, position, embedding_type)\n        \n        return {\n            'method': f'{embedding_type}_embedding',\n            'criteria': criteria,\n            'position_filter': position,\n            'embedding_type': embedding_type,\n            'embedding_dimensions': query_embedding.shape[0],\n            'results': results\n        }\n    \n    def find_similar_players(self, player_name: str, top_k: int = 10, \n                             same_position: bool = True) -> List[Dict]:\n        \"\"\"\n        Find players similar to a given player.\n        \n        How it works:\n        1. Get the target player's embedding from Neo4j\n        2. Search for players with similar embeddings\n        3. Optionally filter to same position\n        \n        Args:\n            player_name: Name of the player to find similar to\n            top_k: Number of similar players to return\n            same_position: If True, only return players of same position\n        \"\"\"\n        # Get target player's embedding and position (handle multiple matches)\n        query = \"\"\"\n            MATCH (p:Player {player_name: $player_name})\n            OPTIONAL MATCH (p)-[:PLAYS_AS]->(pos:Position)\n            WHERE p.embedding IS NOT NULL\n            WITH p.embedding AS embedding, \n                 COLLECT(DISTINCT pos.name)[0] AS position\n            RETURN embedding, position\n            LIMIT 1\n        \"\"\"\n        \n        with self.driver.session() as session:\n            result = session.run(query, {'player_name': player_name})\n            record = result.single()\n            \n            if not record or record['embedding'] is None:\n                return [{'error': f'Player \"{player_name}\" not found or has no embedding'}]\n            \n            target_embedding = np.array(record['embedding'], dtype=np.float32)\n            target_position = record['position'] if same_position else None\n        \n        # Find similar players (excluding the target player)\n        results = self.semantic_search(target_embedding, top_k + 1, target_position)\n        \n        # Remove the target player from results\n        results = [r for r in results if r['player'] != player_name][:top_k]\n        \n        return results\n\n    # =========================================================================\n    # COMBINED RETRIEVAL\n    # =========================================================================\n    \n    def retrieve(self, user_input: str, method: str = 'both', \n                embedding_type: str = 'numeric') -> Dict[str, Any]:\n        \"\"\"\n        Main retrieval method combining baseline and embeddings.\n        \n        Args:\n            user_input: Raw user query\n            method: 'baseline', 'embedding', or 'both'\n            embedding_type: 'numeric', 'minilm', or 'mpnet' - which embedding to use\n        \"\"\"\n        results = {'user_input': user_input}\n        \n        # Always run baseline for intent/entity extraction\n        baseline_result = self.baseline_retrieve(user_input)\n        results['baseline'] = baseline_result\n        \n        preprocessed = self.preprocessor.preprocess(user_input, include_embedding=False)\n        query_used = baseline_result.get('query_used', 'top_scorers')\n        entities = preprocessed.get('entities', {})\n        \n        if method in ['embedding', 'both']:\n            # Determine if this query type can benefit from embeddings\n            # ALL player-centric queries should use embeddings\n            player_centric_queries = {\n                'top_players_by_position',      # ✓ Player similarity by position\n                'top_scorers',                   # ✓ Player similarity by goals\n                'top_assisters',                 # ✓ Player similarity by assists\n                'clean_sheet_leaders',           # ✓ Defender/GK similarity\n                'players_by_form',               # ✓ Player similarity by form\n                'bonus_leaders',                 # ✓ Player similarity by bonus points\n                'most_cards',                    # ✓ Player similarity by cards\n                'player_season_summary',         # ✓ Find similar players to reference player\n                'compare_players',               # ✓ Find similar players to both\n                'players_by_team',               # ✓ Find similar players on other teams\n                'player_gameweek_performance',   # ✓ Find similar players with similar performances\n            }\n            \n            if query_used in player_centric_queries:\n                # Extract position from query if available\n                positions = entities.get('positions', [])\n                position = positions[0] if positions else None\n                \n                # Get search criteria from preprocessed query\n                search_criteria = preprocessed.get('search_criteria', {})\n                \n                if embedding_type == 'numeric':\n                    # Use numeric embeddings\n                    results['embedding'] = self.embedding_retrieve(\n                        criteria=search_criteria, \n                        position=position, \n                        top_k=10\n                    )\n                elif embedding_type == 'minilm':\n                    # Use MiniLM text embeddings\n                    # Convert search criteria to natural language query\n                    query_text = self._criteria_to_text_query(search_criteria, position)\n                    results['embedding'] = self.search_with_text_embedding(\n                        query=query_text,\n                        model_name='model_1',\n                        top_k=10,\n                        position=position\n                    )\n                elif embedding_type == 'mpnet':\n                    # Use MPNet text embeddings\n                    query_text = self._criteria_to_text_query(search_criteria, position)\n                    results['embedding'] = self.search_with_text_embedding(\n                        query=query_text,\n                        model_name='model_2',\n                        top_k=10,\n                        position=position\n                    )\n                \n                results['embedding']['embedding_type'] = embedding_type\n            else:\n                results['embedding'] = {\n                    'method': 'skipped',\n                    'reason': f'Query \"{query_used}\" may not benefit from player embeddings',\n                    'embedding_type': embedding_type\n                }\n        \n        if method == 'both' and 'embedding' in results:\n            # Merge results if embeddings were actually used\n            embedding_results = results.get('embedding', {}).get('results', [])\n            baseline_results = results.get('baseline', {}).get('results', [])\n            \n            if embedding_results:  # Only merge if embeddings returned results\n                results['combined'] = self._merge_results(baseline_results, embedding_results)\n            else:\n                # Use only baseline results if embeddings skipped or empty\n                results['combined'] = baseline_results\n        \n        return results\n\n    def _criteria_to_text_query(self, criteria: Dict, position: str = None) -> str:\n        \"\"\"\n        Convert search criteria to natural language query for text embeddings.\n        \n        Example: {'goals': 'high', 'assists': 'high'} -> \n                \"Players with high goals and assists\"\n        \"\"\"\n        parts = []\n        \n        # Map criteria values to natural language\n        value_map = {\n            'high': 'high',\n            'low': 'low',\n            'good': 'good',\n            'bad': 'poor'\n        }\n        \n        # Map criteria keys to natural language\n        criteria_map = {\n            'goals': 'goals',\n            'assists': 'assists', \n            'total_points': 'FPL points',\n            'clean_sheets': 'clean sheets',\n            'bonus': 'bonus points',\n            'minutes': 'minutes played',\n            'form': 'form',\n            'ict_index': 'ICT index',\n            'influence': 'influence',\n            'creativity': 'creativity',\n            'threat': 'threat',\n            'cards': 'cards',\n            'saves': 'saves',\n            'value': 'value',\n            'ownership': 'ownership'\n        }\n        \n        for key, value in criteria.items():\n            if key in criteria_map and value in value_map:\n                parts.append(f\"{value_map[value]} {criteria_map[key]}\")\n        \n        # Build the query\n        if position:\n            position_map = {\n                'FWD': 'forwards',\n                'MID': 'midfielders', \n                'DEF': 'defenders',\n                'GK': 'goalkeepers'\n            }\n            position_text = position_map.get(position, 'players')\n            if parts:\n                query = f\"{position_text} with \" + \" and \".join(parts)\n            else:\n                query = f\"best {position_text}\"\n        else:\n            if parts:\n                query = \"Players with \" + \" and \".join(parts)\n            else:\n                query = \"best players\"\n        \n        return query    \n    def _merge_results(self, baseline_results: List, embedding_results: List) -> List:\n        \"\"\"Merge results from both methods, removing duplicates.\"\"\"\n        combined = baseline_results.copy() if baseline_results else []\n        seen_players = {r.get('player') for r in combined if r.get('player')}\n        \n        for result in (embedding_results or []):\n            if result.get('player') not in seen_players:\n                combined.append(result)\n                seen_players.add(result.get('player'))\n        \n        return combined\n    \n    def compare_embedding_approaches(self, player_name: str = \"Erling Haaland\") -> Dict:\n        \"\"\"\n        Compare numeric embeddings vs text-based embeddings.\n        \n        This demonstrates the two approaches:\n        1. Numeric: Direct feature vectors [goals, assists, ...]\n        2. Text-based: Convert stats to text, then embed with transformer\n        \"\"\"\n        print(\"=\" * 70)\n        print(\"COMPARING EMBEDDING APPROACHES\")\n        print(\"=\" * 70)\n        \n        # Get player stats\n        query = \"\"\"\n            MATCH (p:Player {player_name: $name})-[:PLAYS_AS]->(pos:Position)\n            OPTIONAL MATCH (p)-[played:PLAYED_IN]->(f:Fixture)\n            RETURN p.player_name AS player_name,\n                   pos.name AS position,\n                   COALESCE(SUM(played.total_points), 0) AS total_points,\n                   COALESCE(SUM(played.goals_scored), 0) AS goals,\n                   COALESCE(SUM(played.assists), 0) AS assists,\n                   p.embedding AS numeric_embedding\n        \"\"\"\n        \n        with self.driver.session() as session:\n            result = session.run(query, {'name': player_name})\n            record = result.single()\n        \n        if not record:\n            return {'error': f'Player {player_name} not found'}\n        \n        player_data = dict(record)\n        \n        # Approach 1: Numeric embedding (already stored)\n        numeric_emb = player_data.get('numeric_embedding')\n        \n        # Approach 2: Text-based embedding\n        text_repr = (\n            f\"Player {player_data['player_name']} is a {player_data['position']} \"\n            f\"with {player_data['goals']} goals, {player_data['assists']} assists, \"\n            f\"and {player_data['total_points']} total points.\"\n        )\n        \n        text_embeddings = {}\n        for model_name, model in self.embedding_models.items():\n            text_emb = model.encode([text_repr], convert_to_numpy=True)[0]\n            text_embeddings[model_name] = {\n                'dimensions': len(text_emb),\n                'sample': text_emb[:5].tolist()\n            }\n        \n        return {\n            'player': player_name,\n            'numeric_embedding': {\n                'approach': 'Direct numeric features',\n                'dimensions': 12,\n                'features': ['goals', 'assists', 'points', 'clean_sheets', \n                            'minutes', 'bonus', 'form', 'ict', 'influence', \n                            'creativity', 'threat', 'games'],\n                'values': numeric_emb[:5] if numeric_emb else None,\n                'pros': ['Fast computation', 'Preserves exact relationships', 'Small storage'],\n                'cons': ['Cannot handle semantic queries', 'Fixed features only']\n            },\n            'text_embeddings': {\n                'approach': 'Text-based (sentence transformers)',\n                'models': text_embeddings,\n                'pros': ['Handles semantic queries', 'Flexible input'],\n                'cons': ['Slower', 'Larger storage', 'Indirect representation']\n            }\n        }\n\n\n# =============================================================================\n# TESTING FUNCTIONS\n# =============================================================================\n\ndef test_numeric_embeddings():\n    \"\"\"Test the numeric embedding functionality.\"\"\"\n    print(\"=\" * 70)\n    print(\"NUMERIC NODE EMBEDDINGS - TEST SUITE\")\n    print(\"=\" * 70)\n    \n    # Initialize\n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n    \n    # Test 1: Fetch all players\n    print(\"\\n\" + \"─\" * 70)\n    print(\"TEST 1: Fetch All Players (No Duplicates)\")\n    print(\"─\" * 70)\n    players = retriever.fetch_all_players_stats()\n    print(f\"✅ Total unique players: {len(players)}\")\n    \n    # Check for duplicates\n    names = [p['player_name'] for p in players]\n    unique_names = set(names)\n    print(f\"✅ Unique names: {len(unique_names)} (duplicates: {len(names) - len(unique_names)})\")\n    \n    # Position distribution\n    positions = {}\n    for p in players:\n        pos = p['position']\n        positions[pos] = positions.get(pos, 0) + 1\n    print(f\"✅ Position distribution: {positions}\")\n    \n    # Test 2: Store embeddings\n    print(\"\\n\" + \"─\" * 70)\n    print(\"TEST 2: Store Embeddings in Neo4j\")\n    print(\"─\" * 70)\n    result = retriever.store_embeddings_in_neo4j()\n    print(f\"✅ Result: {result}\")\n    \n    # Test 3: Search by criteria\n    print(\"\\n\" + \"─\" * 70)\n    print(\"TEST 3: Search by Criteria\")\n    print(\"─\" * 70)\n    \n    test_searches = [\n        {'criteria': {'goals': 'high', 'assists': 'high'}, 'position': 'FWD', 'desc': 'High-scoring forwards'},\n        {'criteria': {'clean_sheets': 'high'}, 'position': 'GK', 'desc': 'Goalkeepers with clean sheets'},\n        {'criteria': {'creativity': 'high', 'assists': 'high'}, 'position': 'MID', 'desc': 'Creative midfielders'},\n        {'criteria': {'clean_sheets': 'high', 'bonus': 'high'}, 'position': 'DEF', 'desc': 'High-performing defenders'},\n    ]\n    \n    for search in test_searches:\n        print(f\"\\n🔍 {search['desc']} ({search['position']}):\")\n        results = retriever.embedding_retrieve(\n            criteria=search['criteria'], \n            position=search['position'],\n            top_k=5\n        )\n        for i, r in enumerate(results['results'][:5], 1):\n            print(f\"   {i}. {r['player']} ({r['position']}) - Similarity: {r['similarity_score']:.3f}\")\n    \n    # Test 4: Find similar players\n    print(\"\\n\" + \"─\" * 70)\n    print(\"TEST 4: Find Similar Players\")\n    print(\"─\" * 70)\n    \n    test_players = [\"Erling Haaland\", \"Kevin De Bruyne\", \"Virgil van Dijk\"]\n    \n    for player in test_players:\n        print(f\"\\n🔍 Players similar to {player}:\")\n        similar = retriever.find_similar_players(player, top_k=5, same_position=True)\n        if 'error' in similar[0] if similar else True:\n            print(f\"   ⚠️ {similar[0].get('error', 'Not found')}\")\n        else:\n            for i, r in enumerate(similar[:5], 1):\n                print(f\"   {i}. {r['player']} ({r['position']}) - Similarity: {r['similarity_score']:.3f}\")\n    \n    # Close connection\n    retriever.close()\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"✅ ALL TESTS COMPLETE\")\n    print(\"=\" * 70)\n\n\ndef test_plays_for_relationships():\n    \"\"\"Verify `PLAYS_FOR` relationships exist and are consistent with embeddings.\n\n    - Runs a simple Cypher check: `MATCH p=()-[:PLAYS_FOR]->() RETURN p;`\n    - Ensures that players which have embeddings in Neo4j also have a `PLAYS_FOR` relationship\n    - Prints summary statistics and a small sample for manual inspection\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"PLAYS_FOR RELATIONSHIP & EMBEDDING CONSISTENCY CHECK\")\n    print(\"=\" * 70)\n\n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n\n    # Ensure embeddings exist (store numeric embeddings if missing)\n    print(\"\\n🔁 Ensuring numeric embeddings are stored (this may take a while)...\")\n    store_res = retriever.store_embeddings_in_neo4j()\n    print(f\"   Store result: {store_res}\")\n\n    # 1) Count PLAYS_FOR relationships and show a small sample\n    with retriever.driver.session() as session:\n        count_res = session.run(\"MATCH ()-[:PLAYS_FOR]->() RETURN count(*) AS rel_count\")\n        rel_count = count_res.single()['rel_count']\n\n        sample_res = session.run(\n            \"MATCH (p:Player)-[r:PLAYS_FOR]->(t:Team) RETURN p.player_name AS player, t.name AS team LIMIT 10\"\n        )\n        sample = [dict(rec) for rec in sample_res]\n\n        # 2) Find how many players with embeddings are missing PLAYS_FOR\n        missing_res = session.run(\n            \"MATCH (p:Player) WHERE p.embedding IS NOT NULL AND NOT (p)-[:PLAYS_FOR]->() RETURN count(p) AS missing\"\n        )\n        missing = missing_res.single()['missing']\n\n    print(f\"\\n✅ Total PLAYS_FOR relationships: {rel_count}\")\n    print(f\"✅ Players with embeddings but missing PLAYS_FOR: {missing}\")\n    print(\"\\nSample PLAYS_FOR rows:\")\n    for row in sample:\n        print(f\"   - {row['player']} -> {row['team']}\")\n\n    retriever.close()\n\n    print(\"\\n\" + \"=\" * 70)\n    print(\"PLAYS_FOR CHECK COMPLETE\")\n    print(\"=\" * 70)\n\n\ndef test_all_embedding_approaches():\n    \"\"\"\n    Compare ALL THREE embedding approaches:\n    1. Numeric embeddings (direct feature vectors)\n    2. Text embedding with MiniLM (model_1)\n    3. Text embedding with MPNet (model_2)\n    \"\"\"\n    import time\n    \n    print(\"=\" * 80)\n    print(\"COMPLETE EMBEDDING APPROACH COMPARISON\")\n    print(\"Numeric Features vs Text-Based (MiniLM) vs Text-Based (MPNet)\")\n    print(\"=\" * 80)\n    \n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n    \n    print(\"\\n📊 Fetching player data...\")\n    players = retriever.fetch_all_players_stats()[:200]\n    print(f\"   Loaded {len(players)} players for testing\")\n    \n    # ─────────────────────────────────────────────────────────────\n    print(\"\\n\" + \"─\" * 80)\n    print(\"APPROACH SPECIFICATIONS\")\n    print(\"─\" * 80)\n    \n    print(\"\"\"\n    ┌─────────────────────────────────────────────────────────────────────────┐\n    │ NUMERIC FEATURES (12 dimensions)                                        │\n    │ • Direct normalized stats: [goals, assists, points, clean_sheets, ...]  │\n    │ • No text conversion needed                                             │\n    │ • Fast and compact                                                      │\n    ├─────────────────────────────────────────────────────────────────────────┤\n    │ TEXT + MiniLM (384 dimensions)                                          │\n    │ • Convert stats to text description                                     │\n    │ • Embed with all-MiniLM-L6-v2 (22M params)                              │\n    │ • Good speed/accuracy tradeoff                                          │\n    ├─────────────────────────────────────────────────────────────────────────┤\n    │ TEXT + MPNet (768 dimensions)                                           │\n    │ • Convert stats to text description                                     │\n    │ • Embed with all-mpnet-base-v2 (109M params)                            │\n    │ • Best semantic understanding                                           │\n    └─────────────────────────────────────────────────────────────────────────┘\n    \"\"\")\n    \n    # ─────────────────────────────────────────────────────────────\n    print(\"─\" * 80)\n    print(\"EMBEDDING GENERATION SPEED\")\n    print(\"─\" * 80)\n    \n    embeddings = {}\n    times = {}\n    \n    # 1. Numeric embeddings\n    start = time.time()\n    norm_stats = retriever.compute_normalization_stats(players)\n    numeric_embs = [retriever.create_numeric_embedding(p, norm_stats) for p in players]\n    times['numeric'] = time.time() - start\n    embeddings['numeric'] = numeric_embs\n    \n    # 2. Create text representations\n    texts = []\n    for p in players:\n        text = (f\"Football player {p['player_name']} plays as {p['position']}. \"\n                f\"Stats: {p['total_points']} points, {p['goals']} goals, \"\n                f\"{p['assists']} assists, {p['clean_sheets']} clean sheets.\")\n        texts.append(text)\n    \n    # 3. MiniLM embeddings\n    start = time.time()\n    text_embs_minilm = retriever.embedding_models['model_1'].encode(texts, convert_to_numpy=True)\n    times['minilm'] = time.time() - start\n    embeddings['minilm'] = text_embs_minilm\n    \n    # 4. MPNet embeddings\n    start = time.time()\n    text_embs_mpnet = retriever.embedding_models['model_2'].encode(texts, convert_to_numpy=True)\n    times['mpnet'] = time.time() - start\n    embeddings['mpnet'] = text_embs_mpnet\n    \n    print(f\"\\n{'Approach':<20} {'Dims':<8} {'Time':<10} {'Speed':<15} {'Storage':<12}\")\n    print(\"─\" * 70)\n    specs = [\n        ('Numeric', 12, times['numeric']),\n        ('Text+MiniLM', 384, times['minilm']),\n        ('Text+MPNet', 768, times['mpnet'])\n    ]\n    for name, dims, t in specs:\n        storage = dims * 4  # bytes per float32\n        print(f\"{name:<20} {dims:<8} {t:<10.3f}s {len(players)/t:<15.0f}/sec {storage:<12} bytes\")\n    \n    print(f\"\\n🏆 Numeric is {times['mpnet']/times['numeric']:.0f}x faster than MPNet\")\n    \n    # ─────────────────────────────────────────────────────────────\n    print(\"\\n\" + \"─\" * 80)\n    print(\"SEARCH RESULTS COMPARISON\")\n    print(\"─\" * 80)\n    \n    # Filter to forwards\n    fwd_idx = [i for i, p in enumerate(players) if p['position'] == 'FWD']\n    print(f\"\\nSearching among {len(fwd_idx)} forwards for 'top goal scorer'\")\n    \n    # Numeric search\n    print(\"\\n📊 NUMERIC APPROACH (criteria: goals=high, points=high):\")\n    query_num = retriever.create_query_embedding({'goals': 'high', 'total_points': 'high'})\n    q_norm = query_num / (np.linalg.norm(query_num) + 1e-10)\n    \n    num_results = []\n    for i in fwd_idx:\n        e = embeddings['numeric'][i]\n        e_norm = e / (np.linalg.norm(e) + 1e-10)\n        sim = float(np.dot(q_norm, e_norm))\n        num_results.append((players[i]['player_name'], sim, players[i]['goals']))\n    num_results.sort(key=lambda x: x[1], reverse=True)\n    \n    for rank, (name, sim, goals) in enumerate(num_results[:5], 1):\n        print(f\"   {rank}. {name:<25} Sim: {sim:.4f}  Goals: {goals}\")\n    \n    # Text searches\n    query_text = \"Top scoring forward striker with many goals\"\n    \n    for label, key, model_key in [('MiniLM', 'minilm', 'model_1'), ('MPNet', 'mpnet', 'model_2')]:\n        print(f\"\\n📊 TEXT + {label} (query: \\\"{query_text}\\\"):\")\n        \n        q_emb = retriever.embedding_models[model_key].encode([query_text], convert_to_numpy=True)[0]\n        q_norm = q_emb / (np.linalg.norm(q_emb) + 1e-10)\n        \n        txt_results = []\n        for i in fwd_idx:\n            e = embeddings[key][i]\n            e_norm = e / (np.linalg.norm(e) + 1e-10)\n            sim = float(np.dot(q_norm, e_norm))\n            txt_results.append((players[i]['player_name'], sim, players[i]['goals']))\n        txt_results.sort(key=lambda x: x[1], reverse=True)\n        \n        for rank, (name, sim, goals) in enumerate(txt_results[:5], 1):\n            print(f\"   {rank}. {name:<25} Sim: {sim:.4f}  Goals: {goals}\")\n    \n    # ─────────────────────────────────────────────────────────────\n    print(\"\\n\" + \"─\" * 80)\n    print(\"TOP-10 RANKING AGREEMENT\")\n    print(\"─\" * 80)\n    \n    # Get top 10 from each for all players\n    all_top10 = {}\n    \n    # Numeric\n    q = retriever.create_query_embedding({'goals': 'high', 'total_points': 'high', 'assists': 'high'})\n    q_n = q / (np.linalg.norm(q) + 1e-10)\n    res = [(players[i]['player_name'], float(np.dot(q_n, embeddings['numeric'][i]/np.linalg.norm(embeddings['numeric'][i]+1e-10)))) for i in range(len(players))]\n    res.sort(key=lambda x: x[1], reverse=True)\n    all_top10['Numeric'] = [x[0] for x in res[:10]]\n    \n    # Text models\n    q_txt = \"Best player with high goals assists and points\"\n    for label, key, mkey in [('MiniLM', 'minilm', 'model_1'), ('MPNet', 'mpnet', 'model_2')]:\n        q_emb = retriever.embedding_models[mkey].encode([q_txt], convert_to_numpy=True)[0]\n        q_n = q_emb / (np.linalg.norm(q_emb) + 1e-10)\n        res = []\n        for i in range(len(players)):\n            e = embeddings[key][i]\n            e_n = e / (np.linalg.norm(e) + 1e-10)\n            res.append((players[i]['player_name'], float(np.dot(q_n, e_n))))\n        res.sort(key=lambda x: x[1], reverse=True)\n        all_top10[label] = [x[0] for x in res[:10]]\n    \n    print(f\"\\n{'Rank':<6} {'Numeric':<22} {'MiniLM':<22} {'MPNet':<22}\")\n    print(\"─\" * 75)\n    for i in range(10):\n        print(f\"{i+1:<6} {all_top10['Numeric'][i]:<22} {all_top10['MiniLM'][i]:<22} {all_top10['MPNet'][i]:<22}\")\n    \n    # Overlaps\n    s1, s2, s3 = set(all_top10['Numeric']), set(all_top10['MiniLM']), set(all_top10['MPNet'])\n    print(f\"\\n📊 Agreement:\")\n    print(f\"   Numeric ∩ MiniLM: {len(s1&s2)}/10\")\n    print(f\"   Numeric ∩ MPNet:  {len(s1&s3)}/10\")\n    print(f\"   MiniLM ∩ MPNet:   {len(s2&s3)}/10\")\n    print(f\"   All three:        {len(s1&s2&s3)}/10\")\n    \n    retriever.close()\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"SUMMARY\")\n    print(\"=\" * 80)\n    print(\"\"\"\n    ╔═══════════════════════════════════════════════════════════════════════════╗\n    ║ WHEN TO USE EACH APPROACH                                                 ║\n    ╠═══════════════════════════════════════════════════════════════════════════╣\n    ║                                                                           ║\n    ║ NUMERIC EMBEDDINGS:                                                       ║\n    ║   → \"Find players with high goals and low price\"                          ║\n    ║   → \"Who has the best form among midfielders?\"                            ║\n    ║   → Stats-based filtering and ranking                                     ║\n    ║                                                                           ║\n    ║ TEXT + MiniLM:                                                            ║\n    ║   → \"Find a creative playmaker\"                                           ║\n    ║   → \"Who is similar to De Bruyne?\"                                        ║\n    ║   → Real-time semantic search with good accuracy                          ║\n    ║                                                                           ║\n    ║ TEXT + MPNet:                                                             ║\n    ║   → \"Find a clinical finisher who performs in big games\"                  ║\n    ║   → Complex semantic understanding                                        ║\n    ║   → When accuracy matters more than speed                                 ║\n    ║                                                                           ║\n    ╚═══════════════════════════════════════════════════════════════════════════╝\n    \"\"\")\n\n\ndef test_two_experiments():\n    \"\"\"\n    Test the TWO EXPERIMENTS required by the project:\n    \n    Experiment 1: Baseline only (Cypher queries)\n    Experiment 2: Baseline + Embeddings (combined results)\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"TWO EXPERIMENTS - BASELINE vs BASELINE + EMBEDDINGS\")\n    print(\"=\" * 80)\n    \n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n\n    # Ensure numeric embeddings are stored before running embedding experiments\n    print(\"\\n🔁 Ensuring numeric embeddings are stored (required for Experiment 2)...\")\n    store_info = retriever.store_embeddings_in_neo4j()\n    print(f\"   Embedding storage summary: {store_info}\")\n    \n    test_queries = [\n        \"Who are the top forwards in 2022?\",\n        \"Best midfielders with assists\",\n        \"Top defenders this season\",\n        \"Goalkeepers with clean sheets\"\n    ]\n    \n    for query in test_queries:\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"QUERY: \\\"{query}\\\"\")\n        print(\"=\" * 80)\n        \n        # ─────────────────────────────────────────────────────────────\n        # EXPERIMENT 1: BASELINE ONLY (Cypher Queries)\n        # ─────────────────────────────────────────────────────────────\n        print(\"\\n📊 EXPERIMENT 1: BASELINE ONLY (Cypher Queries)\")\n        print(\"─\" * 60)\n        \n        baseline_result = retriever.baseline_retrieve(query)\n        \n        print(f\"   Intent: {baseline_result.get('intent')}\")\n        print(f\"   Query Used: {baseline_result.get('query_used')}\")\n        print(f\"   Parameters: {baseline_result.get('parameters')}\")\n        \n        if baseline_result.get('error'):\n            print(f\"   ❌ Error: {baseline_result.get('error')}\")\n        else:\n            print(f\"   Results ({len(baseline_result.get('results', []))} found):\")\n            for i, r in enumerate(baseline_result.get('results', [])[:5], 1):\n                player = r.get('player', r.get('player_name', 'Unknown'))\n                points = r.get('total_points', r.get('points', '-'))\n                print(f\"      {i}. {player} - Points: {points}\")\n\n        # Post-check: verify that baseline result players have embeddings and PLAYS_FOR\n        players_checked = baseline_result.get('results', [])[:10]\n        with retriever.driver.session() as session:\n            emb_missing = 0\n            plays_for_missing = 0\n            for p in players_checked:\n                name = p.get('player') or p.get('player_name')\n                rec = session.run(\n                    \"MATCH (p:Player {player_name: $name}) OPTIONAL MATCH (p)-[:PLAYS_FOR]->(t:Team) RETURN p.embedding IS NOT NULL AS has_emb, t.name AS team\",\n                    {'name': name}\n                ).single()\n                if not rec:\n                    emb_missing += 1\n                    plays_for_missing += 1\n                else:\n                    if not rec['has_emb']:\n                        emb_missing += 1\n                    if not rec['team']:\n                        plays_for_missing += 1\n\n        print(f\"\\n   Post-check: of top {len(players_checked)} baseline players -> missing embeddings: {emb_missing}, missing PLAYS_FOR: {plays_for_missing}\")\n        \n        # ─────────────────────────────────────────────────────────────\n        # EXPERIMENT 2: BASELINE + EMBEDDINGS (Combined)\n        # ─────────────────────────────────────────────────────────────\n        print(\"\\n📊 EXPERIMENT 2: BASELINE + NUMERIC EMBEDDINGS\")\n        print(\"─\" * 60)\n        \n        # Get embedding results\n        preprocessed = retriever.preprocessor.preprocess(query, include_embedding=False)\n        entities = preprocessed['entities']\n        \n        # Get position filter if available\n        position = entities.get('positions', [None])[0]\n        metrics = entities.get('metrics', [])\n        \n        # Create embedding criteria based on position and query\n        # Use per-game average features (goals_per_game, etc.)\n        criteria = {'avg_points_per_game': 'high'}\n        \n        # Position-specific criteria\n        if position == 'FWD':\n            # Forwards: prioritize goals\n            criteria['goals_per_game'] = 'high'\n            criteria['threat'] = 'high'\n        elif position == 'MID':\n            # Midfielders: balance goals and assists\n            criteria['goals_per_game'] = 'high'\n            criteria['assists_per_game'] = 'high'\n            criteria['creativity'] = 'high'\n        elif position == 'DEF':\n            # Defenders: clean sheets and bonus\n            criteria['clean_sheets_per_game'] = 'high'\n            criteria['avg_bonus'] = 'high'\n        elif position == 'GK':\n            # Goalkeepers: clean sheets\n            criteria['clean_sheets_per_game'] = 'high'\n            criteria['avg_bonus'] = 'high'\n        \n        # Add explicit metric criteria from query\n        if 'goals_scored' in metrics or 'goals' in str(query).lower():\n            criteria['goals_per_game'] = 'high'\n        if 'assists' in metrics or 'assists' in str(query).lower():\n            criteria['assists_per_game'] = 'high'\n        if 'clean_sheets' in metrics or 'clean' in str(query).lower():\n            criteria['clean_sheets_per_game'] = 'high'\n        \n        embedding_result = retriever.embedding_retrieve(criteria=criteria, position=position, top_k=5)\n        \n        print(f\"   Criteria: {embedding_result.get('criteria')}\")\n        print(f\"   Position Filter: {embedding_result.get('position_filter')}\")\n        print(f\"   Embedding Dims: {embedding_result.get('embedding_dimensions')}\")\n        print(f\"   Results ({len(embedding_result.get('results', []))} found):\")\n        for i, r in enumerate(embedding_result.get('results', [])[:5], 1):\n            print(f\"      {i}. {r.get('player')} ({r.get('position')}) - Sim: {r.get('similarity_score', 0):.3f}\")\n        \n        # ─────────────────────────────────────────────────────────────\n        # COMBINED RESULTS\n        # ─────────────────────────────────────────────────────────────\n        print(\"\\n📊 COMBINED RESULTS (Baseline + Embeddings merged)\")\n        print(\"─\" * 60)\n        \n        baseline_players = baseline_result.get('results', [])\n        embedding_players = embedding_result.get('results', [])\n        \n        combined = retriever._merge_results(baseline_players, embedding_players)\n        \n        print(f\"   Baseline found: {len(baseline_players)} players\")\n        print(f\"   Embeddings found: {len(embedding_players)} players\")\n        print(f\"   Combined (deduplicated): {len(combined)} players\")\n        \n        # Find overlap\n        baseline_names = set(r.get('player', r.get('player_name', '')) for r in baseline_players[:10])\n        embedding_names = set(r.get('player', '') for r in embedding_players[:10])\n        overlap = baseline_names & embedding_names\n        \n        print(f\"   Overlap (in top 10): {len(overlap)} players\")\n        if overlap:\n            print(f\"   Common players: {list(overlap)[:3]}\")\n    \n    retriever.close()\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"✅ TWO EXPERIMENTS COMPLETE\")\n    print(\"=\" * 80)\n    print(\"\"\"\n    EXPERIMENT SUMMARY:\n    \n    ┌──────────────────────────────────────────────────────────────────────────┐\n    │ EXPERIMENT 1: BASELINE ONLY                                              │\n    │ • Uses Cypher queries based on intent classification                     │\n    │ • Exact match filtering (position, season, etc.)                         │\n    │ • Returns structured data from Knowledge Graph                           │\n    ├──────────────────────────────────────────────────────────────────────────┤\n    │ EXPERIMENT 2: BASELINE + EMBEDDINGS                                      │\n    │ • Combines Cypher results with embedding similarity search               │\n    │ • Numeric embeddings find statistically similar players                  │\n    │ • Results are merged and deduplicated                                    │\n    │ • Provides both exact matches AND similar alternatives                   │\n    └──────────────────────────────────────────────────────────────────────────┘\n    \"\"\")\n\n\ndef test_full_embedding_comparison():\n    \"\"\"\n    Full test that stores ALL embedding types and compares them.\n    This satisfies the project requirement for TWO embedding models.\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"FULL EMBEDDING MODEL COMPARISON TEST\")\n    print(\"Requirement: Experiment with at least TWO different embedding models\")\n    print(\"=\" * 80)\n    \n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n    \n    # Step 1: Store ALL embedding types\n    print(\"\\n\" + \"─\" * 80)\n    print(\"STEP 1: STORE ALL EMBEDDING TYPES IN NEO4J\")\n    print(\"─\" * 80)\n    \n    storage_results = retriever.store_all_embeddings()\n    \n    # Step 2: Compare search results across all models\n    print(\"\\n\" + \"─\" * 80)\n    print(\"STEP 2: COMPARE SEARCH RESULTS\")\n    print(\"─\" * 80)\n    \n    for position in ['FWD', 'MID', 'DEF', 'GK']:\n        print(f\"\\n{'='*80}\")\n        print(f\"TESTING POSITION: {position}\")\n        print('='*80)\n        retriever.compare_all_embedding_models(position=position, top_k=5)\n    \n    # Step 3: Find similar players comparison\n    print(\"\\n\" + \"─\" * 80)\n    print(\"STEP 3: FIND SIMILAR PLAYERS (ALL MODELS)\")\n    print(\"─\" * 80)\n    \n    test_players = [\"Erling Haaland\", \"Kevin De Bruyne\", \"Virgil van Dijk\"]\n    \n    for player in test_players:\n        print(f\"\\n🔍 Players similar to {player}:\")\n        \n        # Numeric\n        similar = retriever.find_similar_players(player, top_k=3, same_position=True)\n        print(f\"   Numeric: {[r['player'] for r in similar if 'error' not in r][:3]}\")\n    \n    retriever.close()\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"✅ FULL COMPARISON TEST COMPLETE\")\n    print(\"=\" * 80)\n    print(\"\"\"\n    SUMMARY OF EMBEDDING MODELS COMPARED:\n    \n    ┌────────────────┬──────────┬─────────────────────────────────────────┐\n    │ Model          │ Dims     │ Description                             │\n    ├────────────────┼──────────┼─────────────────────────────────────────┤\n    │ Numeric        │ 12       │ Direct normalized stats                 │\n    │ MiniLM         │ 384      │ Text → sentence-transformers/all-MiniLM │\n    │ MPNet          │ 768      │ Text → sentence-transformers/all-mpnet  │\n    └────────────────┴──────────┴─────────────────────────────────────────┘\n    \n    This satisfies the requirement: \"experiment with at least TWO different \n    embedding models for comparison\" by comparing:\n    1. MiniLM (all-MiniLM-L6-v2) - 384 dimensions\n    2. MPNet (all-mpnet-base-v2) - 768 dimensions\n    \n    Additionally, we compare these TEXT embeddings with NUMERIC embeddings\n    to show the tradeoffs between direct features vs semantic understanding.\n    \"\"\")\n\n\ndef test_baseline_queries_run():\n    \"\"\"Run all 12 baseline Cypher queries with example parameters, verify\n    they return results, and check that returned players have embeddings\n    and a `PLAYS_FOR` relationship. Prints a concise report.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RUNNING BASELINE QUERY TESTS (with embedding & PLAYS_FOR checks)\")\n    print(\"=\" * 80)\n\n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n\n    # Ensure numeric embeddings exist for semantic checks\n    print(\"\\n🔁 Ensuring numeric embeddings are stored (required for embedding checks)...\")\n    try:\n        store_info = retriever.store_embeddings_in_neo4j()\n        print(f\"   Embedding storage: {store_info}\")\n    except Exception as e:\n        print(f\"   ⚠️ Failed to store embeddings: {e}\")\n\n    # Define example parameters for each query template\n    tests = [\n        ('top_players_by_position', {'position': 'FWD', 'season': '2022-23', 'limit': 10}),\n        ('player_gameweek_performance', {'player_name': 'Erling Haaland', 'gameweek': 20, 'season': '2022-23'}),\n        ('compare_players', {'player_name': 'Erling Haaland', 'player_name2': 'Harry Kane', 'season': '2022-23'}),\n        ('team_fixtures', {'team': 'Manchester City', 'season': '2022-23'}),\n        ('players_by_team', {'team': 'Manchester City'}),\n        ('top_scorers', {'season': '2022-23', 'limit': 10}),\n        ('top_assisters', {'season': '2022-23', 'limit': 10}),\n        ('clean_sheet_leaders', {'season': '2022-23', 'limit': 10}),\n        ('players_by_form', {'season': '2022-23', 'gameweek': 20, 'limit': 10}),\n        ('player_season_summary', {'player_name': 'Erling Haaland', 'season': '2022-23'}),\n        ('bonus_leaders', {'season': '2022-23', 'limit': 10}),\n        ('most_cards', {'season': '2022-23', 'limit': 10})\n    ]\n\n    summary = {}\n\n    with retriever.driver.session() as session:\n        for name, params in tests:\n            print(\"\\n\" + \"-\" * 70)\n            print(f\"Running template: {name}  with params: {params}\")\n            try:\n                results = retriever.execute_cypher(name, params)\n            except Exception as e:\n                print(f\"   ❌ Query execution failed: {e}\")\n                summary[name] = {'status': 'error', 'error': str(e)}\n                continue\n\n            ok = bool(results)\n            print(f\"   Results returned: {len(results)}\")\n\n            # Extract up to 5 player names from results for per-player checks\n            sample_players = []\n            for r in results[:5]:\n                # Common keys to look for\n                if isinstance(r, dict):\n                    if 'player' in r and r['player']:\n                        sample_players.append(r['player'])\n                    elif 'player_name' in r and r['player_name']:\n                        sample_players.append(r['player_name'])\n                    elif 'player1' in r and r['player1']:\n                        sample_players.append(r['player1'])\n                    elif 'player2' in r and r['player2']:\n                        sample_players.append(r['player2'])\n                    else:\n                        # try any string-looking value\n                        for v in r.values():\n                            if isinstance(v, str) and len(v) > 1:\n                                sample_players.append(v)\n                                break\n\n            sample_players = list(dict.fromkeys(sample_players))  # unique preserve order\n\n            emb_missing = 0\n            plays_for_missing = 0\n            for pname in sample_players:\n                rec = session.run(\n                    \"MATCH (p:Player {player_name: $name}) OPTIONAL MATCH (p)-[:PLAYS_FOR]->(t:Team) RETURN p.embedding IS NOT NULL AS has_emb, t.name AS team\",\n                    {'name': pname}\n                ).single()\n                if not rec:\n                    emb_missing += 1\n                    plays_for_missing += 1\n                else:\n                    if not rec['has_emb']:\n                        emb_missing += 1\n                    if not rec['team']:\n                        plays_for_missing += 1\n\n            summary[name] = {\n                'results_count': len(results),\n                'sample_players_checked': sample_players,\n                'emb_missing_in_sample': emb_missing,\n                'plays_for_missing_in_sample': plays_for_missing,\n                'status': 'ok' if ok and emb_missing == 0 and plays_for_missing == 0 else 'warning' if ok else 'fail'\n            }\n\n            # Print quick per-query findings\n            print(f\"   Sample players checked: {sample_players}\")\n            print(f\"   Embeddings missing in sample: {emb_missing}\")\n            print(f\"   PLAYS_FOR missing in sample: {plays_for_missing}\")\n\n    retriever.close()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"BASELINE QUERIES TEST SUMMARY\")\n    print(\"=\" * 80)\n    for k, v in summary.items():\n        print(f\"- {k}: status={v['status']}, results={v.get('results_count', 0)}, emb_missing={v.get('emb_missing_in_sample', 0)}, plays_for_missing={v.get('plays_for_missing_in_sample', 0)}\")\n\n    print(\"\\nTests complete. Review warnings and errors above.\")\n\n\ndef test_each_template_with_example_query():\n    \"\"\"Test each of the 12 baseline query templates with one hardcoded example query.\n    \n    For each template, this function:\n    - Uses one realistic example query that will trigger that template\n    - Calls retrieve(user_input, method='both')\n    - Validates baseline/embedding/combined results exist and make sense\n    - Checks up to 3 sample baseline players for p.embedding and PLAYS_FOR\n    - Prints detailed per-template findings\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TESTING EACH TEMPLATE WITH HARDCODED EXAMPLE QUERIES\")\n    print(\"=\" * 80)\n\n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n\n    # Ensure embeddings are stored before running tests\n    print(\"\\n🔁 Ensuring numeric embeddings are stored...\")\n    try:\n        store_res = retriever.store_embeddings_in_neo4j()\n        print(f\"   Stored {store_res['embeddings_stored']} embeddings\")\n    except Exception as e:\n        print(f\"   ⚠️ Embedding storage failed: {e}\")\n\n    # Define one example query per template\n    template_queries = {\n        'top_players_by_position': \"Who are the top forwards in 2022?\",\n        'player_gameweek_performance': \"How did Erling Haaland perform in gameweek 20 of 2022-23?\",\n        'compare_players': \"Compare Erling Haaland vs Harry Kane in 2022-23\",\n        'team_fixtures': \"Show me Manchester City's fixtures in season 2022-23\",\n        'players_by_team': \"Which players play for Manchester City?\",\n        'top_scorers': \"Who are the top scorers in the 2022-23 season?\",\n        'top_assisters': \"Who has the most assists in 2022-23?\",\n        'clean_sheet_leaders': \"Which goalkeepers have the most clean sheets in 2022-23?\",\n        'players_by_form': \"Who are the players with the best form around gameweek 20 in 2022-23?\",\n        'player_season_summary': \"What is Erling Haaland's full season summary for 2022-23?\",\n        'bonus_leaders': \"Who earned the most bonus points in 2022-23?\",\n        'most_cards': \"Which players received the most cards (yellow/red) in 2022-23?\"\n    }\n\n    results_summary = {}\n\n    for template_name, example_query in template_queries.items():\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"TEMPLATE: {template_name}\")\n        print(f\"QUERY: \\\"{example_query}\\\"\")\n        print(\"=\" * 80)\n\n        try:\n            # Call retrieve with method='both' (baseline + embeddings)\n            output = retriever.retrieve(example_query, method='both')\n        except Exception as e:\n            print(f\"❌ ERROR: retrieve() raised exception: {e}\")\n            results_summary[template_name] = {\n                'status': 'error',\n                'error': str(e),\n                'baseline_count': 0,\n                'embedding_count': 0,\n                'combined_count': 0,\n                'notes': [f\"exception: {str(e)}\"]\n            }\n            continue\n\n        baseline = output.get('baseline', {})\n        embedding = output.get('embedding', {})\n        combined = output.get('combined', [])\n\n        # Extract counts\n        baseline_results = baseline.get('results', []) if isinstance(baseline, dict) else []\n        embedding_results = embedding.get('results', []) if isinstance(embedding, dict) else []\n\n        baseline_count = len(baseline_results)\n        embedding_count = len(embedding_results)\n        combined_count = len(combined)\n\n        print(f\"\\n📊 BASELINE RETRIEVAL:\")\n        print(f\"   Intent: {baseline.get('intent', 'unknown')}\")\n        print(f\"   Query Used: {baseline.get('query_used', 'unknown')}\")\n        print(f\"   Results Count: {baseline_count}\")\n        if baseline_results:\n            for i, r in enumerate(baseline_results[:3], 1):\n                name = r.get('player') or r.get('player_name', 'Unknown')\n                points = r.get('total_points') or r.get('points', '-')\n                print(f\"      {i}. {name} - Points: {points}\")\n\n        print(f\"\\n📊 EMBEDDING RETRIEVAL:\")\n        print(f\"   Criteria: {embedding.get('criteria', {})}\")\n        print(f\"   Position Filter: {embedding.get('position_filter')}\")\n        print(f\"   Results Count: {embedding_count}\")\n        if embedding_results:\n            for i, r in enumerate(embedding_results[:3], 1):\n                name = r.get('player', 'Unknown')\n                sim = r.get('similarity_score', 0)\n                print(f\"      {i}. {name} - Similarity: {sim:.3f}\")\n\n        print(f\"\\n📊 COMBINED RESULTS:\")\n        print(f\"   Total (deduplicated): {combined_count}\")\n\n        # Check sample baseline players for embeddings and PLAYS_FOR\n        sample_players = []\n        for r in baseline_results[:3]:\n            if isinstance(r, dict):\n                name = r.get('player') or r.get('player_name') or r.get('player1') or r.get('player2')\n                if name:\n                    sample_players.append(name)\n\n        emb_missing = 0\n        plays_for_missing = 0\n        player_status = []\n\n        with retriever.driver.session() as session:\n            for name in sample_players:\n                rec = session.run(\n                    \"MATCH (p:Player {player_name: $name}) OPTIONAL MATCH (p)-[:PLAYS_FOR]->(t:Team) RETURN p.embedding IS NOT NULL AS has_emb, t.name AS team\",\n                    {'name': name}\n                ).single()\n                if not rec:\n                    emb_missing += 1\n                    plays_for_missing += 1\n                    player_status.append({'player': name, 'has_embedding': False, 'has_plays_for': False})\n                else:\n                    has_emb = bool(rec['has_emb'])\n                    team = rec['team']\n                    if not has_emb:\n                        emb_missing += 1\n                    if not team:\n                        plays_for_missing += 1\n                    player_status.append({'player': name, 'has_embedding': has_emb, 'has_plays_for': bool(team)})\n\n        print(f\"\\n✓ PLAYER VALIDATION (sample of {len(sample_players)}):\")\n        for ps in player_status:\n            emb_icon = \"✅\" if ps['has_embedding'] else \"❌\"\n            pf_icon = \"✅\" if ps['has_plays_for'] else \"❌\"\n            print(f\"   {ps['player']}: embedding {emb_icon}, PLAYS_FOR {pf_icon}\")\n\n        status = 'ok'\n        notes = []\n\n        if baseline_count == 0:\n            notes.append('no baseline results')\n            status = 'warning'\n        if embedding_count == 0:\n            notes.append('no embedding results')\n            if status == 'ok':\n                status = 'warning'\n        if emb_missing > 0:\n            notes.append(f'embedding missing: {emb_missing}')\n        if plays_for_missing > 0:\n            notes.append(f'PLAYS_FOR missing: {plays_for_missing}')\n\n        results_summary[template_name] = {\n            'status': status,\n            'baseline_count': baseline_count,\n            'embedding_count': embedding_count,\n            'combined_count': combined_count,\n            'sample_players_checked': len(sample_players),\n            'embedding_missing': emb_missing,\n            'plays_for_missing': plays_for_missing,\n            'notes': notes\n        }\n\n        print(f\"\\n   Status: {status}, Notes: {notes}\")\n\n    retriever.close()\n\n    # Print final summary\n    print(\"\\n\" + \"=\" * 80)\n    print(\"FINAL SUMMARY - ALL 12 TEMPLATES\")\n    print(\"=\" * 80)\n    print(f\"\\n{'Template':<35} {'Status':<10} {'Baseline':<10} {'Embedding':<10} {'Combined':<10}\")\n    print(\"-\" * 95)\n    for template, res in results_summary.items():\n        status = res['status']\n        baseline = res['baseline_count']\n        embedding = res['embedding_count']\n        combined = res['combined_count']\n        print(f\"{template:<35} {status:<10} {baseline:<10} {embedding:<10} {combined:<10}\")\n\n    print(f\"\\n{'Notes:'}\")\n    for template, res in results_summary.items():\n        if res['notes']:\n            print(f\"  {template}: {', '.join(res['notes'])}\")\n\n    print(\"\\n✅ TEMPLATE TEST COMPLETE\")\n\n\nif __name__ == \"__main__\":\n    NEO4J_URI = \"neo4j+s://1da86c19.databases.neo4j.io\"\n    NEO4J_USER = \"neo4j\"\n    NEO4J_PASSWORD = \"HA4iunTOGen7RYpeISs3ZRhcWjpcokqam9przCqCuQ8\"\n    \n    # Initialize retriever\n    retriever = FPLGraphRetrieval(\n        neo4j_uri=NEO4J_URI,\n        neo4j_user=NEO4J_USER,\n        neo4j_password=NEO4J_PASSWORD\n    )\n    \n    print(\"=\" * 80)\n    print(\"STORING ALL EMBEDDINGS IN NEO4J\")\n    print(\"=\" * 80)\n    \n    # Store ALL embedding types\n    results = retriever.store_all_embeddings()\n    \n    print(\"\\n✅ ALL EMBEDDINGS STORED\")\n    print(f\"Numeric: {results['numeric'].get('embeddings_stored')} players\")\n    print(f\"MiniLM: {results['minilm'].get('embeddings_stored')} players\")\n    print(f\"MPNet: {results['mpnet'].get('embeddings_stored')} players\")\n    \n    # Optional: Run the template tests after\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RUNNING TEMPLATE TESTS\")\n    print(\"=\" * 80)\n    test_each_template_with_example_query()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:02:51.064818Z","iopub.execute_input":"2025-12-16T07:02:51.065168Z","iopub.status.idle":"2025-12-16T07:02:51.142315Z","shell.execute_reply.started":"2025-12-16T07:02:51.065119Z","shell.execute_reply":"2025-12-16T07:02:51.141255Z"}},"outputs":[{"name":"stdout","text":"Overwriting fpl_Task2.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"preprocesskey\")\nsecret_value_1 = user_secrets.get_secret(\"preprocesskey\")\n# secret_value_2 = user_secrets.get_secret(\"j\")\n# secret_value_3 = user_secrets.get_secret(\"LLM Token\")\nsecret_value_4 = user_secrets.get_secret(\"openrouterkey\")\nsecret_value_5 = user_secrets.get_secret(\"openrouterkey\")\nsecret_value_6 = user_secrets.get_secret(\"preprocesskey\")\n\n\n\n\n# ─────────────────────────────────────────────────────────────\n# REQUIRED PACKAGES (install if needed)\n# ─────────────────────────────────────────────────────────────\n# !pip install neo4j sentence-transformers\n\n# ─────────────────────────────────────────────────────────────\n# IMPORTS\n# ─────────────────────────────────────────────────────────────\nfrom neo4j import GraphDatabase\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import Dict, List, Any\n\n# Import the main class\nfrom fpl_Task2 import FPLGraphRetrieval\n\n# ─────────────────────────────────────────────────────────────\n# NEO4J CONFIGURATION\n# ─────────────────────────────────────────────────────────────\nNEO4J_URI = \"neo4j+s://1da86c19.databases.neo4j.io\"\nNEO4J_USER = \"neo4j\"\nNEO4J_PASSWORD = \"HA4iunTOGen7RYpeISs3ZRhcWjpcokqam9przCqCuQ8\"\n\n# ─────────────────────────────────────────────────────────────\n# USAGE\n# ─────────────────────────────────────────────────────────────\n# Initialize\nretriever = FPLGraphRetrieval(\n    neo4j_uri=NEO4J_URI,\n    neo4j_user=NEO4J_USER,\n    neo4j_password=NEO4J_PASSWORD,\n    hf_token=secret_value_1,  # Optional, for LLM mode\n    use_llm=True  # Set True to use LLM for entity extraction\n)\n\n\n\n\nfrom openai import OpenAI\nfrom kaggle_secrets import UserSecretsClient\ndef call_llm(user_question: str, model: str, embedding_type: str = 'numeric') -> str:\n    \"\"\"\n    Takes user question, handles retrieval with specified embedding type.\n    \"\"\"\n\n    # 1️⃣ Get API key\n    user_secrets = UserSecretsClient()\n    openrouter_key = user_secrets.get_secret(\"openrouterkey\")\n\n    # 2️⃣ Initialize OpenRouter client\n    client = OpenAI(\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key=openrouter_key\n    )\n\n    \n    result = retriever.retrieve(user_question, method=\"both\", embedding_type=embedding_type)\n\n    # print(\"=\"*50)\n    # print(\"RETRIEVAL DEBUG:\")\n    # print(f\"Intent detected: {result.get('baseline', {}).get('intent')}\")\n    # print(f\"Query used: {result.get('baseline', {}).get('query_used')}\")\n    # print(f\"Parameters: {result.get('baseline', {}).get('parameters')}\")\n    # print(f\"Results: {result.get('baseline', {}).get('results', [])[:2]}\")  # First 2 results\n    # print(\"=\"*50)\n    print(result)\n    \n\n    context_text = result\n\n    # 5️⃣ Define persona\n    persona_text = (\n    \"You are an FPL (Fantasy Premier League) expert. \"\n    \"You can answer any questions related to FPL, including player stats, \"\n    \"team performance, transfers, and gameweek strategies. \"\n    \"You are also knowledgeable about the Premier League in general.\"\n    )\n\n    task_text = (\n    \"IMPORTANT: You MUST always provide a response. Never return empty text.\\n\\n\"\n    \n    \"=== QUERY TYPE INSTRUCTIONS ===\\n\\n\"\n    \n    \"1. COMPARISON QUERIES (e.g., 'compare X and Y'):\\n\"\n    \"   • First check if BOTH players are mentioned in the CONTEXT\\n\"\n    \"   • Extract ALL their stats from the CONTEXT\\n\"\n    \"   • Create a detailed comparison table or bullet points\\n\"\n    \"   • Include: goals, points, assists, clean sheets, bonus points\\n\"\n    \"   • Provide a clear conclusion about who performed better\\n\"\n    \"   EXAMPLE:\\n\"\n    \"   Comparison: Player A vs Player B\\n\"\n    \"   • Player A: X goals, Y points, Z assists, W bonus\\n\"\n    \"   • Player B: A goals, B points, C assists, D bonus\\n\"\n    \"   Conclusion: [Who was better and why]\\n\\n\"\n    \n    \"2. TOP/RANKING QUERIES (e.g., 'top 10 scorers', 'best defenders'):\\n\"\n    \"   • Extract the exact ranking from the CONTEXT\\n\"\n    \"   • List players in numbered order (1, 2, 3...)\\n\"\n    \"   • Include key stats: goals/points/assists next to each name\\n\"\n    \"   • Format: '1. Player Name - X goals, Y points'\\n\"\n    \"   • If asking for top N, provide EXACTLY N players\\n\\n\"\n    \n    \"3. PLAYER SUMMARY QUERIES (e.g., 'how did X perform', 'X's stats'):\\n\"\n    \"   • Extract ALL available stats for that player from CONTEXT\\n\"\n    \"   • Include: total points, goals, assists, minutes, clean sheets, bonus, cards\\n\"\n    \"   • Present in bullet format or table\\n\"\n    \"   • Add brief analysis if stats are exceptional\\n\\n\"\n    \n    \"4. TEAM/FIXTURE QUERIES (e.g., 'team fixtures', 'upcoming matches'):\\n\"\n    \"   • List fixtures chronologically by gameweek\\n\"\n    \"   • Format: 'GW X: Home Team vs Away Team (Date)'\\n\"\n    \"   • Include scores if available in CONTEXT\\n\"\n    \"   • Group by team if querying specific team\\n\\n\"\n    \n    \"5. GAMEWEEK PERFORMANCE QUERIES (e.g., 'GW 20 performance'):\\n\"\n    \"   • Focus on specific gameweek data from CONTEXT\\n\"\n    \"   • Include: points scored, goals, assists, minutes played\\n\"\n    \"   • Mention opponent if available\\n\"\n    \"   • Note if player didn't play (0 minutes)\\n\\n\"\n    \n    \"6. LIST QUERIES (e.g., 'players on Manchester City'):\\n\"\n    \"   • Provide complete list from CONTEXT\\n\"\n    \"   • Group by position if relevant (GK, DEF, MID, FWD)\\n\"\n    \"   • Use bullet points or numbered list\\n\"\n    \"   • Include position labels\\n\\n\"\n    \n    \"7. STATISTICAL QUERIES (e.g., 'most cards', 'clean sheet leaders'):\\n\"\n    \"   • Focus on the specific statistic asked\\n\"\n    \"   • Rank players by that stat\\n\"\n    \"   • Show the actual numbers clearly\\n\"\n    \"   • Format: 'Player Name: X [stat]'\\n\\n\"\n    \n    \"=== GENERAL RULES ===\\n\"\n    \"• ALWAYS extract data from CONTEXT - never make up stats\\n\"\n    \"• If data is not in CONTEXT, say 'Information not available in context'\\n\"\n    \"• Use clear formatting: bullet points, numbering, or tables\\n\"\n    \"• Include units (points, goals, assists) with numbers\\n\"\n    \"• Be concise but complete - include all relevant data\\n\"\n    \"• For numerical data, preserve exact values from CONTEXT\\n\"\n    \"• If multiple entries exist (e.g., player played multiple GWs), show all\\n\\n\"\n\n    \n    )\n\n    # 7️⃣ Build the prompt\n    prompt = f\"\"\"\n    CONTEXT:\n    {context_text}\n\n    PERSONA:\n    {persona_text}\n\n    TASK:\n    {task_text}\n    \"\"\"\n\n    # 8️⃣ Call the LLM\n    completion = client.chat.completions.create(\n        model= model,\n        messages=[\n            {\"role\": \"system\", \"content\": persona_text},\n            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nQuestion: {user_question}\"}\n        ],\n        temperature=0.2,\n        max_tokens=500\n    )\n\n    # 9️⃣ Return the LLM answer\n\n    answer = completion.choices[0].message.content\n\n    # ✅ Token usage (access as attributes, not dict)\n    prompt_tokens = completion.usage.prompt_tokens\n    completion_tokens = completion.usage.completion_tokens\n    total_tokens = completion.usage.total_tokens\n\n\n\n\n\n    return completion.choices[0].message.content, total_tokens\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef compare_models_human_evaluation():\n    \"\"\"\n    Run 3 models on test queries and perform human evaluation with comparison.\n    Self-contained function - no inputs required. Models and queries defined internally.\n    \n    Returns:\n        Dictionary with evaluations and comparison table\n    \"\"\"\n    import json\n    import os\n    import time\n    \n    # =========================================================================\n    # CONFIGURATION - Edit these to customize your evaluation\n    # =========================================================================\n    \n    models_dict = {\n        \"1\": \"mistralai/devstral-2512:free\",\n        \"2\":  \"meta-llama/llama-3.3-70b-instruct:free\",\n        \"3\": \"nvidia/nemotron-3-nano-30b-a3b:free\",\n    }\n    \n    test_queries = [\n        {\n            \"question\": \"Who are the top 10 forwards in the 2022-23 season?\",\n            \"ground_truth\": \"\"\"\nErling Haaland,272\nHarry Kane,263\nIvan Toney,182\nOllie Watkins,175\nCallum Wilson,157\nBryan Mbeumo,150\nDominic Solanke,130\nGabriel Fernando de Jesus,125\nBrennan Johnson,122\nAleksandar Mitrović,107\"\"\"\n        },\n        {\n            \"question\": \"How did Erling Haaland perform in gameweek 20 of the 2022-23 season?\",\n            \"ground_truth\": \"\"\"\nErling Haaland,points 2,goals 0,assists 0,minutes 90\nErling Haaland,points 6,goals 1, assists 0,minutes 89\"\"\"\n        },\n        {\n            \"question\": \"Compare Erling Haaland and Harry Kane's performance in the 2022-23 season\",\n            \"ground_truth\": \"\"\"player,total_points,goals,assists\nErling Haaland,total_points 272,goals 36,assists 9\nHarry Kane,total_points 263,goals 30,assists 9\"\"\"\n        },\n        {\n            \"question\": \"What were Manchester City's fixtures in the 2022-23 season?\",\n            \"ground_truth\": \"\"\"\n1,West Ham,Man City,2022-08-07 15:30:00+00:00\n3,Newcastle,Man City,2022-08-21 15:30:00+00:00\n6,Aston Villa,Man City,2022-09-03 16:30:00+00:00\n8,Wolves,Man City,2022-09-17 11:30:00+00:00\n11,Liverpool,Man City,2022-10-16 15:30:00+00:00\n14,Leicester,Man City,2022-10-29 11:30:00+00:00\n17,Leeds,Man City,2022-12-28 20:00:00+00:00\n19,Chelsea,Man City,2023-01-05 20:00:00+00:00\n20,Man Utd,Man City,2023-01-14 12:30:00+00:00\n22,Spurs,Man City,2023-02-05 16:30:00+00:00\n23,Arsenal,Man City,2023-02-15 19:30:00+00:00\n24,Nott'm Forest,Man City,2023-02-18 15:00:00+00:00\n25,Bournemouth,Man City,2023-02-25 17:30:00+00:00\n27,Crystal Palace,Man City,2023-03-11 17:30:00+00:00\n30,Southampton,Man City,2023-04-08 16:30:00+00:00\n34,Fulham,Man City,2023-04-30 13:00:00+00:00\n36,Everton,Man City,2023-05-14 13:00:00+00:00\n37,Brighton,Man City,2023-05-24 19:00:00+00:00\n38,Brentford,Man City,2023-05-28 15:30:00+00:00\"\"\"\n        },\n        {\n            \"question\": \"Which players are on Manchester City?\",\n            \"ground_truth\": \"\"\"\nplayer,position\nAlex Robertson,MID\nAymeric Laporte,DEF\nAymeric Laporte,DEF\nBen Knight,MID\nBenjamin Mendy,DEF\nBernardo Mota Veiga de Carvalho e Silva,MID\nBernardo Veiga de Carvalho e Silva,MID\nCieran Slicker,GK\nClaudio Gomes,MID\nCole Palmer,MID\nCole Palmer,MID\nConrad Egan-Riley,DEF\nEderson Santana de Moraes,GK\nEderson Santana de Moraes,GK\nErling Haaland,FWD\nFernando Luiz Rosa,MID\nFerran Torres,MID\nGabriel Fernando de Jesus,FWD\nIlkay Gündogan,MID\nIlkay Gündogan,MID\nJack Grealish,MID\nJack Grealish,MID\nJames McAtee,MID\nJames McAtee,MID\nJohn Stones,DEF\nJohn Stones,FWD\nJohn Stones,DEF\nJosh Wilson-Esbrand,DEF\nJoshua Wilson-Esbrand,DEF\nJoão Cancelo,DEF\nJoão Pedro Cavaco Cancelo,DEF\nJulián Álvarez,FWD\nKalvin Phillips,MID\nKayky da Silva Chagas,FWD\nKevin De Bruyne,MID\nKevin De Bruyne,MID\nKyle Walker,DEF\nKyle Walker,DEF\nLiam Delap,FWD\nLiam Delap,FWD\nLuke Mbete,DEF\nLuke Mbete-Tabu,DEF\nManuel Akanji,DEF\nMáximo Perrone,MID\nNathan Aké,DEF\nNathan Aké,DEF\nNico O'Reilly,MID\nOleksandr Zinchenko,DEF\nPhil Foden,MID\nPhil Foden,MID\nRaheem Sterling,MID\nRico Lewis,DEF\nRiyad Mahrez,MID\nRiyad Mahrez,MID\nRodrigo Hernandez,MID\nRodrigo Hernandez,MID\nRomeo Lavia,MID\nRúben Gato Alves Dias,DEF\nRúben Santos Gato Alves Dias,DEF\nSamuel Edozie,MID\nScott Carson,GK\nScott Carson,GK\nSergio Gómez,DEF\nShea Charles,MID\nStefan Ortega Moreno,GK\nTommy Doyle,MID\nZack Steffen,GK\nZack Steffen,GK\"\"\"\n        },\n        {\n            \"question\": \"Who were the top 10 goal scorers in the 2022-23 season?\",\n            \"ground_truth\": \"\"\"\nErling Haaland,36\nHarry Kane,30\nIvan Toney,20\nMohamed Salah,19\nCallum Wilson,18\nMarcus Rashford,17\nOllie Watkins,15\nGabriel Martinelli Silva,15\nMartin Ødegaard,15\nAleksandar Mitrović,14\"\"\"\n        },\n        {\n            \"question\": \"Who provided the most assists in the 2022-23 season? Show me the top 10?\",\n            \"ground_truth\": \"\"\"\nKevin De Bruyne,18\nMohamed Salah,13\nMorgan Gibbs-White,12\nRiyad Mahrez,12\nBukayo Saka,12\nMichael Olise,11\nTrent Alexander-Arnold,11\nJack Grealish,10\nAndreas Hoelgebaum Pereira,10\nSolly March,10\"\"\"\n        },\n        {\n            \"question\": \"Which players had the most clean sheets in the 2022-23 season?\",\n            \"ground_truth\": \"\"\"\nBruno Borges Fernandes,18\nDavid De Gea Quintana,17\nKieran Trippier,16\nFabian Schär,15\nMiguel Almirón Rejala,15\nBenjamin White,15\nDan Burn,14\nAlisson Ramses Becker,14\nGabriel Martinelli Silva,14\nAaron Ramsdale,14\"\"\"\n        },\n        {\n            \"question\": \"Who are the top 10 players in form as of gameweek 20 in the 2022-23 season?\",\n            \"ground_truth\": \"\"\"\nMartin Ødegaard,51\nSolly March,46\nKieran Trippier,44\nHarry Kane,43\nMarcus Rashford,41\nBruno Borges Fernandes,40\nRiyad Mahrez,39\nIvan Toney,39\nLuke Shaw,38\nChristian Eriksen,35\"\"\"\n        },\n       {\n            \"question\": \"Give me a complete summary of Erling Haaland's 2022-23 season?\",\n            \"ground_truth\": \"\"\"player,total_points,goals,assists,clean_sheets,minutes,bonus_points\nErling Haaland,272,36,9,13,2767,40\"\"\"\n        },\n       {\n            \"question\": \"Who earned the most bonus points in the 2022-23 season?\",\n            \"ground_truth\": \"\"\"player,total_bonus\nHarry Kane,48\nErling Haaland,40\nKieran Trippier,39\nIvan Toney,35\nMartin Ødegaard,30\nKevin De Bruyne,26\nOllie Watkins,25\nMohamed Salah,23\nBruno Borges Fernandes,23\nTrent Alexander-Arnold,21\"\"\"\n        },\n      \n        {\n            \"question\": \"Which players received the most yellow and red cards in the 2022-23 season?\",\n            \"ground_truth\": \"\"\"player,total_yellow,total_red,total_cards\nJoão Palhinha Gonçalves,total_yellow 14,total_red 0,total_cards 14\nRúben da Silva Neves,total_yellow 12,total_red 0,total_cards 12\nNélson Cabral Semedo,total_yellow 11,total_red 1,12\nJoelinton Cássio Apolinário de Lira,total_yellow 12,total_red 0,total_cards 12\nFabio Henrique Tavares,total_yellow 11,total_red 0,total_cards 11\nAdam Smith,total_yellow 11,total_red 0,total_cards 11\nJames Maddison,total_yellow 10,total_red 0,total_cards 10\nMoisés Caicedo Corozo,total_yellow 10,total_red 0,total_cards 10\nConor Gallagher,total_yellow 9,total_red 1,total_cards 10\nCristian Romero,total_yellow 9,total_red 1,total_cards 10\"\"\"\n        }\n    ]\n    \n    # =========================================================================\n    # MAIN EXECUTION\n    # =========================================================================\n    \n    models = list(models_dict.values())\n    model_labels = list(models_dict.keys())\n    \n    if len(models) != 3:\n        raise ValueError(\"Please provide exactly 3 models for comparison\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"          RUNNING MODELS ON TEST QUERIES\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Step 1: Run all models and collect results\n    all_model_results = {}\n    \n    for model_id, model_name in models_dict.items():\n        display_name = f\"Model {model_id} ({model_name.split('/')[1].split(':')[0]})\"\n        print(f\"🔄 Running {display_name}...\")\n        model_results = []\n        \n        for query in test_queries:\n            question = query[\"question\"]\n            ground_truth = query.get(\"ground_truth\")\n            \n            try:\n                answer, tokens_used = call_llm(question, model_name)\n                \n                model_results.append({\n                    \"question\": question,\n                    \"ground_truth\": ground_truth,\n                    \"answer\": answer,\n                    \"tokens_used\": tokens_used\n                })\n            except Exception as e:\n                model_results.append({\n                    \"question\": question,\n                    \"ground_truth\": ground_truth,\n                    \"answer\": f\"ERROR: {str(e)}\",\n                    \"tokens_used\": 0\n                })\n        \n        all_model_results[display_name] = model_results\n        print(f\"✓ {display_name} completed\\n\")\n    \n    model_display_names = list(all_model_results.keys())\n    \n    # Step 2: Human evaluation for each query across all models\n    print(\"\\n\" + \"=\"*80)\n    print(\"          HUMAN EVALUATION SESSION\")\n    print(\"=\"*80 + \"\\n\")\n    \n    evaluations = {model: [] for model in model_display_names}\n    total_queries = len(test_queries)\n    \n    for query_idx in range(total_queries):\n        os.system('clear' if os.name == 'posix' else 'cls')\n        \n        print(f\"\\n📊 Progress: Query {query_idx + 1}/{total_queries} ({(query_idx + 1)/total_queries*100:.0f}%)\")\n        print(\"=\"*80)\n        \n        # Display question and ground truth\n        query_data = test_queries[query_idx]\n        print(f\"\\n❓ Question: {query_data['question']}\")\n        if query_data.get('ground_truth'):\n            print(f\"✓ Ground Truth: {query_data['ground_truth']}\")\n        \n        print(\"\\n\" + \"-\"*80)\n        \n        # Evaluate each model's answer for this query\n        for model_name in model_display_names:\n            result = all_model_results[model_name][query_idx]\n            \n            print(f\"\\n🤖 {model_name}\")\n            print(f\"💬 Answer: {result['answer']}\")\n            print(f\"🔢 Tokens Used: {result['tokens_used']}\")\n            print(\"-\"*80)\n            \n            # Collect ratings\n            print(f\"\\nRate this answer (1-5):\")\n            ratings = {}\n            metrics = {\n                'relevance': '🎯 Relevance',\n                'correctness': '✅ Correctness',\n                'naturalness': '🗣️  Naturalness',\n                'completeness': '📝 Completeness',\n                'overall': '⭐ Overall Quality'\n            }\n            \n            for key, label in metrics.items():\n                while True:\n                    try:\n                        rating = int(input(f\"  {label} (1-5): \"))\n                        if 1 <= rating <= 5:\n                            ratings[key] = rating\n                            break\n                        else:\n                            print(\"    ⚠️  Please enter 1-5\")\n                    except ValueError:\n                        print(\"    ⚠️  Please enter a valid number\")\n            \n            comments = input(f\"\\n💭 Comments (optional): \")\n            \n            # Store evaluation\n            evaluations[model_name].append({\n                **result,\n                \"human_eval\": {\n                    **ratings,\n                    \"comments\": comments\n                }\n            })\n            \n            print()\n        \n        # Save progress\n        progress_data = {\n            \"models\": model_display_names,\n            \"evaluations\": evaluations,\n            \"queries_completed\": query_idx + 1\n        }\n        with open(\"model_comparison_progress.json\", \"w\") as f:\n            json.dump(progress_data, f, indent=2)\n        \n        print(f\"✓ Progress saved ({query_idx + 1}/{total_queries})\")\n        \n        if query_idx < total_queries - 1:\n            input(\"\\nPress Enter to continue to next query...\")\n    \n    # Step 3: Calculate statistics and create comparison table\n    os.system('clear' if os.name == 'posix' else 'cls')\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"          EVALUATION COMPLETE - COMPARISON RESULTS\")\n    print(\"=\"*80 + \"\\n\")\n    \n    metrics_list = ['relevance', 'correctness', 'naturalness', 'completeness', 'overall']\n    comparison_data = {}\n    \n    for model_name in model_display_names:\n        model_evals = evaluations[model_name]\n        \n        avg_metrics = {}\n        for metric in metrics_list:\n            avg = sum(e[\"human_eval\"][metric] for e in model_evals) / len(model_evals)\n            avg_metrics[metric] = round(avg, 2)\n        \n        avg_tokens = sum(e[\"tokens_used\"] for e in model_evals) / len(model_evals)\n        \n        comparison_data[model_name] = {\n            \"averages\": avg_metrics,\n            \"avg_tokens\": round(avg_tokens, 1),\n            \"evaluations\": model_evals\n        }\n    \n    # Print comparison table\n    print_comparison_table(model_display_names, comparison_data, metrics_list)\n    \n    # Save final results\n    final_results = {\n        \"models\": model_display_names,\n        \"total_queries\": total_queries,\n        \"comparison\": comparison_data,\n        \"config\": {\n            \"models_tested\": models_dict,\n            \"queries\": test_queries\n        }\n    }\n    \n    with open(\"model_comparison_final.json\", \"w\") as f:\n        json.dump(final_results, f, indent=2)\n    \n    print(f\"\\n💾 Full results saved to: model_comparison_final.json\")\n    \n    return final_results\n\n\ndef print_comparison_table(models, comparison_data, metrics):\n    \"\"\"Print a formatted comparison table\"\"\"\n    \n    # Header\n    print(\"📊 COMPARISON TABLE\")\n    print(\"=\"*100)\n    \n    col_width = 28\n    print(f\"{'Metric':<{col_width}}\", end=\"\")\n    for model in models:\n        # Truncate long model names for display\n        display_name = model if len(model) <= 25 else model[:22] + \"...\"\n        print(f\"{display_name:<{col_width}}\", end=\"\")\n    print()\n    print(\"-\"*100)\n    \n    # Metrics rows\n    metric_labels = {\n        'relevance': '🎯 Relevance',\n        'correctness': '✅ Correctness',\n        'naturalness': '🗣️  Naturalness',\n        'completeness': '📝 Completeness',\n        'overall': '⭐ Overall'\n    }\n    \n    for metric in metrics:\n        label = metric_labels.get(metric, metric.capitalize())\n        print(f\"{label:<{col_width}}\", end=\"\")\n        \n        scores = [comparison_data[model][\"averages\"][metric] for model in models]\n        max_score = max(scores) if scores else 0\n        \n        for model in models:\n            score = comparison_data[model][\"averages\"][metric]\n            is_winner = (score == max_score and score > 0)\n            score_str = f\"{score:.2f}/5.00\"\n            if is_winner:\n                print(f\"{score_str} 🏆{'':<{col_width-len(score_str)-3}}\", end=\"\")\n            else:\n                print(f\"{score_str}{'':<{col_width-len(score_str)}}\", end=\"\")\n        print()\n    \n    print(\"-\"*100)\n    \n    # Token usage\n    print(f\"{'🔢 Avg Tokens Used':<{col_width}}\", end=\"\")\n    for model in models:\n        tokens = comparison_data[model][\"avg_tokens\"]\n        tokens_str = f\"{tokens:.0f}\"\n        print(f\"{tokens_str}{'':<{col_width-len(tokens_str)}}\", end=\"\")\n    print()\n    \n    print(\"=\"*100)\n    \n    # Determine winner\n    overall_scores = [(model, comparison_data[model][\"averages\"][\"overall\"]) for model in models]\n    winner = max(overall_scores, key=lambda x: x[1])\n    \n    print(f\"\\n🏆 WINNER: {winner[0]}\")\n    print(f\"   Overall Score: {winner[1]:.2f}/5.00\\n\")\n\n\n    # =========================================================================\n    # USAGE EXAMPLE\n    # =========================================================================    \n    # Results are automatically saved to:\n    # - model_comparison_progress.json (during evaluation)\n    # - model_comparison_final.json (after completion)\n    \n    print(\"\\n✅ Evaluation complete!\")\n    print(f\"📊 {len(results['comparison'])} models evaluated\")\n    print(f\"❓ {results['total_queries']} queries tested\")\n\n\n\n\ndef extract_entities_and_numbers(text: str) -> dict:\n    \"\"\"\n    Extract player names, team names, and numbers from text.\n    Returns dictionary with entities and numbers.\n    \"\"\"\n    import re\n    \n    text_lower = text.lower()\n    \n    # Extract numbers (integers and decimals)\n    numbers = set(re.findall(r'\\b\\d+\\.?\\d*\\b', text))\n    \n    # Extract entities more aggressively\n    entities = set()\n    \n    # Method 1: Extract from lines (handles lists)\n    lines = text.split('\\n')\n    for line in lines:\n        # Remove common prefixes (numbers, bullets, etc)\n        clean_line = re.sub(r'^\\s*[\\d\\.\\-\\*]+\\.?\\s*', '', line)\n        clean_line = re.sub(r'\\([^)]*\\)', '', clean_line)  # Remove parentheses content\n        \n        # Extract capitalized word sequences\n        words = clean_line.split()\n        current_name = []\n        \n        for word in words:\n            clean_word = re.sub(r'[^\\w\\s\\-\\']', '', word)\n            \n            # Stop at position markers or common words\n            if clean_word.lower() in ['gk', 'def', 'mid', 'fwd', 'goalkeeper', 'defender', 'midfielder', 'forward']:\n                if current_name:\n                    entities.add(' '.join(current_name).lower())\n                    current_name = []\n                continue\n            \n            # Check if looks like a name part\n            if clean_word and len(clean_word) > 1:\n                # Add if capitalized or part of ongoing name\n                if clean_word[0].isupper() or current_name:\n                    current_name.append(clean_word.lower())\n                elif current_name:\n                    # End of name\n                    entities.add(' '.join(current_name).lower())\n                    current_name = []\n        \n        if current_name:\n            entities.add(' '.join(current_name).lower())\n    \n    # Method 2: Also extract last names (single capitalized words > 3 chars)\n    all_words = re.findall(r'\\b[A-Z][a-z]{3,}\\b', text)\n    for word in all_words:\n        entities.add(word.lower())\n    \n    # Method 3: Extract hyphenated names\n    hyphenated = re.findall(r'\\b[A-Z][a-z]+(?:-[A-Z][a-z]+)+\\b', text)\n    for name in hyphenated:\n        entities.add(name.lower())\n    \n    return {\n        'entities': entities,\n        'numbers': numbers\n    }\n\n\ndef calculate_accuracy(answer: str, ground_truth: str) -> dict:\n    \"\"\"\n    Calculate accuracy by comparing entities and numbers.\n    More lenient matching - focuses on whether entities are present.\n    Returns dict with accuracy score and details.\n    \"\"\"\n    import re\n    \n    # Check if model refused to answer or has no information\n    refusal_patterns = [\n        'does not contain', 'not contain', 'no information', 'not available',\n        'cannot find', 'unable to find', 'unfortunately', 'i don\\'t have',\n        'not provided', 'insufficient information', 'no data'\n    ]\n    \n    answer_lower = answer.lower()\n    if any(pattern in answer_lower for pattern in refusal_patterns):\n        # Model refused to answer - automatic fail\n        return {\n            'accuracy': 0.0,\n            'is_correct': False,\n            'entity_precision': 0.0,\n            'entity_recall': 0.0,\n            'entity_f1': 0.0,\n            'number_accuracy': 0.0,\n            'entity_matches': \"0/0\",\n            'number_matches': \"0/0\",\n            'details': 'Model refused to answer or stated no information available'\n        }\n    \n    # Parse ground truth (CSV format)\n    truth_lines = [line.strip() for line in ground_truth.strip().split('\\n') if line.strip()]\n    \n    if len(truth_lines) <= 1:\n        return {\n            'accuracy': 0.0,\n            'entity_precision': 0.0,\n            'entity_recall': 0.0,\n            'number_accuracy': 0.0,\n            'details': 'No ground truth data'\n        }\n    \n    # Extract expected entities (skip header)\n    expected_entities = set()\n    expected_numbers = set()\n    \n    for line in truth_lines[1:]:\n        parts = [p.strip() for p in line.split(',')]\n        if parts:\n            # First column is usually the entity name\n            entity_name = parts[0].lower()\n            expected_entities.add(entity_name)\n            \n            # Extract last name (most important for matching)\n            name_parts = entity_name.split()\n            if name_parts:\n                # Add last name\n                expected_entities.add(name_parts[-1])\n                # Add first name if exists\n                if len(name_parts) > 1:\n                    expected_entities.add(name_parts[0])\n                # Add middle parts\n                for part in name_parts:\n                    if len(part) > 3:\n                        expected_entities.add(part)\n            \n            # Extract numbers\n            for part in parts[1:]:\n                if re.match(r'^\\d+\\.?\\d*$', part):\n                    expected_numbers.add(part)\n    \n    # Remove duplicates from expected entities (since ground truth has dupes)\n    unique_expected = set()\n    for entity in expected_entities:\n        # Only keep substantial entities\n        if len(entity) > 2:\n            unique_expected.add(entity)\n    \n    # Extract entities from answer\n    answer_data = extract_entities_and_numbers(answer)\n    found_entities = answer_data['entities']\n    found_numbers = answer_data['numbers']\n    \n    # Calculate entity matching with flexible approach\n    matched_entities = set()\n    for expected in unique_expected:\n        for found in found_entities:\n            # Exact match\n            if expected == found:\n                matched_entities.add(expected)\n                break\n            # Substring match (for partial names)\n            elif expected in found or found in expected:\n                matched_entities.add(expected)\n                break\n            # Check if all words of expected are in found\n            expected_words = set(expected.split())\n            found_words = set(found.split())\n            if expected_words and expected_words.issubset(found_words):\n                matched_entities.add(expected)\n                break\n    \n    entity_matches = len(matched_entities)\n    \n    # For lists without numbers, don't penalize\n    has_numbers = len(expected_numbers) > 0\n    \n    if has_numbers:\n        # Calculate entity precision/recall\n        entity_precision = entity_matches / len(found_entities) if found_entities else 0\n        entity_recall = entity_matches / len(unique_expected) if unique_expected else 0\n        entity_f1 = 2 * (entity_precision * entity_recall) / (entity_precision + entity_recall) if (entity_precision + entity_recall) > 0 else 0\n        \n        # Calculate number matching\n        number_matches = len(expected_numbers & found_numbers)\n        number_accuracy = number_matches / len(expected_numbers) if expected_numbers else 0\n        \n        # Combined accuracy (70% entities, 30% numbers)\n        final_accuracy = (entity_f1 * 0.7) + (number_accuracy * 0.3)\n    else:\n        # For pure list queries (like \"players on team\"), use recall only\n        # If we found at least 50% of expected entities, it's good\n        entity_recall = entity_matches / len(unique_expected) if unique_expected else 0\n        entity_precision = 1.0  # Don't penalize for finding extra entities\n        entity_f1 = entity_recall  # Just use recall\n        number_accuracy = 1.0  # No numbers expected\n        \n        # For list queries, lower threshold\n        final_accuracy = entity_recall\n    \n    # Determine if correct (lower threshold for list queries)\n    threshold = 0.3 if not has_numbers else 0.5\n    is_correct = final_accuracy > threshold\n    \n    return {\n        'accuracy': final_accuracy,\n        'is_correct': is_correct,\n        'entity_precision': entity_precision,\n        'entity_recall': entity_recall,\n        'entity_f1': entity_f1,\n        'number_accuracy': number_accuracy,\n        'entity_matches': f\"{entity_matches}/{len(unique_expected)}\",\n        'number_matches': f\"{len(expected_numbers & found_numbers)}/{len(expected_numbers)}\" if expected_numbers else \"N/A\",\n        'details': f\"Entities: {entity_matches}/{len(unique_expected)}\" + (f\", Numbers: {len(expected_numbers & found_numbers)}/{len(expected_numbers)}\" if expected_numbers else \" (list query)\")\n    }\n\n\ndef evaluate_all_models():\n    \"\"\"\n    Evaluate multiple models on predefined test queries.\n    \n    Measures:\n    - Accuracy (NER-based entity and number matching)\n    - Response time (seconds)\n    - Tokens used\n    \n    Returns:\n        Dictionary with individual results and comparison table\n    \"\"\"\n    import time\n    import pandas as pd\n    \n    # =========================================================================\n    # MODEL CONFIGURATION\n    # =========================================================================\n    \n    models = {\n        \"Llama 3.3 70B\": \"meta-llama/llama-3.3-70b-instruct:free\",\n        \"Devstral 2512\": \"mistralai/devstral-2512:free\",\n        \"Nemotron Nano 30B\": \"nvidia/nemotron-3-nano-30b-a3b:free\",\n    }\n    \n    # =========================================================================\n    # TEST QUERIES CONFIGURATION\n    # =========================================================================\n    \n    queries_with_truths = queries_with_truths = [\n    {\n        \"question\": \"Who are the top 10 forwards in the 2022-23 season?\",\n        \"ground_truth\": \"\"\"player,season_points\nErling Haaland,272\nHarry Kane,263\nIvan Toney,182\nOllie Watkins,175\nCallum Wilson,157\nBryan Mbeumo,150\nDominic Solanke,130\nGabriel Fernando de Jesus,125\nBrennan Johnson,122\nAleksandar Mitrović,107\"\"\"\n    },\n    {\n        \"question\": \"How did Erling Haaland perform in gameweek 20 of the 2022-23 season?\",\n        \"ground_truth\": \"\"\"player,points,goals,assists,minutes\nErling Haaland,2,0,0,90\nErling Haaland,6,1,0,89\"\"\"\n    },\n    {\n        \"question\": \"Compare Erling Haaland and Harry Kane's performance in the 2022-23 season\",\n        \"ground_truth\": \"\"\"player,total_points,goals,assists\nErling Haaland,272,36,9\nHarry Kane,263,30,9\"\"\"\n    },\n    {\n        \"question\": \"What were Manchester City's fixtures in the 2022-23 season?\",\n        \"ground_truth\": \"\"\"gameweek,home_team,away_team,kickoff_time\n1,West Ham,Man City,2022-08-07 15:30:00+00:00\n3,Newcastle,Man City,2022-08-21 15:30:00+00:00\n6,Aston Villa,Man City,2022-09-03 16:30:00+00:00\n8,Wolves,Man City,2022-09-17 11:30:00+00:00\n11,Liverpool,Man City,2022-10-16 15:30:00+00:00\n14,Leicester,Man City,2022-10-29 11:30:00+00:00\n17,Leeds,Man City,2022-12-28 20:00:00+00:00\n19,Chelsea,Man City,2023-01-05 20:00:00+00:00\n20,Man Utd,Man City,2023-01-14 12:30:00+00:00\n22,Spurs,Man City,2023-02-05 16:30:00+00:00\n23,Arsenal,Man City,2023-02-15 19:30:00+00:00\n24,Nott'm Forest,Man City,2023-02-18 15:00:00+00:00\n25,Bournemouth,Man City,2023-02-25 17:30:00+00:00\n27,Crystal Palace,Man City,2023-03-11 17:30:00+00:00\n30,Southampton,Man City,2023-04-08 16:30:00+00:00\n34,Fulham,Man City,2023-04-30 13:00:00+00:00\n36,Everton,Man City,2023-05-14 13:00:00+00:00\n37,Brighton,Man City,2023-05-24 19:00:00+00:00\n38,Brentford,Man City,2023-05-28 15:30:00+00:00\"\"\"\n    },\n    {\n        \"question\": \"Which players are on Manchester City?\",\n        \"ground_truth\": \"\"\"player,position\nAlex Robertson,MID\nAymeric Laporte,DEF\nAymeric Laporte,DEF\nBen Knight,MID\nBenjamin Mendy,DEF\nBernardo Mota Veiga de Carvalho e Silva,MID\nBernardo Veiga de Carvalho e Silva,MID\nCieran Slicker,GK\nClaudio Gomes,MID\nCole Palmer,MID\nCole Palmer,MID\nConrad Egan-Riley,DEF\nEderson Santana de Moraes,GK\nEderson Santana de Moraes,GK\nErling Haaland,FWD\nFernando Luiz Rosa,MID\nFerran Torres,MID\nGabriel Fernando de Jesus,FWD\nIlkay Gündogan,MID\nIlkay Gündogan,MID\nJack Grealish,MID\nJack Grealish,MID\nJames McAtee,MID\nJames McAtee,MID\nJohn Stones,DEF\nJohn Stones,FWD\nJohn Stones,DEF\nJosh Wilson-Esbrand,DEF\nJoshua Wilson-Esbrand,DEF\nJoão Cancelo,DEF\nJoão Pedro Cavaco Cancelo,DEF\nJulián Álvarez,FWD\nKalvin Phillips,MID\nKayky da Silva Chagas,FWD\nKevin De Bruyne,MID\nKevin De Bruyne,MID\nKyle Walker,DEF\nKyle Walker,DEF\nLiam Delap,FWD\nLiam Delap,FWD\nLuke Mbete,DEF\nLuke Mbete-Tabu,DEF\nManuel Akanji,DEF\nMáximo Perrone,MID\nNathan Aké,DEF\nNathan Aké,DEF\nNico O'Reilly,MID\nOleksandr Zinchenko,DEF\nPhil Foden,MID\nPhil Foden,MID\nRaheem Sterling,MID\nRico Lewis,DEF\nRiyad Mahrez,MID\nRiyad Mahrez,MID\nRodrigo Hernandez,MID\nRodrigo Hernandez,MID\nRomeo Lavia,MID\nRúben Gato Alves Dias,DEF\nRúben Santos Gato Alves Dias,DEF\nSamuel Edozie,MID\nScott Carson,GK\nScott Carson,GK\nSergio Gómez,DEF\nShea Charles,MID\nStefan Ortega Moreno,GK\nTommy Doyle,MID\nZack Steffen,GK\nZack Steffen,GK\"\"\"\n    },\n    {\n        \"question\": \"Who were the top 10 goal scorers in the 2022-23 season?\",\n        \"ground_truth\": \"\"\"player,total_goals\nErling Haaland,36\nHarry Kane,30\nIvan Toney,20\nMohamed Salah,19\nCallum Wilson,18\nMarcus Rashford,17\nOllie Watkins,15\nGabriel Martinelli Silva,15\nMartin Ødegaard,15\nAleksandar Mitrović,14\"\"\"\n    },\n    {\n        \"question\": \"Who provided the most assists in the 2022-23 season? Show me the top 10\",\n        \"ground_truth\": \"\"\"player,total_assists\nKevin De Bruyne,18\nMohamed Salah,13\nMorgan Gibbs-White,12\nRiyad Mahrez,12\nBukayo Saka,12\nMichael Olise,11\nTrent Alexander-Arnold,11\nJack Grealish,10\nAndreas Hoelgebaum Pereira,10\nSolly March,10\"\"\"\n    },\n    {\n        \"question\": \"Which players had the most clean sheets in the 2022-23 season?\",\n        \"ground_truth\": \"\"\"player,total_clean_sheets\nBruno Borges Fernandes,18\nDavid De Gea Quintana,17\nKieran Trippier,16\nFabian Schär,15\nMiguel Almirón Rejala,15\nBenjamin White,15\nDan Burn,14\nAlisson Ramses Becker,14\nGabriel Martinelli Silva,14\nAaron Ramsdale,14\"\"\"\n    },\n    {\n        \"question\": \"Who are the top 10 players in form as of gameweek 20 in the 2022-23 season?\",\n        \"ground_truth\": \"\"\"player,form_points\nMartin Ødegaard,51\nSolly March,46\nKieran Trippier,44\nHarry Kane,43\nMarcus Rashford,41\nBruno Borges Fernandes,40\nRiyad Mahrez,39\nIvan Toney,39\nLuke Shaw,38\nChristian Eriksen,35\"\"\"\n    },\n    {\n        \"question\": \"Give me a complete summary of Erling Haaland's 2022-23 season\",\n        \"ground_truth\": \"\"\"player,total_points,goals,assists,clean_sheets,minutes,bonus_points\nErling Haaland,272,36,9,13,2767,40\"\"\"\n    },\n    {\n        \"question\": \"Who earned the most bonus points in the 2022-23 season?\",\n        \"ground_truth\": \"\"\"player,total_bonus\nHarry Kane,48\nErling Haaland,40\nKieran Trippier,39\nIvan Toney,35\nMartin Ødegaard,30\nKevin De Bruyne,26\nOllie Watkins,25\nMohamed Salah,23\nBruno Borges Fernandes,23\nTrent Alexander-Arnold,21\"\"\"\n    },\n    {\n        \"question\": \"Which players received the most yellow and red cards in the 2022-23 season?\",\n        \"ground_truth\": \"\"\"player,total_yellow,total_red,total_cards\nJoão Palhinha Gonçalves,14,0,14\nRúben da Silva Neves,12,0,12\nNélson Cabral Semedo,11,1,12\nJoelinton Cássio Apolinário de Lira,12,0,12\nFabio Henrique Tavares,11,0,11\nAdam Smith,11,0,11\nJames Maddison,10,0,10\nMoisés Caicedo Corozo,10,0,10\nConor Gallagher,9,1,10\nCristian Romero,9,1,10\"\"\"\n    }\n]\n    \n    # =========================================================================\n    # EVALUATE ALL MODELS\n    # =========================================================================\n    \n    all_model_results = {}\n    \n    for model_name, model_id in models.items():\n        print(f\"\\n{'='*80}\")\n        print(f\"EVALUATING MODEL: {model_name}\")\n        print(f\"{'='*80}\\n\")\n        \n        results = []\n        total_response_time = 0\n        total_tokens = 0\n        total_accuracy = 0\n        correct_count = 0\n        evaluated_count = 0\n        \n        for idx, item in enumerate(queries_with_truths, 1):\n            question = item[\"question\"]\n            ground_truth = item.get(\"ground_truth\")\n            \n            print(f\"{'─'*80}\")\n            print(f\"Query {idx}/{len(queries_with_truths)}\")\n            print(f\"{'─'*80}\")\n            print(f\"❓ Question: {question}\")\n            \n            start = time.time()\n            try:\n                answer, tokens_used = call_llm(question, model_id)\n                answer_preview = str(answer)[:300] + \"...\" if len(str(answer)) > 300 else str(answer)\n                print(f\"🤖 Model Answer: {answer_preview}\")\n                \n                # Print ground truth for comparison\n                if ground_truth:\n                    truth_preview = ground_truth[:200] + \"...\" if len(ground_truth) > 200 else ground_truth\n                    print(f\"📋 Ground Truth: {truth_preview}\")\n                    \n            except Exception as e:\n                print(f\"❌ Error: {str(e)}\")\n                results.append({\n                    \"question\": question,\n                    \"ground_truth\": ground_truth,\n                    \"answer\": None,\n                    \"error\": str(e),\n                    \"response_time\": None,\n                    \"tokens_used\": 0,\n                    \"accuracy\": None\n                })\n                print()\n                continue\n            \n            end = time.time()\n            response_time = end - start\n            total_tokens += tokens_used\n            total_response_time += response_time\n            \n            # Calculate accuracy\n            accuracy_result = calculate_accuracy(str(answer), ground_truth)\n            accuracy_score = accuracy_result['accuracy']\n            is_correct = accuracy_result['is_correct']\n            \n            total_accuracy += accuracy_score\n            if is_correct:\n                correct_count += 1\n            evaluated_count += 1\n            \n            status = \"✅ CORRECT\" if is_correct else \"❌ INCORRECT\"\n            print(f\"\\n{status}\")\n            print(f\"📊 Accuracy: {accuracy_score:.2%}\")\n            print(f\"   Entity F1: {accuracy_result['entity_f1']:.2%}\")\n            print(f\"   Number Match: {accuracy_result['number_accuracy']:.2%}\")\n            print(f\"   Details: {accuracy_result['details']}\")\n            print(f\"⏱️  Response Time: {response_time:.3f}s\")\n            print(f\"🔢 Tokens Used: {tokens_used}\")\n            print()\n            \n            results.append({\n                \"question\": question,\n                \"ground_truth\": ground_truth,\n                \"answer\": str(answer),\n                \"response_time\": response_time,\n                \"tokens_used\": tokens_used,\n                \"accuracy\": accuracy_score,\n                \"is_correct\": is_correct,\n                \"accuracy_details\": accuracy_result\n            })\n        \n        num_queries = len(queries_with_truths)\n        avg_response_time = total_response_time / num_queries if num_queries > 0 else 0\n        avg_tokens = total_tokens / num_queries if num_queries > 0 else 0\n        avg_accuracy = (total_accuracy / evaluated_count) * 100 if evaluated_count > 0 else 0\n        correct_percentage = (correct_count / evaluated_count) * 100 if evaluated_count > 0 else 0\n        \n        all_model_results[model_name] = {\n            \"model_id\": model_id,\n            \"results\": results,\n            \"average_response_time\": avg_response_time,\n            \"average_tokens_used\": avg_tokens,\n            \"average_accuracy\": avg_accuracy,\n            \"correct_percentage\": correct_percentage,\n            \"correct_count\": correct_count,\n            \"total_queries\": evaluated_count\n        }\n        \n        print(f\"{'='*80}\")\n        print(f\"SUMMARY FOR {model_name}\")\n        print(f\"{'='*80}\")\n        print(f\"Total Queries: {num_queries}\")\n        print(f\"Correct Answers: {correct_count}/{evaluated_count} ({correct_percentage:.1f}%)\")\n        print(f\"Average Accuracy Score: {avg_accuracy:.1f}%\")\n        print(f\"Average Response Time: {avg_response_time:.3f}s\")\n        print(f\"Average Tokens Used: {avg_tokens:.1f}\")\n        print(f\"{'='*80}\\n\")\n    \n    # =========================================================================\n    # CREATE COMPARISON TABLE\n    # =========================================================================\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"MODEL COMPARISON TABLE\")\n    print(f\"{'='*80}\\n\")\n    \n    comparison_data = []\n    for model_name, metrics in all_model_results.items():\n        comparison_data.append({\n            \"Model\": model_name,\n            \"Correct (%)\": f\"{metrics['correct_percentage']:.1f}\",\n            \"Avg Accuracy\": f\"{metrics['average_accuracy']:.1f}%\",\n            \"Avg Time (s)\": f\"{metrics['average_response_time']:.3f}\",\n            \"Avg Tokens\": f\"{metrics['average_tokens_used']:.0f}\"\n        })\n    \n    df = pd.DataFrame(comparison_data)\n    print(df.to_string(index=False))\n    print(f\"\\n{'='*80}\\n\")\n    \n    # Find best model for each metric\n    print(\"🏆 BEST PERFORMERS:\")\n    print(f\"{'─'*80}\")\n    \n    best_accuracy = max(all_model_results.items(), key=lambda x: x[1]['average_accuracy'])\n    print(f\"Best Accuracy: {best_accuracy[0]} ({best_accuracy[1]['average_accuracy']:.1f}%)\")\n    \n    best_correct = max(all_model_results.items(), key=lambda x: x[1]['correct_percentage'])\n    print(f\"Most Correct: {best_correct[0]} ({best_correct[1]['correct_count']}/{best_correct[1]['total_queries']})\")\n    \n    best_speed = min(all_model_results.items(), key=lambda x: x[1]['average_response_time'])\n    print(f\"Fastest: {best_speed[0]} ({best_speed[1]['average_response_time']:.3f}s)\")\n    \n    best_efficiency = min(all_model_results.items(), key=lambda x: x[1]['average_tokens_used'])\n    print(f\"Most Efficient: {best_efficiency[0]} ({best_efficiency[1]['average_tokens_used']:.0f} tokens)\")\n    \n    print(f\"{'='*80}\\n\")\n    \n    return {\n        \"individual_results\": all_model_results,\n        \"comparison_table\": df,\n        \"summary\": {\n            \"total_models_evaluated\": len(models),\n            \n        }\n    }\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:14:21.321024Z","iopub.execute_input":"2025-12-16T07:14:21.321761Z","iopub.status.idle":"2025-12-16T07:14:23.668222Z","shell.execute_reply.started":"2025-12-16T07:14:21.321725Z","shell.execute_reply":"2025-12-16T07:14:23.667215Z"}},"outputs":[{"name":"stdout","text":"🚀 Initializing FPL Input Preprocessor (Enhanced)...\n✅ Connected to LLM: google/gemma-2-2b-it\n✅ Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n✅ System initialized\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"\ndef main():\n    user_question = input(\"Hey manager, what do you need to know?: \")\n    \n    models = {\n    \"1\": \"mistralai/devstral-2512:free\",\n    \"2\":  \"meta-llama/llama-3.3-70b-instruct:free\",\n    \"3\": \"nvidia/nemotron-3-nano-30b-a3b:free\",\n    }\n    \n    embedding_types = {\n        \"1\": \"numeric\",\n        \"2\": \"minilm\", \n        \"3\": \"mpnet\"\n    }\n    \n    # Ask for model choice\n    model_choice = input(\"Choose a model (1-3): \").strip()\n    \n    # Ask for embedding type choice\n    print(\"\\nEmbedding types:\")\n    print(\"1. Numeric (fast, stat-based)\")\n    print(\"2. MiniLM (semantic, good balance)\")\n    print(\"3. MPNet (most accurate semantic)\")\n    embedding_choice = input(\"Choose embedding type (1-3): \").strip()\n    \n    if model_choice in models and embedding_choice in embedding_types:\n        model = models[model_choice]\n        embedding_type = embedding_types[embedding_choice]\n        \n        print(f\"You selected Model {model_choice}: {model}\")\n        print(f\"Using {embedding_type} embeddings\")\n        \n        answer = call_llm(user_question, model, embedding_type)\n        print(\"\\nLLM Answer:\\n\", answer[0])\n    else:\n        print(\"Invalid choice.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:13:26.813785Z","iopub.execute_input":"2025-12-16T07:13:26.814159Z","iopub.status.idle":"2025-12-16T07:13:41.392119Z","shell.execute_reply.started":"2025-12-16T07:13:26.814114Z","shell.execute_reply":"2025-12-16T07:13:41.390992Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Hey manager, what do you need to know?:  Salah performance gameweek 20 season 22-23\nChoose a model (1-3):  1\n"},{"name":"stdout","text":"\nEmbedding types:\n1. Numeric (fast, stat-based)\n2. MiniLM (semantic, good balance)\n3. MPNet (most accurate semantic)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose embedding type (1-3):  1\n"},{"name":"stdout","text":"You selected Model 1: mistralai/devstral-2512:free\nUsing numeric embeddings\n{'user_input': 'Salah performance gameweek 20 season 22-23', 'baseline': {'method': 'baseline', 'intent': 'performance_query', 'query_used': 'player_gameweek_performance', 'parameters': {'intent': 'performance_query', 'intent_confidence': 0.99, 'gameweek': 20, 'player_name': 'Mohamed Salah', 'threshold': 22.0, 'thresholds': [22.0, 23.0], 'limit': 10, 'season': '2022-23'}, 'results': [], 'cypher': '\\n                // Player performance in specific gameweek\\n                MATCH (gw:Gameweek {season: $season, GW_number: $gameweek})<-[:HAS_FIXTURE]-(f:Fixture)\\n                MATCH (p:Player {player_name: $player_name})-[played:PLAYED_IN]->(f)\\n                MATCH (p)-[:PLAYS_FOR]->(t:Team)\\n                RETURN p.player_name AS player,\\n                    t.name AS team,\\n                    played.total_points AS points,\\n                    played.goals_scored AS goals,\\n                    played.assists AS assists,\\n                    played.minutes AS minutes,\\n                    played.bonus AS bonus,\\n                    played.clean_sheets AS clean_sheets,\\n                    played.ict_index AS ict_index\\n            '}, 'embedding': {'method': 'numeric_embedding', 'criteria': {'total_points': 'high'}, 'position_filter': None, 'embedding_type': 'numeric', 'embedding_dimensions': 12, 'results': []}, 'combined': []}\n\nLLM Answer:\n In Gameweek 20 of the 2022-23 season, Mohamed Salah had the following performance:\n\n- **Player:** Mohamed Salah\n- **Team:** Liverpool\n- **Points:** [Points scored] (Note: The exact points are not provided in the context, but you can check the FPL website or app for the precise value.)\n- **Goals:** [Goals scored]\n- **Assists:** [Assists]\n- **Minutes Played:** [Minutes]\n- **Bonus Points:** [Bonus]\n- **Clean Sheets:** [Clean sheets] (if applicable)\n- **ICT Index:** [ICT Index] (Influence, Creativity, Threat score)\n\nFor the exact stats, you can refer to the FPL official website or app, as the context does not include the specific numbers. Salah is typically a strong performer, so his points in GW20 would likely reflect his usual high output.\n\nWould you like insights on his performance trends or comparisons with other players?\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# compare_models_human_evaluation()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # evaluate_all_models()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 1: INSTALL DEPENDENCIES\n# ============================================\n\n!pip install streamlit pyngrok -q\n!ngrok authtoken \"36ulLRiXuNcPFvvcjBqzMY4fzzD_2YrjyBDEVMznrTiSTi8j\"\n\n# ============================================\n# CELL 2: COPY Task3.py FUNCTIONS TO WORKING DIR\n# ============================================\n\n# Since Task3.py code is in another cell in your Kaggle notebook,\n# we need to create a standalone Task3.py file for Streamlit to import\n\ntask3_code = \"\"\"\nfrom kaggle_secrets import UserSecretsClient\nfrom neo4j import GraphDatabase\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import Dict, List, Any\nfrom fpl_Task2 import FPLGraphRetrieval\nfrom openai import OpenAI\n\n# Get secrets\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"openrouterkey\")\nsecret_value_1 = user_secrets.get_secret(\"preprocesskey\")\n\n# Neo4j Configuration\nNEO4J_URI = \"neo4j+s://1da86c19.databases.neo4j.io\"\nNEO4J_USER = \"neo4j\"\nNEO4J_PASSWORD = \"HA4iunTOGen7RYpeISs3ZRhcWjpcokqam9przCqCuQ8\"\n\n# Initialize retriever\nretriever = FPLGraphRetrieval(\n    neo4j_uri=NEO4J_URI,\n    neo4j_user=NEO4J_USER,\n    neo4j_password=NEO4J_PASSWORD,\n    hf_token=secret_value_1,\n    use_llm=True\n)\n\ndef call_llm(user_question: str, model: str, embedding_type: str = 'numeric'):\n    openrouter_key = secret_value_0\n    \n    client = OpenAI(\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key=openrouter_key\n    )\n    \n    result = retriever.retrieve(user_question, method=\"both\", embedding_type=embedding_type)\n    context_text = result\n    \n    persona_text = (\n        \"You are an FPL (Fantasy Premier League) expert. \"\n        \"You can answer any questions related to FPL, including player stats, \"\n        \"team performance, transfers, and gameweek strategies. \"\n        \"You are also knowledgeable about the Premier League in general.\"\n    )\n    \n    task_text = (\n        \"IMPORTANT: You MUST always provide a response. Never return empty text.\\\\n\\\\n\"\n        \"SPECIAL INSTRUCTIONS FOR COMPARISON QUERIES:\\\\n\"\n        \"When the user asks to compare players (e.g., 'compare X and Y'):\\\\n\"\n        \"1. First check if BOTH players are mentioned in the CONTEXT\\\\n\"\n        \"2. If yes, extract ALL their stats from the CONTEXT\\\\n\"\n        \"3. Create a detailed comparison table or bullet points\\\\n\"\n        \"4. Include: goals, points, assists, clean sheets (if relevant), bonus points\\\\n\"\n        \"5. Provide a clear conclusion about who performed better overall\\\\n\\\\n\"\n        \"CONTEXT:\\\\n{context}\\\\n\\\\n\"\n        \"QUESTION: {question}\\\\n\\\\n\"\n        \"ANSWER (remember: never leave this blank):\"\n    )\n    \n    prompt = f\\\"\\\"\\\"\n    CONTEXT:\n    {context_text}\n\n    PERSONA:\n    {persona_text}\n\n    TASK:\n    {task_text}\n    \\\"\\\"\\\"\n    \n    completion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": persona_text},\n            {\"role\": \"user\", \"content\": f\"{prompt}\\\\n\\\\nQuestion: {user_question}\"}\n        ],\n        temperature=0.2,\n        max_tokens=500\n    )\n    \n    answer = completion.choices[0].message.content\n    total_tokens = completion.usage.total_tokens\n    \n    return answer, total_tokens\n\"\"\"\n\n# Write Task3.py\nwith open('Task3.py', 'w') as f:\n    f.write(task3_code)\n\nprint(\"✅ Task3.py created successfully!\")\n\n# ============================================\n# CELL 3: CREATE STREAMLIT APP FILE\n# ============================================\n\napp_code = \"\"\"\nimport streamlit as st\nimport pandas as pd\nimport sys\nimport os\n\n# Add current directory to Python path\nsys.path.insert(0, os.getcwd())\n\n# Import from Task3\ntry:\n    from Task3 import call_llm, retriever\nexcept ImportError as e:\n    st.error(f\"❌ Could not import Task3: {e}\")\n    st.stop()\n\n# =========================\n# PAGE CONFIG\n# =========================\nst.set_page_config(\n    page_title=\"FPL Graph-RAG Assistant\",\n    page_icon=\"⚽\",\n    layout=\"wide\"\n)\n\n# Custom CSS for FPL theme\nst.markdown('''\n    <style>\n    .manager-header {\n        background: linear-gradient(90deg, #1e3c72 0%, #2a5298 100%);\n        color: white;\n        padding: 25px;\n        border-radius: 15px;\n        margin-bottom: 25px;\n        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n    }\n    .fpl-stats {\n        background-color: #f8f9fa;\n        border-left: 5px solid #28a745;\n        padding: 20px;\n        border-radius: 8px;\n        margin: 15px 0;\n        box-shadow: 0 2px 4px rgba(0,0,0,0.05);\n    }\n    .stButton button {\n        background: linear-gradient(90deg, #FF6B6B 0%, #FF8E53 100%);\n        color: white;\n        font-weight: bold;\n        border: none;\n    }\n    .stButton button:hover {\n        background: linear-gradient(90deg, #FF8E53 0%, #FF6B6B 100%);\n        color: white;\n    }\n    .sidebar-header {\n        background: linear-gradient(90deg, #2c3e50 0%, #4ca1af 100%);\n        color: white;\n        padding: 15px;\n        border-radius: 10px;\n        margin-bottom: 20px;\n    }\n    </style>\n''', unsafe_allow_html=True)\n\n# =========================\n# SIDEBAR CONTROLS\n# =========================\nwith st.sidebar:\n    st.markdown('<div class=\"sidebar-header\">', unsafe_allow_html=True)\n    st.title(\"⚙ FPL Manager Dashboard\")\n    st.caption(\"Configure your AI Assistant\")\n    st.markdown('</div>', unsafe_allow_html=True)\n    \n    st.markdown(\"### 🧠 1. Generation Model\")\n    st.caption(\"Select the LLM that generates the final answer.\")\n    llm_map = {\n        \"Devstral\": \"mistralai/devstral-2512:free\",\n        \"Llama 3 70B\": \"meta-llama/llama-3.3-70b-instruct:free\",\n        \"Nemotron 3\": \"nvidia/nemotron-3-nano-30b-a3b:free\"\n    }\n\n\n    llm_choice = st.selectbox(\"LLM Model\", list(llm_map.keys()), index=0)\n    selected_llm_id = llm_map[llm_choice]\n\n    st.markdown(\"---\")\n\n    st.markdown(\"### 📐 2. Embedding Model\")\n    st.caption(\"How the system finds similar players/stats.\")\n    embedding_map = {\n        \"Numeric (Stats-based)\": \"numeric\",\n        \"MiniLM (Fast, 384d)\": \"minilm\",\n        \"MPNet (Accurate, 768d)\": \"mpnet\"\n    }\n    emb_choice = st.selectbox(\"Embedding Model\", list(embedding_map.keys()), index=0)\n    selected_emb_type = embedding_map[emb_choice]\n\n    st.markdown(\"---\")\n    \n    # FPL Tips Section\n    with st.expander(\"💡 FPL Assistant Tips\"):\n        st.markdown('''\n        **Best Query Examples:**\n        - \"Compare Haaland vs Salah\"\n        - \"Top defenders for clean sheets\"\n        - \"Which midfielders have most assists?\"\n        - \"Erling Haaland gameweek 20 performance\"\n        - \"Manchester City upcoming fixtures\"\n        \n        **Pro Tips:**\n        - Use **Numeric embeddings** for stat-based similarity\n        - Use **Text embeddings** for semantic understanding\n        ''')\n    \n    st.markdown(\"---\")\n    \n    # System Status\n    st.markdown(\"### 📊 System Status\")\n    col1, col2 = st.columns(2)\n    with col1:\n        st.metric(\"LLM\", llm_choice.split()[0])\n    with col2:\n        st.metric(\"Embeddings\", selected_emb_type[:6])\n    \n    if st.button(\"🔄 Reset Conversation\", type=\"secondary\", use_container_width=True):\n        st.session_state.messages = []\n        st.rerun()\n\n# =========================\n# MAIN INTERFACE\n# =========================\nst.markdown('<div class=\"manager-header\">', unsafe_allow_html=True)\nst.title(\"⚽ FPL Graph-RAG System\")\nst.markdown(\"### 👋 Welcome, Manager! Your AI Assistant for Fantasy Premier League Decisions\")\nst.caption(f\"*Current Pipeline:* **{llm_choice}** → **{emb_choice}**\")\nst.markdown('</div>', unsafe_allow_html=True)\n\n# Quick Stats Bar\ncol1, col2, col3, col4 = st.columns(4)\nwith col1:\n    st.metric(\"LLM Models\", \"4\")\nwith col2:\n    st.metric(\"Embedding Types\", \"3\")\nwith col3:\n    st.metric(\"Knowledge Graph\", \"Active\")\nwith col4:\n    st.metric(\"Retrieval Method\", \"GraphRAG\")\n\n# Initialize chat history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [\n        {\"role\": \"assistant\", \"content\": \"Hello Manager! I'm your FPL AI Assistant. Ask me about players, teams, stats, or comparisons! ⚽\"}\n    ]\n\n# Display history\nfor msg in st.session_state.messages:\n    with st.chat_message(msg[\"role\"]):\n        st.markdown(msg[\"content\"])\n\n# Input handling\nif prompt := st.chat_input(\"Ask about FPL players, teams, or stats... e.g., 'Compare Haaland and Salah'\"):\n    \n    # User message\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    # Assistant processing\n    with st.chat_message(\"assistant\"):\n        # Create a container for the \"Working...\" status\n        status_container = st.status(\"🏗 Processing Pipeline...\", expanded=True)\n        \n        try:\n            status_container.write(f\"🤖 Using Model: **{llm_choice}**\")\n            status_container.write(f\"📐 Using Embeddings: **{emb_choice}**\")\n            status_container.write(\"🔍 Retrieving data from Knowledge Graph...\")\n            \n            # Call the LLM function from Task3.py\n            answer, tokens_used = call_llm(\n                user_question=prompt,\n                model=selected_llm_id,\n                embedding_type=selected_emb_type\n            )\n            \n            status_container.write(f\"✅ Generated response ({tokens_used} tokens used)\")\n            status_container.update(label=\"✅ Pipeline Complete\", state=\"complete\", expanded=False)\n            \n            # Display the answer\n            st.markdown(answer)\n            \n            # Show token usage\n            with st.expander(\"📊 Token Usage\"):\n                st.metric(\"Total Tokens\", tokens_used)\n            \n            # Save to history\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n\n        except Exception as e:\n            status_container.update(label=\"❌ Error\", state=\"error\")\n            error_msg = f\"An error occurred: {e}\"\n            st.error(error_msg)\n            st.info(\"Try simplifying your query or checking your configuration\")\n            \n            # Save error to history\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"❌ {error_msg}\"})\n\n# Footer\nst.markdown(\"---\")\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    st.caption(\"⚽ **FPL Graph-RAG System**\")\nwith col2:\n    st.caption(\"📊 Powered by Neo4j Knowledge Graph\")\nwith col3:\n    st.caption(\"🤖 Enhanced with LLM + Embeddings\")\n\"\"\"\n\n# Write the app code to file\nwith open('app.py', 'w') as f:\n    f.write(app_code)\n\nprint(\"✅ app.py created successfully!\")\n\n# ============================================\n# CELL 4: LAUNCH STREAMLIT WITH NGROK\n# ============================================\n\nimport time\nfrom pyngrok import ngrok\nimport os\n\n# Kill any existing Streamlit processes\nos.system('pkill -f streamlit')\n\n# Start Streamlit in the background\nos.system('streamlit run app.py --server.port 8501 &')\n\n# Wait for Streamlit to start\nprint(\"⏳ Starting Streamlit server...\")\ntime.sleep(8)\n\n# Create ngrok tunnel\npublic_url = ngrok.connect(8501)\n\nprint(\"=\"*60)\nprint(\"🎉 STREAMLIT APP IS LIVE!\")\nprint(\"=\"*60)\nprint(f\"📱 Access your app at: {public_url}\")\nprint(\"=\"*60)\nprint(\"\\n⚠️  IMPORTANT:\")\nprint(\"   - Keep this Kaggle notebook running\")\nprint(\"   - The URL will stop working if you close this notebook\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T07:11:14.347182Z","iopub.execute_input":"2025-12-16T07:11:14.347552Z","iopub.status.idle":"2025-12-16T07:11:27.279065Z","shell.execute_reply.started":"2025-12-16T07:11:14.347526Z","shell.execute_reply":"2025-12-16T07:11:27.277832Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n✅ Task3.py created successfully!\n✅ app.py created successfully!\n🚀 Initializing FPL Input Preprocessor (Enhanced)...\n✅ Connected to LLM: google/gemma-2-2b-it\n✅ Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n✅ System initialized\n\n  Stopping...\n⏳ Starting Streamlit server...\n\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\n\n  You can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://172.19.2.2:8501\n  External URL: http://34.148.67.200:8501\n\n============================================================\n🎉 STREAMLIT APP IS LIVE!\n============================================================\n📱 Access your app at: NgrokTunnel: \"https://talia-nonpublishable-shelley.ngrok-free.dev\" -> \"http://localhost:8501\"\n============================================================\n\n⚠️  IMPORTANT:\n   - Keep this Kaggle notebook running\n   - The URL will stop working if you close this notebook\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"2025-12-16 07:11:38.170661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765869098.202723     444 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765869098.212124     444 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}